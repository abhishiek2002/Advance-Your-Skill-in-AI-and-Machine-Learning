Managing feature drift
- [Instructor] Let's discuss the techniques and best practices for managing feature drift in this video. Let's start with measuring feature drift. The biggest use of feature drift comes in when we cannot measure concept drift due to non-availability of true labels. Feature drift serves as a proxy indicator for potential concept drift. It also helps to monitor feature sanctity and whether there are changes or errors happening in upstream data generators and processors. To compute feature drift, we need datasets of features from both training and production. These can end up being very large as most techniques for feature drift involve comparing two sample sets and data mining if they belong to the same population. In terms of techniques, Kolmogorov-Smirnov or KS is a popular technique for measuring drift. There are suites of techniques under maximum mean discrepancy test umbrella that can also be applied. Alternatively, you can build a custom drift detector classifier that can predict if the feature data has drift. Coming to the practices of feature drift management, the practices we discussed in the concept drift video also accrue to feature drift. In addition, it's recommended to measure drift independently for each feature. The drift number should be compared to the correlation values of that feature to determine its impact on the target. When the correlation between a feature variable and a target is high, even small amounts of feature drift can cause concept drift. When drift is noticed, examine changes to the data processing pipelines and application changes to identify possible causes. Look for errors also in data processing. We can also use feature drift techniques to measure drift in the predicted labels. Especially for multi-class classifiers, changes in predicted class distribution may also indicate possible issues.
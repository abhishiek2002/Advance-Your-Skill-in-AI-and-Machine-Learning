Fairness in ML
- [Instructor] We will explore the area of fairness and bias in machine learning in this video. Fairness in machine learning is the process of ensuring that the model provides all individuals with equal opportunity, and does not bias or generalize based on certain segments or attribute values. ML models are notorious for picking up bias quickly if they are not trained with the right type of data. What are the types of bias that we deal with in machine learning? The most common bias is data selection bias, where not all the real world scenarios are equally sampled. For example, consider a diabetes prediction model that is built mostly using data from Europe and then using it for predicting diabetes in Asian populations. The training data would be skewed to races that are prevalent in Europe. Group attribution bias happens when behavior is generalized for the entire group based on a specific attribute. For example, an interview candidate filtering algorithm may assume that all candidates who studied in a given university have the same level of skills. Human bias happens with data scientists. Here, they would make assumptions on how a model is expected to behave, and then conclude that the model is incorrect if it behaves otherwise. Benchmarking bias happens when the benchmark model does not represent the same population as the actual model. For example, using a sentiment analysis model built on movie reviews as a benchmark for validating a sentiment analysis model for a chat bot. Both are text based models, but the context and user behavior would be different. What are some of the best practices to avoid bias? The most important one is in training data selection. Care should be taken to make sure that the training data represents all real world use cases equally. Quality of data is more important than quantity. Then comes input data validation for training and inference to make sure that there are no exceptions and skews. When evaluating model performance, it is important to look at performance by individual target classes. For example, in a diabetes prediction data set, only 10% of the records have age less than 20. The overall model accuracy may be 85%, but the accuracy for records with age less than 20, maybe just 50%. All classes should ideally have equivalent performance. Similarly, model drift analysis should also be done by individual classes to make sure that a specific target class is not drifting, even when overall drift is within thresholds. It is important to have human review and feedback from time to time, possibly using crowdsourcing techniques to see if the model is biased in a specific manner.
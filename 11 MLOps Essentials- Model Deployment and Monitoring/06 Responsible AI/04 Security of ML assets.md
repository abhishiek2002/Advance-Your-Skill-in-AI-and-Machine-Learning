Security of ML assets
- [Instructor] An important area in MLOps around responsible AI is the security of ML assets like data and models. Machine learning solutions also have threats of adversarial attacks that compromise the model as well as data used for training and inference. This is in addition to the other security threats that any other software application would face. Let's discuss the types of ML specific threats that such solutions face. There are two main assets for ML, data and model. When it comes to data, sharing across trust boundaries carries the risk of intrusion during transport, hackers can poison training data by introducing negative samples that influence model behavior, there is also the threat of data theft and break-ins which is becoming widespread these days, there is the risk of hackers reverse engineering redacted or deleted data to uncover private information. On the model side, there is a high risk of the model being stolen if it is not properly protected, hackers can manipulate the inputs to the model and influence the ML solution to behave in undesired ways, known model attacks happen when using open source models when the hacker knows the model and hence can influence its behavior by providing specific inputs, IP infringement is another area of concern where the models architecture can be copied by competitors, hackers can also reverse engineer the models architecture and parameters by looking at the relation between the inputs and the outputs. How do we protect the model and data from adversarial attacks? There should be physical security to where these assets are stored, there should be network security that prevents unauthorized users from accessing the assets; it is recommended to encrypt models and data when they are deployed on edge devices like mobile phones and routers; during training, it's recommended to sanitize data to remove individual negative samples; a number of new techniques like RONI, TRIM , and KNHT are coming up in this space; when using AutoML, perform validations on new training data to ensure that they are not compromised; block model poisoning with training data by using gradient-shaping techniques; use model restoration techniques to remove back doors to information leakage. Some of these techniques may be new to you and I highly recommend further reading on these topics.
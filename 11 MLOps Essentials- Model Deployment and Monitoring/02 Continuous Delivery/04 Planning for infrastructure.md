Planning for infrastructure
- [Instructor] Infrastructure plays a vital part in delivering a stable and resilient serving for machine learning. Let's explore various elements and best practices for infrastructure in this video. There are two types of infrastructure elements that participate in serving: compute resources, and tools and technologies. Having enough compute resources are essential for maintaining operational performance goals like latency and concurrency. First, we need CPUs and GPUs to run the models with expected latency. GPUs are important for ML and need to be managed efficiently. Then comes memory to cache models and other data. Accelerators are available today for improving the performance of the models. Storage also becomes essential with the amount of data being captured and archived. Networking should be fast and reliable. Finally, resiliency features like standbys and backups are also needed for all the infrastructure elements. On the tools and technology side, we have operating systems. Then there are application managers like Kubernetes. Gateways are needed for security and load balancing. Security capabilities are a must. Monitoring tools help monitor performance and ensure service uptime. Finally, operational analytics tools help analyze system and model performance and improve them over time. All these infrastructure elements need to be accounted for while planning for an end-to-end ML serving system. Capacity management plays a key part in provisioning the right amount of infrastructure such that operational goals are met at minimal costs. The first step towards capacity planning is creating estimates for system and service loads. Both average and peak loads need to be estimated based on expected traffic and resources provisioned accordingly. The type of rollout strategy chosen will also impact the required capacity. Autoscaling is a great technique to ensure optimal use of resources, especially when using cloud infrastructure like AWS and GCP. Finally, there has to be mitigation strategies when overload is detected. This can be building up a request queue or throttling request in real time. Today, more and more deployments happen in third-party infrastructure like AWS, GCP, and Azure. All of them have pay as you go systems. ML services can quickly overrun costs if they are not properly planned and monitored. Special attention should be given to GPUs and accelerators as they are expensive. When planning for costs it is important to consider both average and peak loads. It is recommended to monitor resource utilization over time and adjust the provisioned resources to minimize costs. Turn off unwanted resources to save costs. Cost effectiveness is a key determinant in whether an ML solution brings value to the organization. Infrastructure management drives cost effectiveness and hence a critical area to plan and monitor.
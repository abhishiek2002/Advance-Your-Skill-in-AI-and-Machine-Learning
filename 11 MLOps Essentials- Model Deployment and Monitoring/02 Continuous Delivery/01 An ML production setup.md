An ML production setup
- [Instructor] Let's begin this chapter about ML deployment by looking at the different options for deploying ML solutions in production. We first start with the most simple deployment, a notebook. This is technically not a production deployment, but a use case where the notebook is executed for inference manually. A tested and approved notebook serves as the production executable. The notebook would still run on a user desktop or a laptop. The user would open the notebook when needed, and execute the notebook. Required parameters for the execution will be provided manually. The notebook will load the model, and also the inference input data. The predictions from the model are then printed to the console. The predictions may also be persisted to a data store for future reference. The next pattern is embedded model deployment. In this case, the non-ML service runs as an executable to deliver the ML solution. The ML function and the model are embedded within the non-ML service as a function or a package. The non-ML function makes in process calls to the ML part to get inference. Clients talk to the non-ML service API to get the required services. Containers are used to deploy the non-ML service in production environments. This setup is popular when the model itself is small, and when the ML model is natively built by the same organization. The third pattern is distributed ML deployment. In this setup, the non-ML service runs as a separate executable in its own set of containers. The ML service runs its own executable service in another set of containers. The model file is usually stored externally in a data store like S3. The ML service would load the model file dynamically and cache it in memory. Clients access the ML service through the non-ML interfaces. The non-ML service would then call the API on the ML service to execute ML capabilities. This pattern is popular when model serving requires separate and significant resources, and also when the ML service is provided by third parties.
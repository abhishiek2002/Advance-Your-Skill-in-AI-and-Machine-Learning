Deployment pipelines
- [Instructor] How do we deploy an ML solution with its model ML and non ML components? Let's discuss the pipeline options in this video. We will start with deploying embedded ML services. As part of the model training, an approved model is delivered, which usually recides in a modern registry. Code for the ML functions as well as the non ML functions for the solution sit in the code report. The build process then integrates the model ML function and non ML functions into a single executable and packages it to a deployable format. The package is then deployed to a test setup and tested for sanctity and regression. After successful testing, it is deployed into production. How does deploying distributed ML services work? As in the case of the embedded services, the model and the code sit in their respective stores. However, each artifact is built and tested independently. Each of them end up being an independently deployable package that is deployed and tested in their own test setups. During testing, they will be tested against the corresponding versions of the other packages, which are expected to work together in production. Deployment again is done independently. One of the biggest considerations while deploying distributed ML services is compatibility between the multiple artifacts in the distributed setup. Services are deployed independently of each other so they may have to work with older versions of other services. There are multiple ways to ensure compatibility. The deployment of various packages can be sequenced in such a way that backward compatibility is insured in that sequence. For example, the model is deployed first and then the ML service. The old ML service then should be able to work with a newer version of the model until it itself is upgraded. Alternatively, all integrations can be done using specific versions. Clients can access APIs using API versioning. The newer version of the API service will still support the older version and its functionality so older clients will continue to work. Similarly, models can be versioned and deployed concurrently where a specific ML service version can access a specific version of the model.
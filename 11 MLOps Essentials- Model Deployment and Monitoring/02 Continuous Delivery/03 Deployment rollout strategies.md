Deployment rollout strategies
- [Instructor] Deployment rollout strategy for ML applications is the same set of strategies that are used for non-ML applications, also. Let's begin to discuss this with a general guidance on rollout. The rollout strategies we are going to discuss applies to each artifact, whether it is embedded ML or independent ML applications with different pieces being deployed separately. In general, choose a strategy that best suits your application use case and resource constraints. But ensure that compatibility requirements and issues are taken care in all situations. The first strategy is the recreate strategy. In this case, the existing versions of the services running in production are stopped and uninstalled. Then, newer versions are installed and tested. Then production resumes. This is the older strategy, followed from the early days of software, and is simple to implement. But this requires a service downtime, which may be acceptable in some use cases. This strategy is suited for batch processing applications, where clients can be stopped from accessing the services for some time. Next comes the rolling deployment strategy. This is suited for N+1 deployments, where multiple instances of the service are run on independent nodes. Here, each node is upgraded one at a time. After upgrading each node, it is tested for stability. If issues are found, the node is rolled back to the previous version. The overall service stays online during deployment, as the other nodes share the load. This is suitable for APIs and stateless microservices. The biggest concern of N+1 is ensuring backward compatibility with clients, as nodes with current and new versions are active at the same time. Shadow deployments is another strategy that is useful when new version stability is a critical requirement. In this case, a separate instance, or cluster, is set up with the new version of the service. This is called the shadow. It is on a separate set of nodes, with the current ones not being disturbed. The production traffic is duplicated and sent to both the old and the new cluster deployments. The shadow is then validated for stability and performance, while the clients continue to be served by the original deployment. On successful testing, the new cluster is switched to production using gateway switches, and the older version is retired. During shadow deployment, double the resources are needed to run the shadow cluster, so there is a cost impact. Canary deployments have become popular recently to test newer versions before they are fully rolled out. In this case, a separate cluster, called the canary, is set up with the new version. Part of the production traffic, say about 10%, is sent to the canary. Canary is then validated for stability and performance. If there were issues in the canary, the new deployment is rolled back. On successful validation, the new version is deployed to all other production nodes. Additional resources are needed for canary during deployments. Next comes blue/green deployments. In this type of deployment, a new cluster is set up with its own node, with the new version of the service, similar to shadow deployments. The gateway, or the load balancer, is used to switch traffic from the old to the new version. The old version is still running, but receives no traffic. If issues are found, the gateway is then switched back to the old version, else it is retired. The advantage of blue/green deployments is the ability to quickly go back and forth between the two versions, but additional resources are needed to run the new service cluster.
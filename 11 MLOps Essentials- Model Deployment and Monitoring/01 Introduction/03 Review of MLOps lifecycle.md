Review of MLOps lifecycle
- Let's quickly review the MLOps lifecycle before jumping into the deployment and operations aspect of it. MLOps is a set of practices that help manage the creation and use of ML artifacts through efficient workflows, collaboration, and tracking. MLOps is not a specific product or technique. It is a set of processes and best practices to build and run ML supported by automation and tools. What are the goals of MLOps? MLOps extends the DevOps methodology to building and serving machine learning solutions. It integrates the activities of data engineering and model development into the software engineering and deployment life cycle. In addition to the software engineering artifacts of code and records, it manages the machine learning artifacts, data, and models. It enables continuous model development and integration thus following an agile process to reduce time to market. MLOps deals with model deployment and serving. It also includes monitoring, performance analytics, and gathering feedback for further improvements. It helps manage the ML processes through automation and tools to improve efficiency. What does the MLOps life cycle look like? It looks very similar to the DevOps lifecycle. There are three groups of activities here. The software engineering and operations groups are borrowed from DevOps. Additionally, there is the machine learning group also. Let's look at the various activities and how they integrate together. The process starts with defining the requirements for the ML project and a corresponding design. The design would include both non-ML parts like APIs, services, databases, UIs, et cetera, and ML pipelines like data engineering pipelines. This is then used to develop the non-ML parts of the overall solution. The requirements would also then feed into the data engineering for converting raw data into useful features for ML. Then there is a continuous training cycle where a model is built and refined until it meets the stated requirements. Models that are built or managed under a model governance framework. As models are built, they're also integrated continuously with the non-ML code. Continuous here would be specific small intervals like each sprint or each week. And passing integration, the model and non-ML code are packaged together and delivered. Now the operations process kicks in. Continuous deployment takes care of deploying the upward packages to production. This is then served to users. The performance of the model is monitored to ensure that it stays within the expected thresholds. Model performance information, as well as the model drift and bias information is fed back into model governance for tracking and evaluation. Input is also provided into requirements to see if changes or improvements are needed on the ML or non-ML functions. Finally, feature and label data from production is captured and fed into the data engineering pipeline to create new data sets. If model governance determines that the model needs to be retrained, then it kicks off another training cycle with new data. Having seen its life cycle, let's now jump into model deployment and operations.
Serving multiple models
- [Instructor] Let's look at scenarios where multiple models are used in an ML solution. An ML solution may use multiple related models to provide overall user experience. Solution design and deployment should consider this case and optimize across all models. There are multiple ways in which models can be deployed together. Let's review some of the popular configurations. First is the chained models pattern. Here models are chained in sequence. The output of one model becomes the input to the next model. For example, let's consider a chatbot that is answering questions asked by the user. After the user enters their query, the first model would try to understand the context or intent of the user. This is then provided as input to the next model, which would extract information that is relevant to the context. Finally, the information and context are provided to another model that will pull out the best answer to the question. The next use case is single-input independent models. In this case, models are independent of each other but will use the same input. The input is provided to all models in parallel and they work concurrently to deliver the outputs. For example, consider an online review analysis system. The input here would be the user review. This input is passed to three models in parallel, namely sentiment analysis, topic extraction, and obscenity detection. The output from all these models are then collected together and stored. The third option is that of alternate models. In this case, the same input goes to all the models. But based on a condition, only one of the available models is chosen. This is like an if/then/else structure. For example, consider a language translation solution which can take input in any language and translate it to English. The first model will be a language detector. Based on the output of the language detector, only the corresponding translator will be invoked. So if the text is in French, only the French translator will be invoked. What are some of the best practices for multiple model deployments? It is important to test these models together as a solution. Issues in one model may impact other models or overall deployment goals. Parallelize models as much as possible in the deployment to reduce overall latency. Even if the input to the model is the output of a previous model, validate that input to make sure it conforms to the requirements of the model. You can exit these prediction chains early. For example, if the intent detector cannot detect the intent with a given level of accuracy, we don't need to invoke the other downstream models. Scale each model independently as each may have different resource profiles. Always perform end-to-end latency measurements to ensure that the overall user experience is not impacted.
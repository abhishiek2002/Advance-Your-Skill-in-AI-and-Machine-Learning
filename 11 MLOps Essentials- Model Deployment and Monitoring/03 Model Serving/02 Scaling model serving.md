Scaling model serving
- [Instructor] Scaling ML in a cost effective manner is a critical success factor for MLOps. Let's discuss some scaling options for ML in this video. Scaling for batch inference is different from scaling for real time inference. The table here shows several considerations where batch and real time are different when it comes to scaling. To begin with, the goal of batch inference is throughput, like the total number of predictions done in a given time period. Real time inference, on the other hand, focuses on concurrency and latency. The number of concurrent requests that can be processed by a given compute unit and the time the client waits to obtain the results are key measures. When it comes to resource provisioning, average loads are used as the capacity benchmark for batch processing, while peak loads are used for real time inference. Auto scaling is used in real time to optimize resource allocation. How does back pressure gets handled? In batch, as long as the processing velocity is greater than the incoming velocity in a given period, it is sufficient. There can be spikes and delays, but as long as they catch up within an interval, it should be good. For real time, it's more serious. For synchronous API requests, throttling and timeouts may be used to drop requests during peak periods. For asynchronous stream processors, queues may be used to handle back pressure. Finally, for cost, the focus in batch inference is to keep them low. While in real time, it is essential to meet the performance goals first before thinking about cost. The options available for scaling ML services are similar to those for non-ML services, except for a few items. Horizontal scaling of services can be done by deploying ML services in n plus one fashion. For batch or stream processors, this can be the number of node for a given task. For APIs, it could be the number of parts for a given service. Vertical scaling in ML can be done by using GPUs or TPUs to increase capacity within a node. Auto-scaling is a key feature to handle request spikes. Result caching is another option to explore. For the same set of input values, the model will produce the same output. So if multiple instances of the same input set happens frequently, then caching these results can help avoid a request to the ML service altogether. The promise of GPUs and TPUs for inference is a hard topic in ML labs. But before using them for inference, it is important to understand their advantages and drawbacks. GPUs are optimized for parallel math operations, they offer low computational latency and can be many times more powerful than CPUs, they provide parallel processing capabilities to handle multiple requests at a time, they are significantly more expensive than using CPUs. Let's look at some of the best practices for GPUs. Use them only when they are absolutely needed, it may be cheaper to use multiple CPUs to share the load than a single GPU, look for use cases where they are more cost effective than CPUs, batch multiple requests for performing inference as GPUs are optimized for that, it is recommended to do performance benchmarks between CPUs and GPUs for the use case to understand the cost benefit aspect and make deployment decisions.
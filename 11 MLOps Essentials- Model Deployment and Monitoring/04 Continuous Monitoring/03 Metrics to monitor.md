Metrics to monitor
- [Instructor] What are the various types of metrics that are recommended to be monitored for MLOps? Let's review them in this video. Let's begin by looking at the system and infrastructure metrics that need to be monitored. This list is the same for both ML and non-ML services. For CPU, we want to monitor utilization levels and usage trends. For memory, key metrics to monitor are heap utilization and thread counts as increasing levels can lead to memory leaks and process crashes. Garbage collection is also under the metric to keep watch. For networking, we look at latency to make sure that it's within acceptable ranges. Jitter and packet loss tells us if there are issues with communication, resulting in multiple retries. Disks are also an important resource. Disk activity and queuing for disk need to be monitored to make sure that it is not a blocking issue. Next comes application metrics. There are two types of metrics. There are service metrics that are applied for all types of services, and business domain-specific metrics. In service, we typically monitor latency and concurrent sessions to understand the load on the system. Errors and failures indicate if the service is working correctly. Max queue size is applicable when requests are being queued for processing. Availability measures overall service availability across the cluster, even if individual nodes go down. Average response size measures the network load when messages are exchanged between services. Some domain-specific measures would be orders processed per hour for order processing system. Click-through rate for web applications. Approval rate for credit application processing. Thumbs-up percentage for recommendations. And self-service rate for chatbots. Then comes machine learning-specific metrics. These metrics are constrained by the amount and type of data available, especially if true labels are available from production. Performance metrics for models include accuracy, F1-scores, type I and II errors, precision, recall, et cetera. Computation of these metrics would depend upon the availability of both predicted and actual labels. For model drift, the typical measurements are drift rate, out-of-distribution percentage and P-value. We will discuss drift in detail in the next chapter. There are also responsible AI-related metrics, like fairness score and correlation. Responsible AI is discussed in one of the future chapters.
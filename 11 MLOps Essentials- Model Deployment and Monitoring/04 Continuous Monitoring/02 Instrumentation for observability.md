Instrumentation for observability
- [Instructor] Instrumentation is the first vital step to ensuring a good foundation for observability. Instrumentation refers to adding capabilities to systems and applications to track and capture information that can be used to observe the behavior and performance. Instrumentation deals with adding hooks into software that enables this tracking. Let's begin with instrumentation at the system level. What kind of data is captured here? We want to track CPU usage for the container and by individual executables. Similarly, we want to track memory usage. Disk I/O stats are essential to understand storage load. Network traffic information helps understand the performance of the network. Request/response statistics are collected in API gateways to understand the incoming requests and their responses. Error statistics at the system level will monitor any exceptions with resource access and handling. To capture this information, we depend on infrastructure software. Tooling at the operating system level helps understand compute resources. Runtime platforms like Docker and Kubernetes also help track system performance. API gateways help in tracking information about incoming requests and the responses returned. Moving on to the application. We try to capture information about activities that are specific to the application domain. We want to capture inputs and outputs for a given service or function. For batch or stream processors, we want to capture processing results. Usage of other services, like third-party services are also tracked for accounting. Errors and exceptions encountered by the executable are also logged and tracked. Debug trails help developers in troubleshooting. Audit trails help in monitoring access to specific resources and data. In order to capture application-specific data, there are multiple tools available. We can have custom application code to capture and log the information. Programming languages support logging capabilities that can be leveraged. Logging libraries make the job of logging easier. Nowadays, transparent interceptors and aspect-oriented programming helps track information without explicit coding from developers. We then get to instrumenting the model. We want to capture raw input features and transformed features for the model in production. Predictions generated by the model are also tracked along with the input. Whenever possible, the true labels for the predictions are captured and tagged against the input data. Operational metrics like model latency is also measured for each of the requests. For model tracking, typically custom application code is used to log data into monitoring stores and queues. ML libraries today also support the capture of this information with minimal work from the developer. For true labels, user input can be used to capture if the prediction was correct or not. As seen in this video, there is an array of information that needs to be instrumented for. We will discuss how this information is used in the following videos.
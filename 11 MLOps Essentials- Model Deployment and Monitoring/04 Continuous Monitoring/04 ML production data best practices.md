ML production data best practices
- [Instructor] As models in production do inference, they also produce significant amounts of valuable data, usually referred to as ML production data. We will discuss some best practices in capturing and managing this data in this video. ML production data contains multiple aspects of inference, usually linked together by specific identifiers. It contains both raw and transformed model inputs. It contains the model outputs or predictions. It also has statistics about the inference, including latency, and any confidence or error measures. True labels may be available in some cases. Sometimes they are manually collected from users. The production data set is valuable for monitoring the performance of the model, as well as serve as new training and testing data for model improvements. However, this is significant data that needs to be captured and processed, and creates significant load on the serving setup. Here are some recommendations for managing ML production data. For capturing data, use asynchronous and non-blocking methods so that the main inference pipeline is not blocked in trying to capture, store and publish production data. It's recommended to cache the data locally in disk, and then use store and forward to consolidate them in the monitoring hub. There are rarely any real time uses for ML production data, so batch processing is mostly sufficient. Privacy of data is a significant risk, so it is recommended to use techniques like reduction and anonymization before the data is stored in the central hub. It is also recommended to use independent processors for different metrics following microservices patterns.
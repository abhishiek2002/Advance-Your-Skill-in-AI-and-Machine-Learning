The monitoring pipeline
- [Instructor] We will discuss monitoring aspects of MLOps in this chapter. We will begin with a discussion around various components of a monitoring pipeline. Observability is a critical component of a DevOps system and that extends to MLOps too. An end-to-end observability strategy across system, application, and model behavior is critical to MLOps. Implementing effective monitoring and analytics is key to the successful operation of machine learning. We will review a template monitoring pipeline and its components here. This is a general architecture and the specifics of technologies used may vary based on the product choices for monitoring. All services are typically installed on containers or virtual machines. The ML service would itself be deployed in a similar fashion with the model cached inside. A monitoring agent would also run in each container. This agent will collect telemetry data periodically and send it over to a telemetry queue. Data like resource utilization, application statistics, logs, errors and audit trails may be collected. The telemetry queue consolidates this data across multiple containers in a cluster. A central monitoring hub then consumes and manages this data. A data processor or a metrics computer would then read the telemetry data and compute various operational metrics at regular intervals. They are then stored in a metrics database. An analytics service would then provide reports and visualizations on this metrics data. An alerting service can be used to send alerts when specific thresholds are exceeded in the computed metrics. So far, the pipeline is the same whether this is an ML service or a non-ML service. For the model, we will need to collect additional data about the inputs received by the model, the predictions generated and other data, like confidence levels. This is locally stored in a persistent store and is periodically sent over to the monitoring hub. Here, data is first cleaned to remove errors and anonymized for sensitive data. This is then stored in a central production ML data database. Additionally, if the true labels are also available, they are also tagged to the same data. A ML metrics computer would then compute ML-specific metrics, like accuracy and out-of-distribution rates. This data is appended to the metrics database. Similarly, a drift and responsible AI analyzer will run on the same data and update the findings in the same database. The analytics and alerting services would then be used on ML-specific data also. We will explore these services in detail in the rest of the chapter.
Detecting concept drift
- [Lecturer] What techniques are available to detect concept drift? Measuring concept drift is a straightforward activity. For these, we need to instrument the machine learning services to collect predictions made by them and also the associated ground truth. Ground truth is the actual values for the predictions made. For example, if the model predicts that a customer has a high propensity to buy a product, ground truth is the validation that the customer actually bought the product. With predictions and ground truth, we can then compute scores like accuracy, F1, precision, and recall. Then we compare the absorbed values of scores between production and training. We need to compute these scores periodically, say every month, and analyze if the score stays stable or if they deviate. Concept drift happens if there is a significant deviation that persist over multiple time intervals. Concept drift is the best measure to understand if the model is not working to expectations. But there are serious challenges in measuring it. Concept drift requires the ground truth for inferences that are made in production. But ground truth labels are not available ready-made in most of the cases, hence, we cannot compute any of these metrics for drift. There are some options that exist for generating ground truth. Sometimes the downstream processing activities may provide the ground truth. For example, if we are predicting if a customer will buy a product, we can validate that once the customer finishes their shopping experience. We can use manual human labeling where a person can look at the information and provide the ground truth labels. This can be very expensive. There are also automated labeling tools today that help in mass labeling, but they suffer from accuracy challenges when compared to human labeling.
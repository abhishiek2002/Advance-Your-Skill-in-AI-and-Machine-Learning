Detecting feature drift
- [Instructor] Having discussed detecting concept drift, let's now discuss the techniques for detecting feature drift. There are exploratory techniques for analyzing if feature drift is happening for a given feature variables. In this case, we compare the feature value distributions between the training and production datasets. For categorical features, we can use histograms and pie charts for visual inspection. For continuous features, we can use quartiles and box plots. In the chart, we compare the distribution of product classes between the training data and production data. We see that printers have increased in production while keyboards have decreased significantly. Now, this may or may not lead to a corresponding concept drift. How do we measure drift over a large number of feature variables? There are a few popular techniques used. Popular machine learning libraries support computation of these measures out of the box. All these techniques require the training dataset and the production dataset to compare class distributions. Chi-square test is a popular legacy technique used for feature drift. Other techniques include the Kolmogorov-Smirnov test, or KS test, and maximum mean discrepancy, or MMD. Several variations of these techniques are also possible. You can also build a drift detection classifier or regressor that can predict if drift exists between these datasets. Building a custom classifier would require investments in creating and labeling training datasets, though.
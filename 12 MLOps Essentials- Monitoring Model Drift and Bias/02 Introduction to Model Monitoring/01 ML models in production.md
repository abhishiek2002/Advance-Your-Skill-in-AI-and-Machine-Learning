ML models in production
- Before we dive into model monitoring aspects, let's quickly review some background on machine learning model deployments in production environments. How are models typically deployed in production? Models are typically hosted by a wrapper or API service. This service may be custom built or user available third party hosting services like TorchServe or TensorFlow serving. Users interact with the model through intermediate services. These services may be graphical interfaces or APA-driven interfaces. They may exist in the business layer and perform other business functions too. Intermediate services pass the required inputs to the model and receive outputs or predictions from the model. ML services may be deployed in N+1 configurations for scalability and performance. Deployment to production systems typically happen using CI/CD pipelines that test and automate these deployments for the models and associated services. Models and associated services usually have several levels of instrumentation for data collection about performance and associated observability capabilities. Let's look at an example ML service deployment pattern. Other deployment patterns are also used. In this pattern, the non-ML services, like user interfaces or business layer services, are deployed as a separate service in their own containers. The ML service itself is deployed as another service in its own containers. The ML service only takes care of pre-processing model inputs, predictions, and any post-processing needed before returning the predictions. The model file is in a portable format, like pickle, and is stored centrally in a file store. The ML service loads the model dynamically as needed and uses it to serve requests. It may also cache the model for handling future requests. Clients access the model through the non-ML interfaces, which in turn route the request to the ML service. There are several variants of this deployment possible.
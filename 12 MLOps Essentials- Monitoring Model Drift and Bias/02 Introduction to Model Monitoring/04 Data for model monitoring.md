Data for model monitoring
- [Instructor] What data do we need to instrument for and capture to enable monitoring for model drift and bias? Here is the list of key model monitoring data elements. Raw input features that are received by the ML service should be collected in its original form. This becomes the reference data for future comparison and training. Input features may undergo transformation, like one hot encoding and bending during pre-processing. These also need to be collected as they are the direct input to the model. The predictions or outcomes from the model of course need to be collected, and linked to their corresponding features. If true labels can be acquired either through human feedback or explicit labeling, they also need to be collected, and linked with its corresponding prediction. Prediction stats, like confidence levels or probability scores also need to be collected. Finally, latency of the model should be collected for future reference. There are several ways in which this dataset can be collected. Custom application code inside the ML service can be used to collect and propagate data. Popular machine learning libraries also provide ways to collect this data in multiple files. Finally, user input can be used to collect true labels, and validate if the predictions are accurate.
Metrics to monitor
- To identify and overcome problems with machine learning models in production, we first need to understand what is going on. For that, we need metrics. What kinds of metrics are collected in a typical machine learning deployment? Let's begin by looking at the system and infrastructure metrics that need to be monitored. This list is the same for both ML and non-ML services. For CPU, we would monitor utilization levels and usage trends. For memory, key metrics to monitor or heap utilization and threat counts as increasing levels can lead to memory leaks and process crashes. Garbage collection is also another metric to keep watch. For networking, we monitor latency to make sure that it is within acceptable ranges. Jitter and packet loss tells us if there are issues with communication resulting in multiple retries. Disks are also an important resource. Disks activity and queuing for disk needs to be monitored to make sure that it is not a blocking issue. Next comes application metrics. There are two types of metrics. There are service metrics that apply for all kinds of services and business domain-specific metrics. In service, we typically monitor latency and concurrent sessions to understand load on the system. Errors and failures indicate if the service is working correctly. Max queue size is applicable when requests are being queued for processing. Availability measures overall service availability across the cluster, even if individual node go down. Average response size measures the network load when messages are being exchanged between services. Some domain specific measures would be orders processed per hour for order processing systems, click-through rate for web applications, approval rate for credit application processing, thumbs-up percentage for recommendations, and self-service rate for chatbots. Then comes machine learning specific metrics. These metrics are constrained by the amount and type of data available, especially if true labels are available from production. Performance metrics for models include accuracy, F1-scores, type I and II errors, precision, recall, et cetera. Computation of these metrics would depend upon the availability of both predicted and actual labels. For model drift, the typical measurements are concept drift, feature drift, and correlation. There are also responsible AI-related metrics like fairness score and correlation. Monitoring these measures are important to ensure that the models are performing to expectations when deployed in production. In this course, we will focus on drift and fairness monitoring. Let's explore these in detail in the upcoming chapters.
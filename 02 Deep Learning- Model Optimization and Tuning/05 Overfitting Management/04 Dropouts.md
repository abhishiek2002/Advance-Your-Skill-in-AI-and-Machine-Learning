Dropouts
- [Instructor] Another popular technique used to reduce over fitting is called dropouts. Dropout works during forward propagation. By default, during forward propagation, the output of each node in the layer is sent to every node in the next layer. When using dropout, the outputs of some of the nodes in the layer are dropped randomly. During training, we can provide a percentage value to control the number of nodes being dropped. Dropping nodes randomly tends to result in focusing on feature values that have a high influence on the outcomes. But dropouts can also negatively impact the model if they drop relevant nodes that model important features. Try dropouts if you see symptoms of over fitting and use a percentage that provides the similar accuracy for both training and test data. Sometimes having no dropouts may provide the best results too. In the next video, we will experiment with dropouts.
Tuning backpropagation
- [Kumaran] We will tune the optimizers and learning rate for the root cause analysis model in this video. Let's start with optimizers. We will try with all the four popular optimizers, namely SGD, RMSprop, Adam, and AdaGrad. Let's run the experiment and review the results. Other than AdaGrad, all other optimizers seem to perform equally. We will go with RMSprop. It could have been any of the other two, too. Next, we go to learning rates. We will experiment with five values, as shown here. Let's run the experiment and compare the results. The smaller three values seem to provide equal and best results. So we will choose RMSprop for optimizer and 0.001 for learning rate for this exercise.
Attention in language generation and translation models
- [Instructor] You now have a big picture, into the understanding of how attention may help you produce better results in neural networks. Now, let's get a little more detail and talk about attention in language generation and translation models because this is where they're most widely used. Let's say you have a language translation model that translates the English language to the German language. One way this model could work would be for the model to read the entire input sequence in English, understand its meaning, and then produce the equivalent sentence in German. This is how a model that does not use attention would perform language translation. Let's dig a little deeper and see how language translation models are set up. Language translation models operate on words one word at a time. These models are sequence to sequence models. That's because we feed in the sequence of words at the input, and then you get a translated sequence of words at the output. The way sequence to sequence models work is that one word is produced at the output at a single time instance. After reading in the entire input sequence, the first word of the output is produced. That word is then used to generate the second word at the output, which is then used to generate the third word of the output, and so on. The entire sequence of the translated sentence is generated. In order to generate a single word in the output translation sequence, the model looks at the entire input sentence. It uses the information from the sentence as a whole and it also uses the previously generated word in the translated sequence. The input sentence is considered in its entirety and an additional input is the previous word that was generated in the sequence. Now, this could be improved upon. Certain words can be better predicted if attention is focused on portions of the input rather than the entire input sentence. For example, the word ich would be best predicted if attention was focused on the word I. And you can see similar relationships with other words in this translation. The prediction of words in the sequence would improve with attention. The relationship could be more complex than one to one. There might be a single word at the input that is related to multiple words at the output and the reverse may also be true. Multiple words at the input may be related to one word at the output, so it's not just about focusing attention on a single word of the input. Attention can help focus on the right bits of the input to generate a particular output. And this is what attention based models do. They allow neural networks to focus on the most important features of the input thus reducing better results at the output.
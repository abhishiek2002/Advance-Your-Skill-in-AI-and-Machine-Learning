What are attention-based models?
- [Instructor] So what exactly are Attention-based Models, and how do they help? Well, let's understand this with a few examples. Starting off with a very simple text summarization example. Let's say I were to ask you to summarize this text and write two or three sentences of what this was about. One technique that you would follow could be to read the entire paragraph, parse it, really understand it. And then using the information that you've understood, create a two or three line summary. Well, that actually seems rather daunting. When I looked at this text, I couldn't understand anything that was going on. But let's say you were to break things up into more manageable chunks. What if you focused your attention on parts of the text at a time? So you created summaries looking at only one part at a time. Now you can see this first part here, you could summarize just this sentence. You could say "Grammar is complex and nuanced." That would be your summary for the first part. Then you would turn your attention to the second part. And then you would talk about the complexity of grammar that it exists across languages and it's not just in English. And finally, you'd turn your attention to the last part of this text. And you'd say "If grammar is universal, who created it?" That is a summary of these last few sentences. You'd put all of these sentences together to produce a three sentence summary of this paragraph. And this, of course, made your task a lot easier. It's pretty clear that by allowing you to zoom in and focus attention on parts of the text, helped us improve the result that you produced. Now, let's try this once again this time for an image. Your task is to generate a caption for this image. Now, if you look at the image as a whole, it's really hard to tell what exactly the image is about. What are the important parts of this image? What are the unimportant parts? In order to be able to generate captions, it's far better to focus attention on important bits. So you'd zoom in on the horse, the beach, maybe the waves, maybe the houses in the background. Human beings are trained to focus attention on the relevant bits of the image to generate captions or descriptions. A part of focusing attention on the right bits is to ignore the irrelevant or the less important portions of the image. They maybe irrelevant, so you don't actually look at them or pay attention to them. You then use the relevant portions that you've identified to generate your caption. So you might say, "Okay, there is a man on a horse here." Then you'd look at a different portion of the image, "The man on the horse is on a beach." And then you'd look at the background and say, "There are houses in the background." The use of attention allowed you to focus on the important bits of the input. And this is what attention does in your neural network models.
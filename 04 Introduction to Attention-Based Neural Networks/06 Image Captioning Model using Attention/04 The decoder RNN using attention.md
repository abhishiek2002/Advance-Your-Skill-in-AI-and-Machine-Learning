The decoder RNN using attention
- [Instructor] After the encoder and the attention model it's now time to look at the decoder network. This is the network that uses the attention model to generate captions. While initializing the decoder, you need to specify a number of parameters, the size of the embeddings for the words, the size of the vocabulary, the size of the attention, the size of the hidden state of the encoder and the size of the hidden state of the decoder. These will all be inputs to the decoder R and N, and we also have the dropout probability set to 0.3. We'll use dropout in the final linear layer. Initialize some of these parameters we'll need them later. We'll cap size, attention size, and decoded hidden size. The words of the caption input to the decoded will be represented in the form of one hot vector. We'll use the embedding layer to generate embeddings for these words. Next, we initialize the attention model. The attention model requires the size of the hidden state of the encode, size of the hidden state of the decoded and the attention size. Init_h and init_c are linear layers that take in the feature representations which are the output of the encoder, and convert them to a hidden state that can be fed into the decoded. The layers of our recurrent neural network will use the long short term memory cell. The LSTM cell defines a single layer in our recurrent neural network. When we use this particular construct we have to unroll the R and N ourselves over time. And you'll see that in the forward path through our model. The input to the R and N at each time instance is a concatenation of the embedding of the current word in the sequence and the context vector. So that's why the size of the input is embedding size plus encoding hidden size. The hidden state of the R and N has the size, decoder, hidden size. That's the second input argument, and bias is true. Bias true specifies that the layer should use bias weights. We then have a fully connected linear layer at the end. This is the linear layer that outputs a one hot vector for the predicted word. In addition to the linear layer, we'll also have a dropout layer during the training process with the dropout probability that we have specified. Let's take a look at a forward pass through this decoded R and N. The input here is a batch of feature representations of the input image along with the corresponding captions. The caption text is represented as one hot encoded vectors. Please convert these to embeddings using the embedding layer. Let's get the first hidden state of the decoder by passing the encoder features through the linear layer that we had set up for this hidden state. Init_h and init_c. Let's initialize the length of the sequence. Which is the entire text, other than the start of sentence, the SOS token. That's why length of captions are zero minus one, and the batch size. That is the number of captions that we're dealing with in one batch. If you remember the feature representations that are the output of the encoder have this particular structure. Batch size 49. That is for the seven by seven feature maps and there are 2048 feature maps. Let's extract the number of features that we have, which is essentially 49. We'll now use this information to initialize an array that will hold the predicted captions in the form of one hot encoded vectors for all of the images in this batch. We'll also initialize another array that will hold the attention weights. We have a different set of attention weights for each time instance in the sequence. That's why we have the sequence length here in this array structure, and we have the attention weights for all of the images in a single batch. We'll now define the loop that'll feed in the input at the different time instances, along with the context vector and other details to our decoded R and N. For each time instance, for the entire range of the sequence, we first pass in the features from the encoded convolution neural network and the decoded hidden state. Get the attention beat and the context vector. Remember that the attention weight will be different for each time instance. That's why this has to be computed for each iteration of the loop. The input to the lstm cell that is a single layer of our R and N, is a concatenation of the embeddings representing the word at this particular time instance, along with the context vector that determines the current attention. We pass this input along with the previous hidden state of our decoder R and N, and that'll give us the hidden state for the next time instance. Which we store in h and c once again. To get the final predicted word we pass the hidden state through the dropout layer and the fully connected layer, and we basically save the predictions of this model in the preds array and the attention weight in the attention weights array. In this after we iterated over the entire sequence length, the preds and the attention weights array are what we returned from this forward pass.
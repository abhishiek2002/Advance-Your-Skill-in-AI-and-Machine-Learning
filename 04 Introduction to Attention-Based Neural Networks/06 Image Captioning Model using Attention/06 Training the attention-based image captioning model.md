Training the attention-based image captioning model
- [Presenter] Now that we have implemented our encoder, the attention model, and the decoder, we are ready to set up our sequence to sequence model, in order to generate image captions. So, here is the model, it's called Image to Captions. Initialize the model with all of the input parameters that it needs. These are the input parameters that are needed to initialize the encoder and the decoder. The encoder is straightforward. It doesn't need any of the input config parameters. The decoder requires all of this information: embedding size, vocab size, attention size, and the size of the hidden states of the encoder, as well as the decoder. The forward path takes in a batch of images and the corresponding captions. We then pass the images through the encoder, in order to get the feature map representations. The features output of the encoder are then passed to the decoder, along with the captions. We can now initialize the parameters of our sequence to sequence model. Get the vocab size, that is simply equal to the length of the vocabulary of our training data. The embedding size is 300, attention size is 256. All of these are parameters you can tweak. The hidden size of the encoder is 2048. The decoder hidden size is 512. And, the learning rate that I've chosen here, is three multiplied by 10 to the power minus four. Using the parameters that we've initialized, I'm going to set up the image to captions sequence to sequence model. Now, I want this model to be trained on the GPU so I save the model weights to my coder device. Once again, we use the CrossEntropyLoss function and the Adam optimizer. This is exactly the same as in the previous demo. Initializing the image to captions model will download the weights of our resnet50 pre-trained model. So, our model is now set up. I have this additional utility function called save_model, that checkpoints the model's parameter, after a certain number of iterations of training. So, we just save out all of the model details. This serves as a checkpointing utility. Observe, I call torch.save to save out the model. The training process is the same as before, so we can go through this quickly. We'll train for 10 epochs, and we'll print out some details every hundred batches. We have our nested for loop here, iterating over the epochs and batches of our data set. For each epoch, transfer the training data, images, as well as captions, to the GPU, so that training occurs there. Zero out gradients of the optimizer. We'll do this for every batch. Pass the image, as well as the captions, through the model. That will give you the output, as well as the attention. Get the actual caption training data. We'll use this in computing the loss function. Reshape the output and the targets, so that they match, and compute the cross entropy loss. Loss.backward will compute gradients. Optimizer.step will update the model weights for this pass. Once again, after every hundred batches of training, I'll print out a few details of the model. I'll take a sample image from the next batch of data and I'll pass that to the encoder of the model. Get the features, pass the features through the decoder, and generate captions for this image. Once we have the generated caption and the image, let's display it, to see how the model performs after a little bit of training. Make sure you switch the model back to train mode after evaluating the model. Now, finally, I save the model for after every epoch of training. That's just a checkpointing feature. Go ahead, hit shift enter, and start the training process. You can see in our attention model, after just hundred batches of training in the first epoch, the model has already started generating coherent sentences. The image has a few boys. The model thinks they are men. Well, that's not too bad. Then, here is the next caption generated. A man in a red shirt. Well, there is a man, but no red shirt. Again, not too bad. And again, here is the next caption, from the very first epoch of training. A man in a red shirt is jumping on a rock. There is a rock, and there is a man, but no red shirt, and no jumping. The captions are already far better than the captions that we saw for our model without attention, even after 10 epochs of training. Here is one. There are two dogs playing with a ball in the grass. That's perfect. This is what we get after three epochs. Let's run training for a little longer, and then after 10 or so minutes of waiting, at epoch six, I get this amazing caption. A dog is running through the snow, which is perfect. And then, finally at epoch 10, you can see how the captions improve. In the meantime, I get this: A little girl in a pink shirt and a blue shirt is sitting on something. This is a fairly complex caption. It's not perfect, but it's pretty good. You can see that our model with attention performs far better than our model without attention.
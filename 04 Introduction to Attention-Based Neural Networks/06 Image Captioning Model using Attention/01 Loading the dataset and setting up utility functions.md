Loading the dataset and setting up utility functions
- In this demo, we'll see how we can improve our image captioning model using attention. We'll use Bahdanau additive attention. We'll get our decoded R & N to pay attention to important portions of the input image when it generates the captions for that image. Now, a lot of the code is exactly the same as before, you can see all of the import statements are the same. We need to mount our Google Drive folder so that our data set is accessible within this notebook. The code for that is exactly the same. Once we mounted Google Drive we'll extract the images and captions from the zip file and take a look at the captions' text. Since you've seen this code before I'm going to quickly run through all of this code. Just hit shift, enter on all of the cells. We'll use the G-Q for training so that training can run within 30 to 50 minutes. I've already enabled the G-Q run time for this notebook. As before, we have the tokenizer to tokenize the text in the captions and the counter to keep track of the frequency of the words in the training data. We'll see how the tokenizer works. You can see that it extracts the individual word. It also pre-process the words by converting all of the text to lowercase. We iterate through all of the captions in the training data to generate a vocabulary. We'll use only those words, which have occurred a minimum of five times in the training data frequency equal to five. As before, we'll add in the special tokens to our vocabulary, the unknown, add start of sentence and end of sentence token. We'll insert each of these and we set the default index to the unknown token exactly as we did before. Next we have the data set class. Now the core for this class is unchanged. I don't really need to go through this again. The data set class accesses the images and the corresponding captions and converts them to a denser format to be used by the data loader. It also converts the captions text to be represented in the one-hot and coded format. We have the same callable class generate batch captions that is used by the data loader so that the caption text in each batch of training data has the same length. This is done by padding the sentences in the caption text so that within a batch, the length of each caption is equal to the length of the longest caption text in that batch. We have the same utility function to display an image and the corresponding caption for that image. The transforms that will apply to the input images are the same as before. Remember, image sizes have to be at least 224 by 224, so that the pre-trained resonant model can use those images. We instantiate the flicker data set, and once we have that, let's quickly confirm that we can access the individual images and call display image on them. Here's an image at random. You can see the image and the corresponding caption. Every caption starts with an SOS or start of string token and end with an or EOS of string token. Next, let's set up our bad sites. That is 128. Our bad index is the index of the pad token in our vocabulary, and let's instantiate the data loader. The data loader uses the generate batch captions callable in order to generate caption text all of the same size within a batch.
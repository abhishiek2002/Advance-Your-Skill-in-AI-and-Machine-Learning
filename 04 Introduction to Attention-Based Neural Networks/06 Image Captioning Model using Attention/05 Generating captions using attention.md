Generating captions using attention
- Let's take a look at how our decoder generates captions. The input here includes the feature representations, the output of the encoder CNN, the max length of the caption is set to 20. We also pass in the vocabulary for the captions. We then initialize the hidden state and the batch size. The hidden state is initialized using the encoder output. We store attention rates in an array. To start the generation sequence, we set up a one-hot vector of the SOS token and convert that to an embedding. The words of the caption that we generate will be stored in this captions list. Set up a for loop going up to the max length of the sequence that we want to generate. For each time instance, the first step is to take the endcoder output and compute the current attention. We'll append this attention to the attention weights list so that we have all of the attentions for different time instances at the end of generating this caption. We'll use these attention weights to visualize how our model actually generated each word of X in the image caption. The LSTM input is a concatenation of the current word in the sequence and the current context vector. So, get the LSTM input, pass that through the LSTM cell and get the hidden state for the next time instance. In order to get the one-hot vector representation of the predicted word, we pass the hidden state through the fully connected layer through the dropout, we reshape it, and then we use argmax to get the idea of the predicted word, and then we append this ID to the captions list. Now, if the ID of the predicted word is basically equivalent to the eos token when we reach the end of the caption that was generated and we can break out of this loop, otherwise we simply get the current word predicted in its embedding form, and then we use it for generating the next word in the sequence. And once the entire sequence has been generated, we use the vocabulary to convert the IDs to actual words. And here is the code for initializing the hidden state of the decoder. We use the features output of the endcoder CNN, we compute the mean and pass that through the linear layers init_h and init_c to get the initial hidden state.
The encoder CNN generating unrolled feature maps
- Once again, in the end coder network, we'll be using the Resnet-50 pretrained model, but we'll choose the output of a different layer. So here's the architecture of the Resnet-50 model. The last convolutional block, that is layer four, generates feature map representations of the input image. These feature map representations will be the input to our attention model, so that the attention model can direct the decoder to pay attention to the right portions of the input image. In the Resnet-50 model, these feature map representations are then fed in through an average pulling layer and then finally through the last linear classifier layer. But remember, in our attention model, we are interested in the feature map representations and that is the output that we'll use from Resnet. Here is our end coder of CNN within the init function. That's just a simple initialization. We also instantiate the Resnet-50 model, we use pretrained weight. The Resnet-50 model will not retrain. That is it's weights will not be updated during the training process of our image captioning model. This is why we iterate over the Resnet model and set requires grad to false. Now, remember that the output of the Resnet model that we are interested in is the output of the last convolutional block. There are two layers after that; the average pooling layer and the final linear classifier. We are not interested in those. So we only select those modules of the Resnet-50 model that we are interested in. That is everything but the last two layers. We set up a new sequential module with all of the layers of Resnet, except the last two. Now let's define the forward pass through this encoder of CNN. The CNN operates on a batch of input images and reduces feature maps for each image. Now, the last convolutional block of the Resnet-50 model reduces 2,048 feature maps, where each feature map is seven by seven. So the dimensionality, when we pass in a batch of images through the Resnet, the dimensionality of the output will be batch size 2048. That's the number of feature maps. Seven by seven, that is the height and the width of this feature map. Now, how did we get the feature map size as seven by seven? Well, it turns out the Resnet-50 model takes the original image size and because of all of the transformation that it applies, the image size is shrunk by 32. So the final size of the feature map will be image size divided by 32, which gives us 7 2 24 by 32, gives us seven. Passing our input batch of images through the Resnet model will give us these feature map representations of the image that will later on feed into the attention model. Let's extract the individual dimensions from these feature maps. We have the batch size and the number of feature maps, that's 2 0 4 8 and size one and size two give us the height and width of each feature map. I'll reorganize these features to switch the axes around so that we can use them in our decoder. So we'll end up with batch size as the first dimension, the height and width of the feature map as the second, and third dimensions and the number of feature maps as the last dimension. I'm now going to unroll these feature map representations, batch size, then the seven by seven feature maps get flattened to be a single 49 pixel vector. So we'll get 2 0 4 8 of these 49 pixel vectors for each image. This is the structure of the data that will return from the encoder.
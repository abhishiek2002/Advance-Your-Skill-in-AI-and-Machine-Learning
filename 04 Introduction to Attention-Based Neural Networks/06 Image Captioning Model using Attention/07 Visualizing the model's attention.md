Visualizing the model's attention
- [Narrator] Now we'll see something really cool. In our image captioning model, you can actually see what part of the input image our model focuses on while generating a particular word in the caption. For that, let's set up some utility functions first. The generate caps function is just a helper function that takes in images at its input and then generates captions for those images. We place the model in eval mode, so dropout doesn't apply. We then turn off gradient computation, pass the images through the model encoder, get the features, and pass the features through the model decoder to generate captions. We then display the image and the caption that was generated. We have the caption and the attention weight from the model decoder, and we return that. We'll be using that next. Show attention takes in an image and the attention weights generated by our model and highlights on the image what part of the image was used to generate a particular word in the caption. Most of the code here is just map plot LIP code for displaying images. You can see the input attention plot which is reshaped to be 7 by 7 and then reshaped once again to display what portion of the input image the attention is focused on at any point. I'll now pick one image at random from our training data, access the image and pass that through the generate caps function, which basically generates the captions from our model. Then call show attention along with the image, the captions and the attention weight, and let's take a look. So here is the original image. It seems like there is a dog and a cat in here. Let's scroll down and let's see what caption was generated and what portion of the image generated the caption. The caption says "a black and a brown dog is laying on a bed" and you can see what portion of the image the model focused on to generate each word of the caption text. Let's try this once again with another image. This image is of some people playing on a beach. Let's take a look, and you can see that the caption says "a group of people are playing in the sand at the beach" and you can see which portion of the image the decoder concentrated on, or focused attention on, to generate each word in the caption. It's pretty amazing.
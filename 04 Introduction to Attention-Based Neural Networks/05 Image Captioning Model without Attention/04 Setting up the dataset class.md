Setting up the dataset class
- Now that we've set up our vocabulary, the next step is to set up a dataset object that will allow our training data to access images and the corresponding captions in the right format, in batches. Our Flickr dataset class here inherits from the base dataset class in the PyTorch library. The init method, we pass in a bunch of information, the directory where our training images are located, the pointer to the captions file, the vocabulary to generate captions, and transformations that we want to apply to the input images. Now we assign these to variables of this object. We have a pointer to the captions text file, we'll need to extract the contents of the captions file, so, I'm going to read it in as a pandas data frame. The length of our training data is equal to the number of captions that we have in the file. That is the length of the data frame. We'll extract the captions and the image names in member variables for use down below. The length of this data set is equal to the number of captions in the training data, so, we return self.length. We want to be able to use square brackets to access the data at index positions, so, make sure you specify a definition for this magic method, get item. We access the caption X and the name of the image file at this particular index, and we now use this to load in and set up the data in a tenser format. We access the image file where it's located in our current working directory. That's where we extracted the image and we call Image.open to load the image into our program. Image is a class available in the PIL library, a free open source library for Python. If you've specified any image transforms, make sure you apply the transformations to the input image. The next step is to get our caption X in an tenser format. This tenser will be the captions represented in the form of one-hot encodings. In this one-hot encoding, each word will be represented by its corresponding index, and that index represents the position of the one corresponding to that word in a one-hot encoded vector. This lambda function converts an input text to a list of indices, where the indices correspond to the words in the caption. Next, let's set up our caption vector. Initialize an empty list. The first element in that list should be the index for the start of sentence token, and then we have the entire sentence which we construct by calling caption_text_to_index and then we end the caption vector with the EOS or end of sentence token. We then return the image object and our captions text in the tenser format.
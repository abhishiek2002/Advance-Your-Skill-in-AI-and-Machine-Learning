Building the encoder CNN
- [Instructor] We are now ready to construct the encoder for our image captioning model. Now, the batch size that you're going to be using for training is 128. So I set that up in a variable. The index of the padding in our vocabulary is the pad_idx, and I've constructed the data loader that we'll use to train our encoder decoder model. The data loader is the class that makes available the training data in batches, when we train our model. Observe how we use the generate batch captions callable object here. We specify the pad index and that this is the first batch. And the data loader will use this collate function, that we have specified, to generate batches of image data and the corresponding captions, such that all of the captions in a batch will be of the same length. The length of the longest sequence in that batch. The encoder model is a convolutional neural network which we'll use to generate image representations. These image representations will be fed into the decoder, and we'll use that to generate a caption for that image. Convolutional neural network that we use for our encoder is the ResNet-50 pre-train model. ResNet-50 is a very powerful image classification model. It was first released in 2015, when it won the ImageNet challenge. ResNet stands for residual network. Now, if you look at the structure of this model, you'll see a number of convolutional and pooling blocks. At the very bottom, you'll see that the input to the final linear classifier for this convolution neural network, takes in input of dimensionality 2048. This means that every image is represented using an embedding 2048 in size. And we need to remember this dimensionality. We'll use it in our encoder. Now, let's set up our encoder, using this ResNet-50 pre-trained model. There is the class for the encoder, the encoder convolution neural network, which inherit from nn.Module. In the init function, we take in the size of the embedding that we want to generate. The embedding size will be the size of the initial hidden state that we feed into the DecoderRNN. We use a pre-trained ResNet-50 model to generate embeddings. We won't use the pre-trained weights of the model. We're not going to actually train this model. We are going to generate embeddings, using the trained model. Requires_grad is false, because you're not propagating gradients through this network. That's not part of the training. Now, in order to generate image representations, we'll use all of the layers in this model except the last layer. Remember the last layer is a simple linear classifier. We only want the image representation from the last, but one layer, of the ResNet model. So I set up a sequential model with all of the layers except the last linear classifier, and I add in my own linear layer to generate our encoder embeddings. The dimensionality of the input to this linear layer will be 2048. That is the input dimensionality of the last linear layer in ResNet. The final hidden state output by the encoder will be of embed_size. That's what we specify. We then define the method that contains the code for the forward pass through this encoder. In passing the images, we generate the features corresponding to these images, by passing the images through the ResNet backbone. We then reshape the features so that it can be fed into the final linear classification layer. And we finally generate our own encoder embedding. This embedding is a vector representation of all of the input images in this batch. Observe that at every step I've specified, the shape of the data, as it's passed into that layer, and the output shape of the data as well. This is useful for you to track what exactly is going on in the neural network.
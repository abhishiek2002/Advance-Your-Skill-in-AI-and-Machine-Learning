Building the decoder RNN
- [Instructor] Now that you've set up the encoder CNN, the next step is to set up the decoder RNN. And this decoder recurrent neural network is what we'll use to generate captions. The init method takes in a number of variables as input: The embed_size. That is the size of the embedding for each word in the vocabulary. Hidden_size, the size of the hidden state of the RNN. Vocab_size is the length of our vocabulary. Now, you can build RNNs with more than one layer. By default, I've set num_layers to 1. This is something that you can tweak. The first step is to set up the embedding layer. The embedding layer is used to generate embedding representations for the input words. Remember, we input the words as one-hot vectors. That is, every vector will have a size equal to the size of our vocabulary. The embedding layer will take the one-hot vector representation of a word and generate a lower dimensionality embedding. The embeddings will be found during the training process. Next, we specify the cell that makes up the layers of our recurrent neural network. Here, I've used an LSTM, or a long short-term memory cell. The first input, embed_size, is the number of expected features in the input x. That is the number of expected features to represent a word. The second input argument here, hidden_size, is the number of features in the hidden state edge of this LSTM. Number of layers indicates how many recurrent layers a single cell will have. If you set this to 2, you'll have two LSTM cells stacked one on top of the other. Batch_first equal to True indicates that, in the input, the first dimension represents the batch dimension. This is the LSTM cell that will be unrolled to make up the layers of our neural network. Then finally, at the very end, we have a linear layer that produces the final output. It takes in an input of size_hidden and produces a one-hot vector representing a word from our vocabulary. The one-hot vector is a word from the generated caption. In the forward pass through this decoder RNN, we take in the features of the image itself. This is the output of our encoder. We also take in the tensor corresponding to the image captions for each of the images in the batch. The image captions are in one-hot form. We pass these through the embedding layer to generate embeddings for each word in each image caption. Every word in the caption text is now represented using embeddings. Now, if you remember our encoder-decoder model, the first input to the decoder is always the hidden state of the encoder. So concatenate the hidden state of the encoder with the embeddings representing the caption text. Now, we take this entire sequence, starting with the hidden state output of the encoder and the caption text, and feed that through our LSTM layers in our recurrent neural network. We then fit the final linear layer to get the one-hot encoded representations of the generated captions. The caption x and the encoder output that is the representation of each image will be used to train our decoder. This is the forward path through the neural network. Next, let's see how this decoder RNN will generate captions given an image. When we invoke generate_captions, we'll pass in the encoder output that is in the inputs variable. Hidden will be set to 1. The max length of the caption that we want to generate, I've set to 20 by default. And we'll pass in the vocabulary that the generator should use for the caption. The vocab object contains a mapping of index to word. We may generate captions in batches. So extract the size of the batch that is the first dimension of the inputs answer. As we generate captions, we'll have the IDs corresponding to individual words. We'll store that in this captions list. The maximum length of the caption generated will be max_len. The caption can be shorter if we encounter an EOS token before we reach max length. Pass the initial input to the LSTM cell and get the output of the LSTM and the next hidden state. We then pass the output through the last linear layer to get a one-hot representation of the predicted word. We then reshape the output so that we can get the index of the predicted word using argmax. We append the index of the predicted word from our decoder RNN to vocab. If we find that the word token is the EOS, or the end-of-sentence token, we simply break out of this for loop. We've got the entire caption text predicted by our decoder. If we haven't reached the end of the caption, then we use the current word to predict the next word in the sequence. So we get the embedding of the current predicted word, that is inputs, and that input is then fed back to the LSTM, along with the hidden state output of the previous layer in our network. Once the entire caption has been generated, we convert the index representations of the caption to actual words using the vocab object.
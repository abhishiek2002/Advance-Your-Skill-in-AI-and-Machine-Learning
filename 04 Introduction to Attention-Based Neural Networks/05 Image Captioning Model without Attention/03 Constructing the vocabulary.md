Constructing the vocabulary
- Now that we have loaded in the training data, let's process this data so that it's in a form that can be fed into our machine learning model. First, let's make sure that we are running all of our code using the GPU. Torch.cuda.is_available(), checks for the gpu. Yes, indeed, we do have it on this notebook, which means the device that we use to run our code is a coder device. The image captions will be generated by an RNN decoder, and in order to feed in data to the RNN decoder during training and during prediction, we need to tokenize our English sentences. And that's why we instantiate this tokenizer. In order to run training a little bit faster, and to keep our vocabulary size manageable, we use a counter to track the frequency of the different words that occur in the captions. This will allow us to limit our vocabulary to words that occur fairly frequently. The counter object here will allow us to keep track of word frequencies in the training data. Let's see how the tokenizer works. I invoke the tokenizer on some text and you can see that word tokens are generated at the output, a list of word tokens that make up the original sentence. Observe that some pre-processing has been applied to the word tokens before tokenization. All of the words have been converted to lowercase. And also, punctuations have been preserved. We are now ready to build up our vocabulary. Now, this requires us to tokenize every sentence in our training data, that is the image_captions. We then use the tokens generated from these sentences, the individual words, to update the counter object. The counter object will keep track of how often each word occurs in the training data. Once we've done this for all of the words in the input training data, we'll use that to build up a vocabulary object. This vocab object takes in a counter as an input argument and observed that I've specified min_freq = 5, meaning I only want to include those words in my vocabulary, which have occurred at least five times in the training data. This min_freq is something that you can change, but now we have a vocabulary with the most frequently used words. Next, I add some special tokens to my vocabulary. The unk_token, or the unknown token, will be for words that are not recognized by the vocabulary words, which haven't occurred very frequently or haven't occurred at all. The pad_token is another token, which allows us to add our caption sentences so they're all of the same length. This is required in the training process because we want all of the sentences in one batch of training data to be of the same length, so that the recurrent neural network decoder, can be unruled to that length. The sos_token signifies the start of a sentence. The eos_token signifies the end of a sentence. Our vocabulary needs to include these special tokens, so I'm going to go ahead and insert these tokens at the very beginning. unk_token has an index of 0. The eos_token has an index of 3. And finally, we set the default index of our vocabulary to be the unk_token. So if there is a word that is not part of our vocabulary, that will be replaced by unk. Now let's take a look at the size of our vocabulary. These are the words that will be used by our decoder model to caption images. And you can see our vocabulary is roughly 3000 words in size. 3005. The vocab object that I taught uses an index to represent every word in the vocabulary. So each word will have a corresponding index, and that index is a one-hot representation of the input word. For example, the index corresponding to the word dog, is 29. This means that, in a one-hot vector that has 3005 elements equal to the number of words in our vocabulary, the index at 29 corresponds to the word dog. Let's take another example here. The index of the word boy in our one-hot encoded vector is 142. And our special tokens also have corresponding index values. For example, the eos_token corresponds to index of 3.
Implementing utility functions for training data
- [Instructor] Next, I set up a callable class that will be used by the PyTorch data loader to load the image as well as the corresponding caption for that image in batches. The data loader is what our training code is used to iterate over batches of training data. The init method of this class accepts a pad index, that is the index of the pad token in our vocabulary, and whether this is the first batch to be generated or not. This class is a callable, that is it can be invoked like a function, so define the call method, and pass in one batch of training data, we'll now get this batch in the right format. The batch passed in is a couple of the image tensors and the caption tensors. You'll see that across this code, I've added in a bunch of comments explaining the structure of data passed in the operations that we apply and the resulting changed shape or structure of the data. The image tensor at the input has the following dimensions, pad size, num channels, height and width of an image. The caption tensor as the first dimension that is the pad size, and the second dimension is the number of words in the caption. Now captions can be of different lengths, so you'll find that in the same batch, we'll have sentences of different length, and then RNNs can't deal with those captions in a batch. We need all of the captions to be of the same length, and we'll achieve this using padding. We then iterate over each image in the batch and add an extra dimension to each image. We'll need this for the conation operation that we perform next. We get all of the images together in a single tensor using torch.cat. We then process each caption in turn. Now all of the captions in the batch are currently of different lengths, and in order to feed the captions together as a batch to the RNN, we need the caption lengths to be the same. so that we achieved this by adding a padding token to all of the captions that are shorter than the longest caption in this batch. The pad sequence function here will ensure that all captions in the batch will have the same length equal to the length of the longest sequence in the batch and our decoder RNN will be unrolled per batch to the length of the longer sequence. The display image utility function displays an image and the caption corresponding to that image, encoded version of the caption. Now, there are a bunch of operations that we need to do. We multiply by the mean of the image net data set, and then add the standard deviation so that the pixel values are such that Matplotlib can plot this image. You'll see that we use these numbers to pre-process the data, we're going to get rid of the pre-processing so that we can display an image from the dataset. Observe that I perform a transpose. Remember, PyTorch expresses every image with the dimensions, num channels, height and width. I'm going flip the dimensions so that it's of the form height, width, num channels, so that Matplotlib can display this image. We are now ready to instantiate our Flickr dataset. Specify the transforms you want to apply to the input images. I perform a resize to 226 by 226, then a random crop to 224. These images are going to be fed into a ResNet-50 pretrained model to extract representations of each image. Now, pretrained models in PyTorch require that the minimum image size be 224 by 224, and that's why I've picked that crop. We convert the image to a tensor, and normalize it by subtracting the mean and dividing by the standard deviation of the ImageNet dataset. This is pretty standard practice. Next step is to instantiate the Flickr dataset, and take a look at the sample images. We have the data set. The length of the dataset will give us the number of training instances, and we know that's roughly 40,000. Remember, each image has five different captions. Let's pick an image at random. I'm going to pick the 10th image in my dataset. We'll display the caption, and also the caption as a token of IDs. Here is the image. You can see the token IDs that make up the caption, and you can also see the actual caption here at the bottom. Note that the caption starts with a SOS or start off sentence token. Let's look at another sample image here. This is the image at index 1500, and this is an image of some kind of balcony. You can see the caption here below. You can see the token IDs. You can see that there are a few unknown words here in the caption text as well, so instead of the actual word, you can see the unknown token, the UNK. A word is likely to be unknown because it hasn't occurred more than five times in our vocabulary. Remember, that was the minimum frequency that we chose for words to make up our vocabulary.
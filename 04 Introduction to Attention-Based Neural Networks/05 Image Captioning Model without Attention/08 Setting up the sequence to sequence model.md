Setting up the sequence to sequence model
- Having implemented the encoder and the decoder. we are now ready to build our sequence to sequence model and that's now very simple because of the encoder and decoder objects that we've set up. Let's set up the image to caption class. This class takes in a number of input parameters needed to initialize the encoder as well as the decoder embedding size, hidden size, vocab size. Num layers in the decoded set by default to one. The only other initialization needed is to set up the encoder convolution neural network and the decoder recurrent neural network by passing in the right input parameters. The forward pass through this model is straightforward. We accept the training data in the form of images and the corresponding captions. We pass in the images through the encoder and get representations for every image in the batch. These representations form the encoder output that are then into the decoder, along with the captions. Next, let's instantiate this image to caption model. We use an embedding size of 300. The hidden size of the RNN will be 512. The vocab size is the size of the words or the vocabulary in the flicker dataset. This includes all the additional tokens, to start of sentence, end of sentence, pad and unknown. We use two layer RNN, so num layers, I have set to 2. So we'll have two (indistinct) cells stacked one on top of the other. In every layer of the recurrent neural network I've specified the learning rate of 0.0001. This is something that you can tweak and see how things work. Next step is to instantiate our sequence-to-sequence model image to caption by passing in the right parameters. Remember, we want to train this model on the GPU so I call model to device, to place the model parameters on the GPU. We'll use the cross entropy loss to compare the probability distribution of the output generated word with the probability distribution of the actual word in the vocabulary. Now, while computing the cross entropy loss we'll ignore all the extra padding that we've given the caption sentences. If you remember, we padded the caption sentences, so that all captions in the same batch have the same length. This allows our decoder RNN to be unrolled to the same number of layers. And finally, we'll use the 'Adam' optimizer for training. When you run this code and instantiate our sequence-to-sequence model, the pre-trained weights of the resnet 50 model, that we use In the encoder, will be downloaded and loaded to initialize our resnet 50 model within the end quarter.
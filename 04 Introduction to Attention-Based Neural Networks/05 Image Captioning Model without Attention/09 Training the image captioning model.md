Training the image captioning model
- [Narrator] We are now finally ready to train, our sequence to sequence model, and see what kind of captions this model generates. Remember, we are not using attention. I'm going to train for 10 epochs. And I'm going print out a log to screen, every hundred iterations. That is every hundred batches. I run two for loops here, one for every epoch. And in each epoch, I iterate over the entire dataset. So I run an inner for loop, for my training data, accessed in patches. I have the images and captions entered for every batch. I'm now going to place these tensors, on the Kuda device on the GPU, so that the entire training runs on the GPU. I'll zero out the gradients of the optimizer. And I'll pass the image and captions. As a forward pass through the model, and get the outputs of our sequence to sequence model. I'll then reshape the data. So that we can compute the cross entropy loss, between the output generated by the model, and the actual captions in the real data. In the training data. We then make a backward pass to compute gradients, for the decoder. And then call optimizer.step, to update the model parameters. And we do this for every batch. Now for every hundred batches, I'm going to print out a bit of information to screen. I'll also take an image at random, and generate a caption for that image, based on our current model. In order to evaluate an image, I call model.eval. So that our model is set to the evaluation phase. I make sure I don't compute gradients, and I access a single image from the next batch of data. I passed that image through the model end quarter, to get the feature representations of the image. And then I use these feature representation output, of the CNN, to generate a caption for that image. And I'll use the display image utility that we had set up earlier, to display the image and the caption, generated by our network. And I'll switch back to model.train, so we can continue training our model. This way we'll see how the captions improve, as the model proceeds through training. Let's start the training process. At about one epoch, you can see that, the sentence is just all As. Starting with an SOS token, ending with an EOS token. Let's see how things improve, after a few more batches of training. Still nothing really legit that has been generated. After Epoch 1, you can see a few words showing up. The caption talks about a man, and I guess there are indeed men in this image. Things seem to be getting a little better. Let's wait for some more epochs. After Epoch 3, you can see that the captions, start getting a little more meaningful. They're not right yet. It talks about a man in a red shirt. There's no red shirt here. A white dog. No real dog, here. But the captions are getting more complete. At about, Epoch 7, you'll find that things start making much more sense. Here is a caption generated for this image. A dog is running through the grass. There is a dog, there is grass. The dog is not running, but the captions are getting better. And you'll kind of see this gradual improvement over time. At the end of training at about 10 Epochs, you can see the caption is about a group of people, standing somewhere. That's pretty good. There is another caption about a man in a red shirt, riding a bike on a dirt road. That's pretty good. And then there's something about a man in a blue shirt, where there are no blue shirts here. So the captions are getting better, but they're not perfect. And you'll find that when you train such a model, without attention. You may need to train for a very long time, to get really good captions. And even then, your model may never be very good.
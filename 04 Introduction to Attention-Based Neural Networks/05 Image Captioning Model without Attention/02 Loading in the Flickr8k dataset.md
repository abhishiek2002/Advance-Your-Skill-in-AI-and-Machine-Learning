Loading in the Flickr8k dataset
- We have CoLab set up. We have our dataset uploaded to Google Drive. We're ready to get started coding. Now, the name of this notebook is Image Captioning Without Attention because that's the model that we are going to build. We're going to look at images and try to generate captions for them using an RNN decoder but we will not use Attention. We'll look at the captions generated by this model and use that as a baseline for the next model where we will, indeed, use Attention to generate captions. The code that I've used here, in this demo is a modified version of the code available at this URL here on Kaggle and the data set for image captioning is also available on Kaggle at this link here. The Flickr 8k set. Now let's set up our import statements. You can see that I'm going to be building our neural network using PyTorch. You can see a number of imports for pandas and MAT plot lib. That is to load in, process and visualize our data and then we have all of the Torch related imports. Our data set is on Google Drive. In order to access that data from CoLab, I need to mount the contents of Google Drive to this notebook. Once you run this code you'll get a dialogue, connect to Google Drive and you'll need to set up the right permissions. Log in using your username and password. Click on allow here, so that you give CoLab the permissions to access the contents of your Google Drive folder. Now we can confirm that our Google Drive has been mounted to this CoLab notebook. Click on this folder icon off to the left and under that, you should find a sub folder called Drive and if you expand this, MyDrive is where your drive contents are located. Now within here, you should be able to find the flickr_dataset sub folder and under that we have the flickr8k.zip file. This is the zip file with our training data. It contains a subfolder with images and the captions.txt file that contains the captions. Now I'm going to get out of here and let's unzip this file into our local folder here on CoLab so that it's accessible to us for training. This code references the zip file on Google Drive and unzips it to the current working directory. Once the unzip process is complete, we'll run an LS command to see what our current working directory looks like. There you can see captions.txt, and the images sub folder with our data. Let's run a quick count of how many images are present in the forward slash content forward slash images sub folder. So how many images are present in our training data? And you can see that there are about 8K images 8 0 9 1 images to be precise. If you remember the contents of captions.txt that contain the image name and the corresponding caption. I'm going to read it in as a pandas data frame and let's take a look at a sample of the content in this image captions data frame. This data frame has two columns. The image column with the name of the image file, the caption column with the corresponding caption. If you remember, I had mentioned earlier that every image in our training data has five different captions, similar captions, but different and you can see from the shape of the captions data frame that there are 40,000 captions. roughly. Let's quickly take a look at the first 10 captions in our training data and you'll see that these captions correspond to two images. The first five captions here correspond to the first image and the second five captions correspond to the second image in our training data.
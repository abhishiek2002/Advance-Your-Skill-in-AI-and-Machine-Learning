Attention models for image captioning
- Now, so far we've discussed how attention models work with encoders and decoders for language translation. But how will we use attention models in image captioning? Well, the principle is the same as that of language translation models, but there are some interesting twists. The main thing is if you're working on images, images are not really sequential input. Which means when you focus attention on parts of an image, you're not actually focusing attention at different time instances in an input sequence. You're actually focusing attention across a two dimensional representation, the image. Also, we generate embeddings or representations of images using convolution neural networks. So, we pass an image through a CNN, and we get a representation of the image at the output of the CNN. This image representation, which is the output of the convolution neural network, can be thought of as the hidden state of our encoder. So, our convolution neural network is actually the encoder for an image. What we want to do in our image captioning model, is to take this hidden state, or the representation of the image that we have and then generate text using these image representations. Now, let's get back to the original problem that I had outlined. An image is not really a sequence, and the image representation that we get from a convolution neural network is a representation of the image in its entirety, of the whole image. And it's very hard to generate image captions from the image as a whole. What you really want is an attention model that is used to focus attention on relevant parts of the image that can be used to generate captions. So you want to identify two-dimensional patches on the image that can be used to generate captions. Let's visualize how an attention model works for image captioning. The encoder is a convolutional neural network that generates a representation of an image. Now, the problem here is, that the representation generated by a CNN offers no spatial information. It does not focus on different subparts of the image, which means that our image captioning model that uses attention should not work with the final output of a CNN. Instead, it should extract feature map representations of the input from the output of a convolutional block. The output of some convolutional block may produce C feature maps where every feature map has a height and a width that is M by N. So rather than use the final vector representation of an image, we'll extract these feature maps. Each feature map is of size M by N, and we have a total of C feature maps. Once we have these feature maps, we then extract a slice of the feature map representation. The slice has a depth of C, and dimensions of one by one pixel. We can think of this one by one by C slice of the feature map as representing a portion of the input image. So, this can be projected back and you can think of this slice as containing information from a part of a input image. Unrolling the feature map representations will give us slices of the input image to work with. And we can then focus our attention mechanism on these slices. Remember, these slices represent portions of the input image, and we want our attention model focused on the relevant portions of the input image while generating captions.
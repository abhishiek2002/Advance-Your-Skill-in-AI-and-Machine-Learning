The role of attention in sequence to sequence models
- Attention plays a very important role in sequence to sequence models and it can greatly improve their performance. Now why is that? Well, let's take a look at our language encoder, decoder model, our sequence to sequence model, and see if you can figure out what's wrong with this particular framework. Essentially, we have a single hidden state, the output at time step t, which is supposed to represent the entire input sentence. The hidden state is a single vector, the output of the last encoder RNN layer, and it's supposed to represent the entire sentence so it's a very compressed representation. So all of the complexities of the input sentence is embedded into this single hidden state. If you're only working with very short sentences, this may not be a problem, but as you work with longer sequences, more complex sentences, a single vector is unlikely to be able to hold all of the information in the input sentence. Especially in the input sequences very, very long. And this will result in information overload. This hidden state becomes some kind of bottleneck. Let's say you wanted to translate this really long sentence from Stuart Little. Well, that will cause an information overload in the hidden state. If you have very long sentences for translation, the word generated in the target language might reference something very far back in time in the source language. The question then is will the hidden state remember the beginning of the original sentence in order to provide the right translation at the end of the output sentence? Maybe unlikely. Now, language translation models are often evaluated using a metric called the Bleu Score, the bilingual evaluation understudy, it's a score for comparing a candidate translation of text to one or more reference translations. The expectation for a language translation model is that as the sentences get longer, the translations get better and they level off at some high level. But what researchers found was the score for sentences tends to fall drastically as sentences get longer, and that's because of our hidden state bottleneck. Now, let's think about the information that we have and the information that we actually use. In fact, all of the hidden values generated in the encoder network are meaningful and they contain information. All of these hidden values contain important information. We have them right there. Then why are we not using them? Another thing that we've seen in language translation is that different hidden values may have different relationships to different parts of the output. Let's say we are trying to produce a translation for the word apple. Then the word apple from the input sequence will have the most information for the corresponding translation. The hidden states for the other words, "I ate an", are less relevant. The same is true, for say, translating "ate". This roughly maps to two words in the German output which means while producing those two words at the output the decoder should pay special attention to the hidden state for the word "ate". And this is exactly why attention is needed. We know that all hidden values in the encoder contain information. Now, if you just have a single hidden state to represent the entire input, that dilutes the information that is passed on to the decoder and this results in a worse model than you could build if you use attention. Now, we know that different parts of the input affect different parts of the output but we may not know this upfront. We don't know which part of the input sequence affects which part of the output sequence. Especially in language translation, the sequence of the words that the input may not match the same sequence at the output because the structure of languages is often different. Now, a naive approach to improving the performance of this model might be to say connect all of the inputs to all outputs. But this does not work well in practice because your model then ends up being overparameterized and it's hard for the decoder to pay attention to everything. Connecting up everything essentially tells the decoder everything is important. It also ignores the synchronous dependence of the output on the input. The fact that the input is a sequence which affects the output sequence. And this is exactly why attention models are needed. They help pay attention to relevant portions of the input at every time stamp in the output.
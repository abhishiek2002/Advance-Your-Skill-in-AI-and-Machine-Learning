Bahdanau attention
- [Narrator] Since our image captioning model, we'll use Bahdanau's Attention. Let's understand the mathematical formulation of this attention. This is additive attention. The raw score in Bahdanau's attention mode is computed using this formula. Now there is a lot going on here, but let's break this down. It's very straightforward. S of T minus one here refers to the hidden state of the decoder at time instance E minus one. This is an input to the attention model. Now another input to the attention model is the hidden state of the encoder. H J here is the hidden state for each element of the input sequence. This is fixed. Once we know the input sequence it does not change over time. Bahdanau's attention is addictive attention because you apply a weight vector to each of the hidden states the encoders and decode hidden state and then you add these weighted hidden states together. Once you've added the weighted hidden states, you pass that through a tanh function and the output of the tanh is then multiplied by another trainable vector. Let's understand Bahdanau's attention mechanism visually. Now we have the hidden states of both the encoder and the decoder and we pass both of these through separate weights layers. So we have two different weights layers, W decoder, which acts on the decoder hidden state. W N coder that acts on the encoder hidden state. The result is the weighted output of the encoder and decodor hidden state. That's within the brackets of our formula. Now this weighted output is then added together and passed through a tanh function. The output of the tanh function is then multiplied by a trainable weight vector. A weight vector whose weights are found during the training process. And the result of this multiplication gives us the alignment scores. These are the raw alignment scores. Once we have the raw scores from our additive model, we can pass these raw scores through a soft max function to get alignment weights in terms of probabilities. These probability weights are the attention weights. Soft max generates the probabilities representing the weights associated with each input in the input sequence. We use these alignment weights and apply them to the output of the encoder and this will give us the context vector. The context vector will us magnify the important portions of the output at this particular time instance, and drown out irrelevant or unimportant portions. Now that we have the context vector that tells the decoder what portions of the input to pay attention to, that becomes an additional input to our decoder recurrent neural network. So, for every layer of the decoder RNN, we feed in the decoder hidden state from the previous layer, the context vector at this current time instance, and the decoder output of the previous layer. The decoder output of the previous layer is the last word generated in the sequence which is now used to generate the next word.
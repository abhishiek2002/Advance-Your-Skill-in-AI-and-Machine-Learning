Encoder decoder structure for image captioning
- [Instructor] Now that we have a big picture understanding of how attention models can be used for image captioning let's look in some detail at the encoder decoder structure that we are going to be used in our image captioning model. Now, this is what the model looks like. I know there are lots of moving parts here, but we'll break things down and understand them bit by bit. The encoder of the image captioning model is essentially a convolutional neural network. Now, in order to set up our attention mechanism we'll use the feature map representations generated by a convolutional block as an input to our attention model. The decoder of our image captioning model is a recurrent neural network, which makes sense because we want to generate language. The caption for every image will be text that we generate in the English language. The center block that you see here is our attention block. We will be using Bahdanau's attention. The additive attention that we studied earlier. Observe that the attention block connects the encoder output directly to the decoder network. The convolution neural network that we'll use as our encoder is going to be a ResNet-50 pretrained model. ResNet, which stands for residual network is a widely used image classification model. This model has been pretrained on the ImageNet data set and can classify images into a thousand different categories. This is what we'll use to generate feature map representations of our input image. The output of the ResNet model will be taken from the last convolutional block. The last convolutional block produces 2048 feature maps. Then each feature map is seven by seven. This is, of course, considering that the input image is of size 224 by 224, and that's the size that we'll use. These feature maps will be passed through a linear layer to generate the hidden output of the encoder. This is the hidden state that represents the entire input image. The input to the Bahdanau's attention model will be the hidden state as well as the unrolled feature map. Remember, we have 2048 feature maps where each feature map is seven by seven. We unroll them to get 49 feature maps with a dimensionality of 2048. That's the input to our attention block. The attention mechanism that we'll use will be Bahdanau's attention, and this will generate to output. The context vector, that will be the input to the decoder and the attention weights that we use to visualize how our image captioning model generates captioning text. Now, the dimensionality of the attention rates will be 49. One corresponding to each unrolled feature map. The decoder recurrent neural network will be trained on captions in the training data. Captions will be represented using one hot encodings, which are then converted to embeddings. So the embedding at time instance T is the word from the caption fed into the decoder network at time instance T. The context vector from the attention model will be concatenated with the embedding at time instance T, and that will be the input to our decoder RNN. The decoder will have all of the inputs, its previous hidden state, the context vector, the embedding at time instance T to produce the output at time instance T plus one. That is the next predicted word.
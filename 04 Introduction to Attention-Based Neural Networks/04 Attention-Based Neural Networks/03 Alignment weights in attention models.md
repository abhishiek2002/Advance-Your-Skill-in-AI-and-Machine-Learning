Alignment weights in attention models
- [Narrator] In this video, we'll discuss how these alignment weights are computed, and that will lead us to understanding how different kinds of attention models can be used in our sequence-to-sequence models. At this point, hopefully you've already understood the role of these alignment weights in our attention mechanism. The weights capture the relative importance of each position in the input to the current output, that is the output at a certain time step. So now the question is, how exactly are these alignment weights computed? In order to compute alignment weights, you first compute raw weights. And raw weights are computed using a scoring function which operates on the hidden state of the decoder and the encoder. This raw scoring function is what determines the kind of attention mechanism that you're using in your model. Change the scoring function; you change the attention mechanism. We'll briefly discuss a few different kinds of attention models at the end of this video, but you should know it's the attention model that generates these raw scores. In order to convert these raw alignment scores to weighted probabilities, we pass the raw score through a soft max layer. The actual weights assigned to each input in the sequence are computed using the soft max function. The output of the soft max layer is a probability distribution, which gives us the importance of each input state. Inputs that have higher probabilities at a certain time instance are more relevant to the output that is to be produced at that time instance. The soft max function is applied to the raw scores that were computed using some kind of attention mechanisms. Now, there are a number of different attention mechanisms that you could use. There is content-based attention, dot-product attention, scaled dot-product attention, location-based attention, and so on. Each have their own formula for computing the raw scores. Now the kind of attention that we are going be using, later on when we build and train an image captioning model, is this additive attention, also called Bahdanau's attention, named after the person who originally researched and published this.
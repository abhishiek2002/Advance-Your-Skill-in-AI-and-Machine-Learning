Summary and next steps
- [Female Speaker] And with this demo on image captioning using Attention Models, we come to the very end of this course. Now, let's take a look at some of the references that I used while building up the material for this course. These are references that you can use for further study. This video on Attention Models is by a professor from Carnegie Mellon. And I found it incredibly insightful. And it has a lot more detail than what I've covered in this course. This video from NPTEL IITM address is also a great resource. Among blog posts, I found this one by Lillian Weng, incredibly useful and another one was this medium.com article. Here is a quick summary of what we covered in this course. We understood that we need recurrent neural networks to operate on sequences. Recurrent neurons suffer from short term memory which is why we need long memory cells such as the LSTM and GRU. We discussed different types of (indistinct) and focus our attention on sequence-to-sequence models for language generation and translation because these are the models in which attention is incredibly useful. We understood how attention allows the decoder network in a sequence-to-sequence model to pay attention to relevant portions in the input. We discussed different attention models, but we focused on Bahdanau's attention or addictive attention because this was the attention mechanism we used in our image captioning model. We implemented image captioning with attention and without attention and compared results. If you're interested in deep learning and you want to study further, here are some other resources on LinkedIn Learning that you might find interesting. Deep learning Image Recognition, Self-supervised Machine Learning, and GANS and Diffusion Models in Machine Learning. And it's time for me to say goodbye. I hope you enjoyed this course. Thank you for listening.
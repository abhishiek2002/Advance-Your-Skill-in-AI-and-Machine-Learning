The need for long memory cells
- RNN memory cells have a hidden state that allow the cell to remember the past. However, the memory capacity of such a cell is not very big which is why, in order to get the best results from our recurrent neural networks, we need to use long memory cells. Now, the regular recurrent cell suffers from what we call short-term memory. We've discussed in the earlier video that the memory of a recurrent neuron is because of the hidden state that it holds. It is this memory that allows recurrent cells to remember what happened in the past, what went by in the sequence, and that's what allows recurrent neurons to predict the next bit of information in the sequence. However, the memory capacity of a regular recurrent neurons is not very large. The simple hidden state is not enough to store all of the information from the past. Now, if you're working with very long input sequences your recurrent neurons may not process these well and you will not get good results at the output. That's because these recurrent neurons will not have very much memory capacity and they'll not be able to remember information from time instances very far back in the past, the distant past. We've discussed that the layers of your recurrent neural network are made up of unrolled RNN memory cells. The number of layers in your neural network depends on the length of your input sequence, the number of time instances of your input. Now, recurrent neural networks are trained like regular neural networks, once the RNN cell has been unrolled. They're trained using gradient descent. You get the output of the recurrent network, you compute a loss function, you compute gradients, and gradients are then propagated back through the recurrent layers. The model weights are updated using these gradients. Now, the number of layers in your recurrent neural network is equal to the number of time instances in the input data that is equal to the length of the input sequence. Now, often input sequences are very, very long which means you need very deep RNNs in order to process such sequences. Once you have a very deep neural network, all of the problems associated with training deep neural networks come into play. Once you compute your gradients, if you try to propagate these gradients back through many layers of your deep neural network, that may cause the gradients to become very small for the earlier layers, or kind of explode in magnitude at different layers, leading to non-convergence during training. If your gradients fall to a very small value, that's referred to as vanishing gradients. if your gradients explode in magnitude and do not converge that's referred to as exploding gradients. RNNs are extremely prone to the problem of vanishing and exploding gradients. This leads to overall poor performance of your RNN model, long training times, and in general, the model that you get won't be a very good one. If the memory capacity of your RNN cell isn't very high, well, you have other problems. Very long sequences may result in your RNN forgetting earlier parts of the input. Let's take a very simple example. Let's consider this little bit of text. "I just love sushi at this restaurant". Now, if this is a review, is this a positive or negative review? Well, for a short sentence, it's super easy. Finding the important word is simple when the sequence is short. Your RNN, while processing this text one word at a time, just needs to look back a little way into the text to find the right word to determine the sentiment of this text. But consider a slightly different example. You have a much longer piece of text that is a much longer sequence, and you are asked whether this is a positive or negative review. Remember, you feed the input text into the RNN one word at a time. Observe that the same information is available as in the previous short sequence, but at the very beginning of this long sequence. Now, by the time the RNN processes every word in the sequence and gets to the very end it's likely to have forgotten what the review said at the very beginning, which means that it's quite possible that the RNN has forgotten that this is a positive review and this is why long memory cells are so important. Long memory cells in recurrent neural networks allow RNNs to remember important details from the past and in general, improve the performance of your recurrent neural networks. Now, there are several different kinds of strong memory cells. The two most popular ones are LSTM, or long short-term memory, and the GRU, or the gated recurrent unit.
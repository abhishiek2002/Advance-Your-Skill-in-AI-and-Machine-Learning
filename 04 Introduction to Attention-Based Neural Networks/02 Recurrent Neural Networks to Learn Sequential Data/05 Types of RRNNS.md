Types of RRNNS
- [Instructor] Now that we've understood recurrent neural networks, we are closer to discussing attention based models, but there are a few details to cover before that. The first of these are the different types of RNN's, now, RNN's can be categorized based on whether they accept vectors or sequences as input and whether they generate vectors or sequences as output. This results in four categories, we have Vector in, Vector out RNNs, we have RNNs that take in a Vector and produce a Sequence, we have RNNs which take in a Sequence and produce a Vector and finally we have RNNs that take in a Sequence and generate a Sequence at the output. Now, if your RNN is not really dealing with sequences it's just a simple feed forward neural network because it does not have input over time periods or time instances. Vector in Sequence out, these are RNNs, this is where you start with a single input and generate a sequence. An example of a Vector in Sequence out model is a text generation model, a text generation model, you might feed in the first word of a sentence and get the remaining words of the sequence. So this is the vector input, exactly one word that is the start of a sentence, the vector input is used to generate the next word in the sequence, the next word is then fed in as an input for the next layer, which generates the third word which is then fed into the next layer which generates the fourth word till we get a sequence at the output. A Vector in Sequence out RNN is also referred to as a One to Many RNN. The next type of RNN is a Sequence in Vector out RNN, you start with a sequence and generate a single output for that entire sequence and an example of such a model is something that we would use for sentiment analysis. Sentiment analysis models accept the entire sequence of X at their input, so you'll feed in one word at a time so that the ordering of the word is also considered and at the output, the final output will be a sentiment, positive or negative. Even though you might get an output at all of the intermediate steps or layers of the RNN, you only consider the output at the last time instance, that is the final prediction. Sequence in Vector Out models are also referred to as Many to One RNNs. And finally, we have the last type of RNN, this is the kind that we'll be working with, sequence to sequence models, you start with the sequence and use that to generate another sequence at the output. There are different kinds of sequence to sequence models depending on what you are trying to predict, for example, if you have a model that identifies Parts of Speech, such a model has an equal length output for a particular input. The length of the output is always equal to the length of the input in such a sequence to sequence model. However, Language Translation Models, which are also sequence to sequence models work a little differently, this is because the length of the input in a certain language may not match the length of the corresponding output. Language Translation models are also sequence to sequence models. Now, why was it important to know these different types of RNNs? When you work with attention based models, attention is generally used to improve the performance of sequence to sequence models.
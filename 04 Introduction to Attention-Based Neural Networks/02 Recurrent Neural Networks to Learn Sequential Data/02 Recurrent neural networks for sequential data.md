Recurrent neural networks for sequential data
- [Instructor] If feedforward neural networks don't let you cope with sequential data, what do you do? Well, you use recurrent neural networks whose very structure is meant for working with sequences. We've seen earlier what a single feedforward neuron looks like. Y is equal to activation applied to Wx plus b. A recurrent neuron is a little different. A recurrent neuron has the output Y at some time instance t fed back to its input. A recurrent neuron not only accepts X values at a certain time instance t at its input, but it also accepts the previous output, the previous Y at t-1 at its input. Let's understand and break this down bit by bit. The input to a recurrent neuron is X at some time instance t. This X will be different at different time instances over the sequence. The input X is multiplied by its own set of weights. That's Wx. In addition, a second input to the recurrent neuron is the output from the previous time instance, the output Y of t-1. This output Y of t-1 has its own weight vector W of y. In essence, a recurrent neuron differs from a feedforward neuron. In that, the output Y is fed back to the input, and this feedback loop is used to generate the next output in the sequence. Practically in a recurrent neuron, it's not the previous output Y that's fed back to the input. Instead, every recurrent neuron has a hidden state usually represented by h of t. This hidden state of the recurrent neuron can be thought of as an intermediate representation of the input data that was last seen. And this hidden state gives the neuron the ability to remember what happened in the past. And it's for exactly this reason that the hidden state of a recurrent neuron is often referred to as the memory of a neuron. So here is how recurrent neurons are different from feedforward neurons. Recurrent neurons have a memory that allow them to remember the past and learn from the past. And it is this property of a recurrent neuron that allows it to exploit time relationships that exist in data, that allows it to work with sequences. A recurrent neuron has the hidden state from the previous time instance fed back in the next time instance. That is a hidden state from time instance t-1 is fed back to the neuron at time t. This hidden state that's fed back in the next time instance contains information of all of the previous inputs and outputs that this particular recurrent neuron has seen. This hidden state contains the memory of the recurrent neuron. Now the question arises: How does a recurrent neuron accept inputs at different time instances? So how do you feed in a sequence to a recurrent neuron? And this is done using a process that's called unrolling through time. A recurrent neuron is able to accept input at different time instances by unrolling through time. This is how recurrent neurons deal with sequential input. Imagine a single recurrent neuron that's able to accept the value of X at a certain time instance along with the previous hidden state, at the previous time instance and then unroll this through time. Let's see how that would work. At time t equal to 1, the first input in the sequence is fed to the first neuron. Now this will produce some output Y of 1 and some hidden state h of 1. In the next time instance at time t is equal to 2, the hidden state of the first neuron and the second input in the sequence is fed in to the second neuron to produce an output Y of 2 and produce a second hidden state h of 2. This process is then repeated. The hidden state of the second neuron and the third input in the sequence is fed in to the third neuron to get the third output Y of 3 and produce a third hidden state, which is then fed in to the fourth neuron, which then feeds in to the fifth neuron, and so on. So how far do you the input neuron? Well, you unroll the neuron for as many time instances as you have in your input. That is the length of your input sequence determines how far you unroll the recurrent neuron. When you're building neural networks, you arrange your neurons in layers. You don't work with single neurons. How are the layers of a recurrent neural network constructed? Well, you set up an RNN cell, which corresponds to a single layer in your recurrent neural network. And an RNN cell will be made up of multiple recurrent neurons. And you then unroll this RNN cell to make up the recurrent layers of your neural network. The number of layers in your recurrent neural network depends on the length of the sequence that you feed in, the number of time instances you have in your input data.
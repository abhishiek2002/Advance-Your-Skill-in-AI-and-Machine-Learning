LSTM and GRU cells
- [Instructor] In the previous video, we discussed that the performance of your recurrent neural network can be greatly improved by using long memory cells. And we just quickly mentioned the LSTM or long short-term memory and the GRU or gated recurrent unit as popular long memory cells used in RNS. Now, in this video, let's understand how these work, starting off with the LSTM. The key thing about these long memory cells is that these cells hold additional state within them. For example, LSTM cells hold additional memory that allows them to keep track of important information that went by in the past. LSTM cells also have the capacity to forget unimportant information so that the cell state is not overloaded with irrelevant details. Each time a new input from the input sequence is presented to the cell, this new input is considered in addition to all of the existing information that the cell already remembers and this new input and the existing information are weighed against one another, and the cell makes the decision as to what parts to remember and what parts to forget. Let's see a quick overview of how the long short term memory or LSTM cell is set up. The information available to the LSTM cell at any point in time is processed using a number of different gates. Each of these gates can be thought of as neural networks of their own. Each gate plays a different role in helping the cell figure out what information to remember and what to forget. Let's consider them in turn. We have the forget gate here. The forget gate is responsible for deciding what information should be kept with the cell and what information should be forgotten or thrown away. The forget gate looks at two bits of information to make its decision. It uses the previous hidden state of the cell and the input at the current time instance to the cell. The forget gate will output values to decide what to keep and what to throw away. Values close to zero means information should be forgotten. Values close to one indicate that information is important and should be preserved. Next, we have the input gate. The input gate is used to add new information to the cell at the current time instance. At each time period, a new part of the input sequence will be available, and the input gate determines how important that is. The new input is combined with the previous hidden state. Once again, the input gate uses zero and one values to determine what to throw away and what to keep. Values close to zero meaning irrelevant information. Values close to one means important information that has to be remembered. The forget gate and the input gate acting together produce the cell state of this long short term memory cell. We also have this output gate. The output gate is responsible for figuring out what the new hidden state of the cell should be and this new hidden state is then what is carried over to the next time instance. And a combination of all of these gates and the additional cell state allows LSTM cells to remember important details from the past and forget irrelevant details from the past. Having understood how the LSTM functions we can turn our attention to the GRU or the gated recurrent unit. Now, the GRU cell is just a newer generation of the LSTM cell, and it has much of the same functionality. It's built using the same underlying idea. You use gates to remember important details and forget unimportant details. A major advantage of the GRU cell tends to be that it uses fewer gates as compared to the basic LSTM cell. Because there are fewer gates, there are fewer tensor operations to perform during the training of your neural network. So GRU cells tend to be faster to train. Now whether GRU cells are better than LSTM? Well, that's hard to say. That really depends on your specific use case. Here is an overview of what a GRU cell looks like. You can immediately see that it's far simpler than the corresponding LSTM cell. We have an update gate here. The update gate is similar to the forget gate and input gate for the LSTM. This gate looks at the new input available at that time instance and then decides what information to keep and what information to throw away. Another gate that's here is the reset gate and the reset gate is used to determine how much or what part of the past information should be forgotten. This is the GRU cell, a simpler version of the LSTM.
Sequence to sequence models for language translation
- Now that we have a big picture idea of how language generation models work, let's understand how language translation models work. These are sequence to sequence models. Let's assume we have a simple language translation model to translate from English to German. So if you feed an I ate an apple you'll get the equivalent sentence in German. Please forgive me if I don't try to pronounce the German sentence. I'll just fail miserably. Now, language translation models are sequence to sequence models, and the model structure uses two different networks. The first network is an encoder, and the second network is a decoder. Because the encoder takes in the sentence in the original language as an input, The encoder is an RNN. The decoder produces a sentence in the target language as an output. The decoder is also an RNN. The encoder works on the sequential input in the source language. Decoder produces sequential output in the target language. Now the job of the encoder is to read in the sentence in the source language and capture the entire sentence in the form of a representation, so the end coder learns a representation of the input. Let's zoom into the encoder RNN here. Because it's a recurrent neural network, and it's operating on the sequence, a single word from the source language is fed in at each time instance. This is the sequence of the input sentence fed into the encoder recurrent neural network. Usually you use a special token, such as, the end of sentence token to indicate that the sentence has ended or has been terminated. The last hidden state output of the encoder R.N.N. captures the entire information about the input sequence. so all of the details of the input sequence is embedded within this hidden state. This hidden state output of the encoder that captures the essence of the entire input sentence is what is fed into the second recurrent network that is the decoder, which is responsible for generating the translated sentence. So zooming out to our original diagram here the hidden state output of the encoder is fed into the decoder RNN. The decoder RNN then uses this hidden state to start generating the words in the translated language that is the target language. The decoder is also a recurrent neural network which means it produces or generates one word for each time instance. Now the output at single time instance is then an input to the next time instance during the generation of the translated sentence. So the output produced at time T minus one is the input at time P, along with the hidden state of the decoder at time T minus one. The translated sentence is generated till your decoder RNN producers and US token that indicates the end of sentence. So starting with the hidden state at the encoder output which captures the information of the entire input sentence the decoder generates the translator sentence one word at a time. As in the case of language models, the standard practice is to represent the input words using one hot vectors. These one hot vectors are then converted to embeddings, that is lower dimensionality representations. The embedding layer is a trainable layer and embeddings are learned as the neural network is trained. The output produced by the decoder is also in the form of embeddings, which are then converted to one hot vector representations of words. As we discussed in the previous video when we spoke of language generation models, the output at each time instance is not a word. It's actually a probability distribution over all the words in the target vocabulary. In order to train this encoder decoder network, we compute the loss using a loss function such as the cross entropy loss that we compute the divergence between the actual word in the training data and the word generated by our model. Once training is complete, during prediction, when we actually producing the translation, we'll always use the previous word in the generated sequence to produce the next word.
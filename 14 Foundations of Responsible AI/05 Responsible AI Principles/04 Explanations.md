Explanations
- Explainability is a key principle of responsible AI. However, creating explainable models and systems alone, are not enough to mitigate the harms of AI systems. When discussing explainability, we must first understand that some models, despite being explainable, can still be rooted in colonialism and enforce inequality. For example, we can create an explainable model to help allocate mental health resources for communities. This model will likely highlight the communities with lower incomes have lower access to healthcare providers. However, this model will miss important context about factors that play into how we should allocate mental health resources. Our model might recommend we create an online portal for residents to sign up for free therapy, but it may miss that only 40% of a neighborhood has reliable internet access. We should also think critically if an ML model is appropriate for mental health services at all. In various black box models like neural networks, it can be incredibly difficult to explain exactly why a decision was made. Black box models are those whose explanations for its conclusions remain opaque or black. Since most neural networks have multiple hidden layers, it can be increasingly difficult to understand which elements of a data set are most important. You must also consider who models are interpretable for. A model that helps doctors and radiologists better treat cancer patients, should be explainable to both healthcare providers and to some extent patients. However, this isn't how we currently design AI technology. Neural networks are just one type of probabilistic model, meaning with a small change in inputs or randomization, our neural networks may end up with various different outcomes. ML model should be interpretable because that makes them far easier to debug, but they must also be interpretable so outliers can be dealt with properly. Think about a machine learning model like a simple function. We input different data features and our predictions are the outcomes of this function. If we create a classifier that can identify different species of birds from their images, the inputs for this classifier would be pictures of birds, or specifically, the pixels of the images. And the output would be the model's guess at what type of bird is in the picture as a probability. When a model is complex, data scientists have to use explainable solutions to understand exactly how the model works, either through approximate models or through visual analysis. Ensemble models have proven to be stronger than their parts. And many applications, random forests, which are made up of many decision trees, perform better than any single decision tree. A random forest works by taking into account the predictions of all individual trees when making the final prediction. To make a random forest explainable, we'd have to understand how each tree makes decisions, and even with just a few trees, this becomes a difficult task. To address this issue, technical teams use solutions to understand how inputs and outputs are connected after we've built an ML model. Even state of the art machine learning models often can't provide explanations for their predictions, at least not in a way that humans can easily understand. They use so many features to draw conclusions, even expert data scientists may not be able to understand why a model produces decisions. Explainability and interpretability are often confused, but think about it this way, explainability is why a certain decision was made and interpretability is how a decision was made. It's important to remember both concepts are crucial to responsible AI as we cannot easily be responsible for the outcomes of AI systems if we can't investigate why or how a decision was made.
Transparency
- Transparency is the key to communicating the kinds of harms an AI system might suffer from, as well as the varying degrees of severity. There are some bare minimums we should meet while being transparent about our algorithms, but I'll introduce you to frameworks to go above and beyond to build user trust in your products. Algorithmic transparency is important, but it's not a one-stop solution. Part of being transparent about what machine learning models can do means explaining why a decision was made. In the case of regulated industries like finance, we do this frequently. Credit card companies have to share details about why someone was approved or denied for a loan. We need full transparency when explaining why a model made a certain decision to users. In many cases, we just don't know what the dangers are when we use ML models. Not only are the models themselves black boxes, but they're systematically black boxes in the way many structural harms are. For example, many people don't know that early cameras were optimized to capture the appearance of light skin. These Shirley Cards shaped how we developed the predecessors to modern cameras, but there are artifacts of these methods in today's relevant technology. These issues were eventually fixed because of complaints from chocolatiers and wood furniture manufacturers, not from people of color, despite the fact that they were outspoken about cameras not capturing them well. We must also clearly describe how we automate decisions about users, like on signup pages. Know this can't be legally used buried in the terms and conditions. All segments of our product that make decisions about users should both clearly and succinctly inform users, and also allow them to obtain the information on file about them, and let them dispute that information if they believe it to be inaccurate. This means informing users exactly where in our products we make decisions about them, how long those decisions are good for, and why we use automated decision systems in the first place. We have to be appropriately transparent given the organizational context, like telling users if only marketing teams have access to their automated decisions and relevant stakeholders, like in healthcare applications, where users are all citizens and are subject to automated decisions. The level of transparency needed to properly address disparities in AI is far greater than what companies are used to. As AI has expanded to many industries and is used in high-risk ones, we should have radical transparency be the norm. We can also engage users, increase trust, and stand out from competitors by inviting users to spot algorithmic bias via bounty hunts. Borrowed from information security and successfully run by Twitter in 2021, this process allows users to generate evaluations of the models and expose previously unidentified risk before a major AI incident happens. As a rule of thumb, imperceptible AI is not ethical AI. Good design refuses to sacrifice transparency in order to create a smooth user experience. As your company develops AI, think about ways to embrace transparency. After all, it's transparency that allows us to uncover biased algorithms. For example, a recent healthcare algorithm was identified as racially biased despite not using race as a predictive feature. If the hospital was not transparent about how they built their models and independent researchers didn't inspect it, we'll never know that this model had racial disparities. When you think about transparency, think about conducting and publishing the results of independent audits and opening your data or source code to outside inspection. This transparency is a signal of trustworthiness and figuratively opens the black box to allow users to understand how and why we use AI to make decisions about them. As the public has seen mounds of AI incidents over the past few years, there are many more people who are thinking critically about the products and services they use. Users want their data to be protected and they want to know in what context companies use AI and what they can do about the decisions made about them.
Interpretability
- Interpretability is to what extent a model's decision or results can be communicated. A data scientist might be able to look at model summaries or coefficients of regression models to understand more about how a model came up with a decision. While some models are interpretable as is, others must be made interpretable. The need for interpretable models and industry is far overdue. Often, we build complex models that are good enough, and assume because our application domain may be in social media, that these models don't need to be interpretable. We think, "What's the worst that could happen?" In fact, by not building interpretable models as the default, we've reached a point where many models in real world products cause lasting negative effects on citizen and same users. Take for example a social media algorithm that thinks you're a teenage girl. Well, they compare you to other women in your age range, and populate your feed with what they think you want to see. This can be anywhere from diet tips to fitness supplements, more akin to snake oil. While the algorithm isn't really doing anything wrong it can still reinforce negative habits on human behavior. We shouldn't assume that only high risk applications of AI have the potential for harming people. Interpretability is a major key to being able to debug a model or discover why a model is making biased or unwanted predictions. The rate in which algorithmic solutions are adopted, it becomes more and more critical that we understand how a model makes predictions as discussion on how ML models should fit in our lives is debated. Interpretability is a big problem, but we can face it in two ways. The first is to measure how much a target variable such as the probability a mole is cancerous depends on its features, like its size, color, shape, and other factors. We can do this with visualizations like partial dependence plots and individual conditional expectation curves. Partial dependence plots are an intuitive way of understanding the way a feature impacts an outcome variable. ICE plots visualize the functional relationship between the predictive response and the feature separately for each instance. So a disaggregated version of PDPs. Another method is to create interpretations for black box models. This can be done with permutation feature importance, and individual conditional expectation to more deeply understand black box models. Other solutions to the interpretability problem include using model surrogates and assessing models with partial dependence plots. PDPs, ICE plots, and local interpretable model-agnostic explanations or LIME are considered model-agnostic. They can be applied to interpretable models such as linear regression in decision trees, or to black box models like neural networks. There are also various methods specific to interpreting neural networks, such as learned features, saliency maps, concepts, adversarial examples, counterfactual explanations, and influential instances. SHAP short for Shapley Added exPlanations is an approach to explain the output of any machine learning model. They measure how much each feature contributes to an outcome. Without knowing how and why a model works, it's difficult to determine when a model works correctly, when it fails, and how it can be further improved. Automated decisions made by these models can have far reaching societal implications such as increasing inequality among social class and race, and perpetuating bias and discrimination in their systems.
Fairness
- Fairness is complex and deeply contextual. In AI, fairness is a socio-technical challenge, meaning that it cannot be handled from purely social or purely technical perspectives. Fairness is also really hard to measure. There are 21 quantifiable definitions of fairness, but some notions of fairness contradict each other. Even these 21 quantifiable definitions don't encapsulate other aspects of fairness like equity and justice. Unlike other ML metrics like precision and recall, you can't reach 100% fairness. Even if one fairness metric indicated 100%, other fairness issues might still exist. Prioritizing fairness is always ongoing, and requires us to make imperfect trade offs, and specify who we're working towards fairness for. An AI system cannot be fair for all groups at once, so we typically frame fairness in terms of fairness-related harms, as a mechanism to prioritize the most harmed, and reduce unfairness for those groups. AI systems can cause a variety of fairness-related harms, including harms involving people's individual experiences with AI systems, or the ways that AI systems represent the groups to which they belong. There are various kinds of harm an AI system can produce. They can extend or withhold opportunities, resources, or information, leading to allocation harms. An AI system should also work as well for one person as it does another, even in the case if no opportunities, resources, or information are extended or withheld. When the systems don't work the same way for all, we typically call these quality of service harms. For example, when Chinese iPhone users attempt to use the security feature Face ID, there have been many cases where users' friends were able to get access to their phone. Harms of representation can arise when AI systems over-represent, under-represent, or even completely erase particular groups of people, or subpopulations. Demeaning harms happen when algorithms output things that are obviously offensive, like when Google Photos automatically labeled Black users as gorillas. Some unfair outcomes can appear minor, like not being allowed access into a building because of the facial recognition system, and consistent lighting issues. While this may seem small, these types of harms compound, and if the same groups of people keep experiencing similar harms, it may result in them feeling unwelcome in a space. Additionally, these facial recognition systems tend to fail for people with head coverings, such as nuns or people undergoing chemotherapy, but asking them to remove their head covering is disrespectful. Solutions must be provided in these cases that don't make them feel unwelcome or singled out. Although fairness is often discussed with respect to groups of people who are protected by anti-discrimination laws, such as groups defined in terms of race, gender, age, or disability status, the most relevant groups are often context-specific, and intersectional. Without looking at intersectional groups like Black people who are women, it would be unknown how facial recognition performs specifically worse on Black women than on Black men. Finally, I would be remiss if I didn't highlight that implementation in institutional factors can either improve or reduce fairness, regardless of the fairness of the algorithms themselves. A best practice for organizations deploying algorithms is to put procedures in place to monitor or evaluate the response to algorithmic decision-making artifacts, not just the performance of these artifacts.
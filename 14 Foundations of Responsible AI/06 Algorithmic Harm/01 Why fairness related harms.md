Why fairness related harms?
- When we consider fairness harm specifically, we can leverage a framework for deeply understanding the kinds of harms that can arise because of AI systems. Focusing on the harms caused allows us as practitioners to prioritize the people involved rather than on the sources of algorithmic bias. We've already described the main types of harms, allocation, quality of service, representational and demeaning. These harms are unlike other software issues because AI systems can make millions of unwanted decisions at a time. It's important to recognize the impact our algorithms have and not just the bottom line or expenses we saved. AI has unintentionally caused self-driving car accidents, misidentification leading to innocent people getting arrested and punishment for students looking away from the screen during an online test. These harms can arise quickly, but are initially difficult to detect. One reason this happens is human bias in training data. We've covered how historical bias can exist in data, but data sets are poor quality often because they're limited for some groups. Let's say we only have a hundred samples of Black users in a data set of 10,000. This will lead to major issues because we have two few examples to make a model that can robustly predict for plaque users. Additionally, when we remove sensitive attributes like gender or race without fully understanding what features work as proxies for these attributes, we create unaware models which are not helpful for improving fairness. We build models on data that has strong proxies or ties between factors like zip code and race, they can still infer race based just on zip code, even if we remove the race column from our training data. However, when these features aren't available during training, techniques such as resampling and relating can help adjust the balance of different groups in a data set. Most commonly fairness related harms are difficult to detect. Many times organizations don't even have access to important sensitive features to test for fairness. Unfortunately, this is a massive barrier that requires us to direct our attention to more focused data collection with the objective of training ML models. We need to adopt the practice of tracking model drift over time when we have models in production, but this practice of consistently observing ML metrics is still relatively new. Think about processes to mitigate bias in ML as mitigating the harms caused by AI systems whether they were working properly or not. Mitigating bias can happen in various stages of the AI design and development process. During pre-processing, the goal is to reduce biases in training data before training a model on it. In-processing methods aim to mitigate bias with fairness constraints or regularization. Fairness constraints remove unwanted behavior from a system. For example, we could require our models to approve an even ratio of men and women who have similar credit histories. Post-processing methods reduced bias by modifying the predictions themselves. We can change model predictions to satisfy specific fairness definitions, or for classifiers. We can exploit the low confidence region, and reduce bias close to the decision boundary. In the case of data with historical discriminatory practices baked in, such as hiring, we can use techniques like relabeling and data transformations to reduce the impact of bias and predictions. As this field evolves, we need to take steps to mitigate human biases that are embedded in training data, as well as the AI design and development life cycle. The intensity of these harms varies greatly, but with the focus on people we can shift our development practices to match the outcomes we intend to see. Now that we've understood why we should focus on the harms caused by AI, we'll dive deeper into other AI incidents and what we can learn from them in the next video.
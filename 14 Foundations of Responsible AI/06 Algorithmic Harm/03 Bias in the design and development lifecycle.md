Bias in the design and development lifecycle
- One of the reasons bias is exhibited in the design and development life cycle is because of how we train budding new computer scientists. Most students in the top computer science programs across the country don't receive lectures on ethics, technology and society, or digital privacy, despite coursework on the ethics of technology emerging in the '70s and '80s. For those who are required to take an ethics course, it's often a single class that's taught separately and towards the end of an academic program. This method is common but teaches students how to debate ethical dilemmas and not how to operationalize those morals into technic solutions. This results in poor human decisions when it comes to assumptions about what a model can or can't do. Infamously, there's been research that investigates a person's criminality based on their facial features. Not only is this construct invalid as there's no correlation between immutable facial features and a person's likelihood to commit a crime, it perpetuates many criticized theories in phenology, the pseudoscience, which involves the measurement of bumps on the skull to predict mental traits. We often make far reaching assumptions that some things can be detected or predicted based on the information we have. For example, there are many human attributes such as intelligence that cannot be measured or predicted based on photos of someone. In this case, we've poorly constructed a mental model that exceeds the capabilities of AI. The decisions development teams control include what data should be used, how it should be cleaned, which ways we want to deploy models, and what metrics we should use to measure success. But people are still at the core of AI decision making. Data scientists and ML engineers get to choose what evaluation metric they benchmark ML models against. While there's a variety of metrics to choose from, we hardly ever document why a metric was chosen other than it was the best performing metric. This bias towards choosing metrics that make our models look better isn't uncommon. Every area where humans are in charge of how we develop AI has the potential for bias to sneak in. To do better, we need to leave a paper trail of documentation to guide teams in the future. We also need to embed social and behavioral scientists on our engineering teams and value the feedback of users and those most impacted by our models.
Technical aspects of sociotechnical solutions
- We've already discussed how important it is to have social and behavioral experts at the forefront of AI development. So, let's discuss some of the ways to address bias with statistical tools. Studies have shown that we can't create somewhat fair AI without investigating data sets that include sensitive features such as age, gender, and race. Because algorithms can still be biased even without being trained on sensitive features, we have to work towards mitigating bias. Non-sensitive features in our data set may be correlated with sensitive features. So, if we remove those features, we don't know our algorithms exhibit bias. In general, mitigating bias in the outcomes of machine learning systems is the goal of responsible AI. First, if we want to make bias systems better, we have to identify what kinds of bias they exhibit. Then take steps to mitigate those biases. For identifying bias in AI there are four common methods typically used in industry, disparate impact, statistical parity, equal opportunity, and average odds. You might want to jot some of these definitions down. The first is disparate impact which states each group should have an equal opportunity of achieving a favorable outcome. We find our disparate impact score by calculating the ratio of the rate of favorable outcomes for unprivileged groups compared to that of privileged groups. The ideal value for disparate impact is one. And a value less than that implies that there's a benefit towards one group. To infer statistical parity, we look at the demographics of those receiving any classification and they should be the same as the demographics of the underlying population. Statistical parity is calculated by taking the difference of the rate of favorable outcomes for the privileged group by the rate of favorable outcomes for the unprivileged group. Ideally, we'd want our statistical parity value to be zero which implies there's no benefit towards either group and they have the same rate of getting favorable outcomes. Scores under zero mean that there's a benefit towards one of these groups. Equal opportunity is a metric that attempts to make sure each group is incorrectly classified equally. This is done by taking the difference of true positive rates between unprivileged and privileged groups. We also want equal opportunity to equal zero and anything under that means that one group is receiving benefits over another. Equal opportunity attempts to measure how two groups are incorrectly classified but average odds takes the average difference of false positive rates and true positive rates between unprivileged and privileged groups. Score of zero here indicates each group is incorrectly classified equally and any negative number means one group has privileges over the other. Let's talk about more ways to mitigate bias in AI systems. The first and often most effective method is by setting fairness constraints. This allows us to specify a trade off between a classifier's fairness and its accuracy. Sometimes, for incredibly badly biased data sets, an accurate classifier might be unfair like a data set of past hiring decisions or a data set of arrests. To correct for this, we can set fairness constraints like a minimum disparate impact score so the classifier is as accurate as possible while exceeding the minimum we set for disparate impact. Reweighting is another method to mitigate bias that allows us to reweigh examples in each group or label combination differently so that we can ensure fairness before classification. This involves looking at the protected attribute and the real label. Then we calculate the probability of assigning favorable labels assuming the protected attribute and our target attribute are independent. Then the algorithm divides calculated theoretical probability by true empirical probability of this event to create our weights. We pass those weights as model parameters. However, some models don't have weight parameters and can't benefit from this method. Resampling duplicates observations from a deprived subgroup when the label is positive and omits observations with a negative label. The opposite is then performed on the favored group. In optimized pre-processing, we run a probabilistic transformation that edits the features and labels in the data with group fairness, individual distortion, and data fidelity constraints and objectives. Adversarial debiasing involves training a classifier that maximizes prediction accuracy and simultaneously reduces an adversary's ability to determine the protected attribute from the predictions. This approach leads to a fair classifier as the predictions cannot carry any group discrimination information that the adversary can exploit. Lastly, rejected option based classification changes predictions from a classifier to make them fairer. It provides favorable outcomes to unprivileged groups and unfavorable outcomes to privileged groups in a confidence band around the decision boundary with the highest uncertainty. In some cases, the most ethical thing to do is collect new data. It's possible that using one or more of these methods still doesn't meaningfully reduce the bias of model predictions. The next step in that case is to explore new data collection techniques and retrieve data that's relevant and up to date for the model that you are building.
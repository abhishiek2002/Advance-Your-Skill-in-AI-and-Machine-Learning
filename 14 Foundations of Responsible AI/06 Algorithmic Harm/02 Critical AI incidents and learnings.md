Critical AI incidents and learnings
- You may have heard about various AI incidents that received major press, but let's talk about what we can learn from past mistakes. We can think of an AI incident as a failure of an AI system that harms people in some way. What we can learn from past harmful AI incidents is that the impact of our systems are more important than the technical details. There's so many past models that have hurt users and citizens alike. Let's dive deeply into an AI incident with widespread impact. An apartment in New York City installed a facial recognition system to replace their old key fob system. The neighborhood around Brooklyn's Atlantic Plaza Towers is predominantly Black. This fact already makes any facial recognition system less effective, but the system was also used to surveil and punish residents. In this incident, 130 residents protested the installation for various reasons, such as weariness over the use of their biometric information, lack of regulation around facial recognition tools, and potential abuse of their personal data. So what's the takeaway? Don't build systems without the input of those most affected. As seen in the Atlantic Plaza Towers case, when we build AI systems that won't be used on us, at a bare minimum, we should include a diverse pool of members from relevant communities to give input on the systems we build. Consumers are more educated than ever about how AI impacts them. After nearly a decade of "AI is coming for your job" articles, consumers care and are doing their research on which companies respect their data, and which don't. Let's go over an example from my own experience. When I was building ML models to inform companies like Uber and Lyft of how risky a potential driver was, I had some knowledge of what the day-to-day was like for rideshare drivers. I was a rideshare driver throughout grad school and could remember how tough it was to make ends meet. While this experience had some impact on how we developed our models, we should have had a pool of rideshare drivers of all genders across the country give us feedback and point out where our assumptions were wrong. Usually, we're good at clearly stating our assumptions when it comes to a data set, but more broadly, a common mistake is assuming we know how to build domain-specific machine learning as well. After all, our methodology was flawed. Just because you get a lot of tickets doesn't mean you're a bad or risky driver. This may sound counterintuitive, but the amount of traffic violations you receive is highly dependent on where you live. Most drivers are aware that some neighborhoods are speed traps or areas with low speed limits and are more highly policed for traffic violations. This is the case in Brookside, Alabama, a city recently in the news for relying on traffic violation revenue when it hired its new police chief in 2018. The town has merely 1,200 residents but has one police officer for every 144 of those residents. The police chief helped increase the town's revenue from ticket fines and forfeitures from 82,000 in 2018 to 610,000 in 2020. Now, given what we know, would you still compare a driver in Brookside to one in another city based merely on the number of violations? No, we'd want to look at the drivers relative to others in their city or region. This is the flaw with much of machine learning. For the vast amounts of data we have, we build models that make bad decisions because we exclude vital context. In other more grave examples of flawed systems, there are instances of recommendation algorithms recommending poisonous substances to online shoppers. This shows us that the harms that we cause are not just political or financial. There are real lives at stake, and we must be responsible for how our algorithms impact human lives. The consequences are not just bad movie recommendations or flawed healthcare models. Something seemingly innocuous as a product recommendation can mean life or death. And we must reconcile with the fact that technologists alone cannot address this issue. AI systems need proper safeguards. Without safeguards, mitigation plans, and recourse, it's impossible to create AI responsibly. When AI systems hurt people, we have to acknowledge the harm and operate from a place of humility and respect. After all, it's our model that harmed them. AI systems often act in ways we don't expect, so we must prepare to reduce and mitigate the harms these systems perpetuate.
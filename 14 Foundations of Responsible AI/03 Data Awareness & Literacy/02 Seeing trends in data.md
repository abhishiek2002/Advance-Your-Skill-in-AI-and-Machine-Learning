Seeing trends in data
- The ability to analyze data or identify trends in datasets is essential in this data-driven age. First, in order to identify trends in data, we must have a baseline understanding for what we think our data represents. A problem relevant to identifying machine bias is construct validity or how well our data matches the constructs we assume it represents. There are many observable properties in our world, such as skin tone, facial expression, and someone's vital signs. However, algorithms often fall short when we attempt to use observable properties as indicators of unobservable phenomena, like the range of human emotions. A person's race is also unobservable, although we use proxies like skin tone and other physical features to estimate what someone's race is. This is where even ground truth data fall short. As there are many measurable attributes, we overestimate how these attributes represent patterns and other phenomena we're interested in. Even as we train statisticians and data scientists to identify patterns, what we teach and learn are heavily influenced by specific perspectives. This is one of the many reasons teams should work with a variety of professions, including those in behavioral and social science. Identifying the relationship between our target variable and feature variables is our entire goal, and we can use what we've learned to inform how we develop models that use this data. When we do find trends in data, we should be able to dig further to see if these trends are meaningful. There are many examples of spurious correlations, like the divorce rate in the State of Maine is correlated with the per capita consumption of margarine. Clearly, in this example, there isn't a relationship between the two, but they are correlated. This is a good reason for working closely with domain experts and social scientists to verify if correlated features are meaningful. We also need to consult user experience researchers and those who experience the downstream effects of machine learning models. This is a very broad group that can be the general public in the case of healthcare models. For those in organizations that create machine learning, it can be difficult to identify all of these groups. For example, imagine you're on a technical team trying to develop a new audio note taking app. You may consider several factors when trying to create datasets representative of your deployment audience. While this task is crucial to developing robust AI systems, the difficulty of this process is often understated. You might pay attention to the geography of users and try to match representation to your current user base. It should be inspected how close your user base is to the general population of where you're headquartered. If you're in the United States, should your representative data match the racial and gender breakdown observed in the population? If so, is that a breakdown that would provide positive outcomes for all users, especially those in the least represented groups? This is a problem for those who study the societal impact of technology to help us decipher what's appropriate in our context and how to communicate this clearly. Aside from the decision on what constitutes representation, we can't look at just a few factors for demographics. We need to understand the factors like social class, gender identity, age, and health as they can also impact language variation. Next, on the technical path, we must consider what sampling methods are best suited for our data as well. While most groups tend to lean towards convenient sampling, it's recommended that groups use engineered sampling methods or stratified random sampling. In stratified random sampling, each individual in the group has the same probability of being selected, but it allows for proportional representation of specific groups. So it's starting to get complicated, I know. Let's have a look at another example to make it a little clearer. In this demonstration, you can see that we're getting some results that don't make sense for our data. We just can't seem to find a linear model that works well for both naturally occurring groups in the data. If we try a model like polynomial regression, we can better find a line that passes through both groups. Commonly, we attempt to use modeling techniques that don't match the data we're dealing with when we don't have a firm grasp of the data at hand. So to identify trends; we should have a critical eye for historical bias, stay aware of varying cultural norms, recognize that not all trends are meaningful, and rely on expertise from domain experts when we're unsure.
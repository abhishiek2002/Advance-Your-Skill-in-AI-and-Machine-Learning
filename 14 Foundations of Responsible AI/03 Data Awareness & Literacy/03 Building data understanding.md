Building data understanding
- Let's flick some of our bias identification skills by looking at historical housing data, despite our goals it's unfortunately easy to perpetuate bias with our algorithms. Let's remember for a moment that the purpose of machine learning models are to extract some kind of understanding of why past decisions were made and apply that logic to new data. Housing data represents actual housing decisions like who is approved for a home loan and the humans involved in making those decisions may have been discriminatory. Thinking critically about what could go wrong when using this data is crucial because it's simple to perpetuate biases with our algorithms. In this example if we investigate just one protected class, race, we can see that systemically homes in Black neighborhoods are sold for less and appreciate in value slower than homes in mostly white neighborhoods. If we train a model that's 90% accurate on this data we'll create home valuations that are just as skewed and withhold financial opportunities for Black homeowners. It's easy to unintentionally create automated tools that behave in the same biased ways as humans do. We can't train models on just bad decisions because they were what we had available. Training ML tools to learn from the past human decisions assumes that those decisions were correct. So what do we do if historical data is all we have? We can take steps to greatly mitigate the harms of unfair algorithms. This requires us to identify and prioritize specific groups of people. By prioritizing people first, we reduce some of the biggest algorithmic harms rather than attempting to solve for every unintended consequence. Unfortunately, we can't attain fairness for all groups at once. However, if we focus on the communities that experience the most harm we can mitigate the damage caused by our systems dramatically. Identifying these groups can be incredibly difficult and requires domain specific knowledge.
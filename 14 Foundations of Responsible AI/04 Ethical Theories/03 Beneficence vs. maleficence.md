Beneficence vs. maleficence
- There are various reasons the do no harm stance hasn't helped us create ethical products. By attempting to do no harm, many development technologists think of ethics as a matter of finding technical solutions for technical problems. This problem has been identified as tech solutionism, or the conviction that problems caused by technology can always be fixed by more technology. We need a new design process for responsible AI and building minimum ethical products instead of MVPs is one way to do it. As we see ethically misaligned products appear and face reputation-damaging criticism in various industries, trust in AI products is low and encourages technophobic attitudes that limit society's ability to make the most of groundbreaking science. Our minimum ethical product should have elements of AI control, meaning we should be able to pause predictions without losing all product functionality. A minimum ethical product should meet all ethical standards and principles, even if it may not meet all business needs. The purpose is to start with a system that behaves ethically, however we define that. Then slowly add functionality that helps us meet business goals. Beneficence is the guiding principle for developing AI this way. Deploying minimum ethical products also involves the careful monitoring of how those products impact users. Sometimes we don't want to surface decisions like in a shadow deployment context. A financial services recommender might exceed expectations in predicting fluctuations in the lives of middle class insurance subscribers. But it may also hide income protection packages in their product field. This means if customers may not be able to access emergency funds, they may be in more difficult financial situations due to an AI system. We shouldn't ignore research that critiques AI systems as it's not a war between machine learning developers and AI ethicists. While we have widespread adoption, we also have widespread damage to human life. So instead of being laser-focused on groundbreaking new metrics, we should recognize that critical research is necessary for healthy growth and adoption of AI. If we build an AI system with the goal of ending global warming, it could recommend that we just blow up the planet. Frightening, isn't it? Technically that solves the task, but remember, purely technical solutions don't always work the way we expect. We should be asking questions like, what is the goal you're trying to achieve by creating this AI? Is there a need to use AI to achieve this goal? Can you use other non-AI related methods to achieve the same goal with lower risk? And lastly, is AI even likely to be effective in helping us achieve this goal? A tactic that's growing in popularity is to align values by first creating models that provide information for making decisions. For example, instead of building an AI system to automatically make decisions on loan applications, we can build a model that can be used to ensure enough employees are available to process those loans. Some of the most successful AI applications have values aligned for three parties. The person interacting with the system, the creators of the system, and the society in which the creator and the interactors live. We shouldn't forget that many people interact with AI and many don't know the system uses AI or they haven't been given a choice to opt out of interacting with AI. In other words, not everyone is a willing participant when encountering AI systems. So do you have an idea for an AI system that will increase your company's revenue by 20%? Awesome. Go and check how that initiative coincides with the people that will be interacting with your system and how society will look upon your idea. How does it measure up? You probably discovered there may be drawbacks or consequences for one group. We'll discuss consequences more in the next video.
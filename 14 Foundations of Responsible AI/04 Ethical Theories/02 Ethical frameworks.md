Ethical frameworks
- Any system created to find patterns in data might find the wrong patterns. A recent IBM study revealed a global shift in the job roles responsible for managing and maintaining AI ethics. When asked who was in charge of AI ethics, 80% of respondents identified a non-technical executive, such as a CEO. A dramatic increase from just 15% in 2018. Machine learning relies heavily on data. And while the math in an algorithm isn't biased, when trained on biased data, ML can easily become a tool that expands inequality. It's a common misconception that AI is objective. AI is objective only in the sense that it can make decisions based on what humans teach it. The data, methods used to acquire it, and context in which AI systems are deployed are not merely unbiased math. It's important to know it's not just the data that causes issues in AI systems. The opinions and beliefs of the people involved in the development process are also encoded in the development life cycle. We have a legal precedent to create AI systems that are responsible. The four-fifths rule says that a selection rate for any race, sex, or ethnic group, which is less than four-fifths or 80% of the selection rate for the group with the highest acceptance is evidence of adverse impact. If we have 135 students apply for admission to a selective medical program, the gender breakdown might look something like this. 72 men applied, and of those, seven were admitted. Of the 63 women who applied, three women were admitted. The acceptance rate for men is about 10%. So the acceptance rate for women should be no less than 8%. However, we've only accepted 5% of women who applied, creating an adverse impact for women who applied to this program. Moving into the frameworks used to evaluate ethics, ethics has a pretty straightforward dictionary definition. Moral principles that govern a person's behavior or conduct. But it's an incredibly complex subject. Let's cover some ethical schools of thought to know about. The first is consequentialism. Meaning the greatest happiness of the greatest number is the foundation of morals and legislation, also known as the ends justify the means. It's a close cousin of utilitarianism. Utilitarianism is a family of ethical theories. It conceives benefits as actions that maximize wellbeing across all affected individuals. Utilitarianism is a version of consequentialism which states that the consequences of any action are the only standards of right and wrong. Next is deontology, which proposes it's our duty to always do what is right, even if it produces negative consequences. Pragmatism states that morals evolve over time and that our rules should take this into account. Moral institutionalism tells us that it's possible to know what's ethical without prior knowledge of other concepts, such as good or evil. Moral relativism is the view that moral judgements are only true or false relative to some particular standpoint. For instance, that of a culture or a historical period, and that no standpoint is uniquely privileged over all others. As an organization, if you're transparent about your ethical principles, it's your duty to think about and decide what ethical branch you base your principles on. To be clear, there is no single ethical framework to rule them all. AI ethics depends on industry, social norms, and regulations governing your industry. We must investigate which ethical framework is most relevant to our field. Not all guidelines you'll find out there are relevant. For example, if AI is applied in the field of media and communications there's more to be concerned about with the impact of AI on free expression, unwanted impersonation, like deep fakes and fake news, as opposed to technical displacement by automation. So if ethics is so complex, what do we do? Prepare to build minimum ethical products. The landscape in terms of responsibility is constantly evolving. But instead of waiting for regulations to force action on your team, practice delivering a minimum ethical product with as much transparency as possible and use this to gather feedback from users and downstream direct or indirect stakeholders. Then, use your newly acquired evidence to steer your responsible AI development. Choosing an ethical framework is no easy task but the key is to rely on experts in responsible technology. There are many consultancies that focus on helping teams identify their ethical shortcomings and retool their products and help turn ethical frameworks and standards into action items for executives and technical teams.
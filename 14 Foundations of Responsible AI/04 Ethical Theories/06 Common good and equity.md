Common good and equity
- When discussing ethics, we often talk in terms of what's good for the majority of people. And philosophy, the common good, refers to what is shared and beneficial for most members of a given community. How does this relate to how we develop technology? Well, one moral philosophy we've already discussed has a relatively simple method for deciding whether an action is morally right or not. We'd call this utilitarianism. First, we outline the various actions that we could take. Next, we estimate the benefits and harm that would result from each action. Lastly, we're going to choose the action that provides the greatest benefits after the costs have been taken into account. Let's imagine what a product development practice would look like if we took the utilitarian route. First, we'd sit down and outline our options. If this is a facial recognition system, we could choose to build a model knowing we have data that doesn't represent our entire population. We could also choose to take mitigation steps to collect our own data or use benchmarked data sets with care. After investigating the potential harms of our facial recognition system, we may also decide to abandon the project. So if these are our initial actions, we'll work out the benefits and harms of each. During this step, we'll consult with domain experts and user preferability groups. This requires us to also think about the demographic groups that have seen the most harm from similar technologies. For example, with facial recognition, the Gender Shades study revealed that darker skin women have quality of service harms and larger accuracy disparities compared to other groups. Once we've had these nuanced conversations, we can move forward with the action that provides the greatest benefits while documenting our decision-making process and considerations in detail. The next concept I want to introduce is equity. First, let me say equity is a contested concept and it requires domain-specific examination to know who we want to create equity for. Everyone should have access to the benefits of AI, while harms are being mitigated through various methods. It's important to ensure that potential benefits of AI don't accumulate unequally, and are made accessible to as many people as possible. One great example of this is the article "Hiring Algorithms Can't Be Fixed by Employers Alone" from "Weapons of Math Destruction" author Cathy O'Neil. In this, she proposes a method for ensuring that artificial intelligence doesn't perpetuate or worsen discrimination in hiring. If we only create equality going forward, we do nothing to take accountability for how our algorithms have perpetuated inequality thus far. Even when a system is accurate and works perfectly, it can still cause harm. The data that an AI system relies on can reflect previous discrimination or the system can be applied in unjust ways. As practitioners, we should seek not only to create equal outcomes across groups, but to use our systems to help allocate additional resources to the historically marginalized. If we think about historical marginalization in terms of technology, women and non-white people have had algorithms use biased data to allocate them less access to resources like financial systems. Equity is a contested topic because it's relative, and begs the question: Equitable for whom? Whereas equality means providing the same resources to all, equity means recognizing that we don't all start from the same place and we must acknowledge and make adjustments to imbalances. There is no magic bullet, and the answer varies for each organization, but we should at least do no foreseeable harm and value justice. Justice requires designers and deployers to think carefully about harm, and unavoidably harmed users may be entitled to compensation and remediation. There are some considerations that can be made for specific domains. If you are doing machine learning work in healthcare, mostly based on data in the United States, you must understand how some demographic groups are statistically underserved. The Department of Health and Human Services characterizes underserved, vulnerable, and special needs populations as communities that include members of minority populations or individuals who've experienced health disparities. This includes Native American, Latino, Black, and Asian populations, as well as refugees, people with disabilities, religious minorities, new mothers and women with children, and individuals with limited English proficiency amongst others. These groups are not merely captured in demographic information, but require deeper investigation. So, for disparities in healthcare, teams must consider which demographics and sensitive attributes must be prioritized. Most frequently, we choose these groups based on levels of historical harm and marginalization. In an ideal world, everyone benefits from AI with minimal harm. Developing systems that achieve this goal in a way that's just and equitable is an ongoing process, but it's well worth the time and investment as we continue to learn how to make systems that work for as many people as possible.
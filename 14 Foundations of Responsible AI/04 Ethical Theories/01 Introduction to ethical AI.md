Introduction to ethical AI
- It's crucial to understand the many different ideas in ethical AI in order to choose an ethical standpoint for building AI systems. Unfortunately, ethics is complex. It also varies from culture to culture and cannot be implemented by technical means alone. We should shift to a focus on responsibility, this way we can reduce the harm caused by irresponsibly developed AI systems. Let's take a step back from the nitty-gritty of AI for a moment. When we consider ethical dilemmas like the trolley problem, we build mental models for the cost of human life. The trolley problem poses that you're a driver of a trolley and around the corner are five people strapped to the track, but there's a switch you can pull and divert your course and only hit one person strapped to the track. There's no way around it, no matter what you do, someone will die. The ethical dilemma is do you do something and are actively responsible for someone's death while reducing the number of deaths, or do nothing and more people die. This example has been used to discuss the benefits and disadvantages of technologies like self-driving cars. This process of quantifying human life isn't new or exclusive to moral philosophy. What's interesting is not everyone has the same opinion on what we should do. In Japan and China for example, people are less likely to harm the elderly. Poorer countries tend to be more tolerant about law breakers and extend more empathy to those experiencing poverty. More individualistic countries generally prefer to spare more lives. Using just Western philosophy to develop global technology is a flawed method. This is why creating algorithms is highly contextual and should be created for one population at a time. Here's an example from a recent real life situation. in Arizona, doctors were instructed to evaluate the probability of a COVID-19 patient living more than five years. And for those who don't meet this criteria, denying them a ventilator, a machine that breathes for human lungs, and thus reducing their overall chances of survival. This is an example of an allocation harm in which the human algorithm or decision-making system harms patients by withholding resources, in this case, a ventilator. In this scenario, the number of positive infections was far larger than the number of available ventilators so some system had to be used to help hospitals prioritize. So, if we're looking purely at probability, a patient who's 80-years-old has a lower probability of living five more years than a patient who's 40, even without considering their actual health and comorbidities. By this metric, all elderly people will inherently be less likely to receive a ventilator compared to someone who's younger, even if they have other underlying health conditions or present worse symptoms. As you can see from these ethical examples, it can be challenging to make decisions based solely on data. They're often cultural and societal norms that come into play, which is why it's important for us to investigate how we're developing AI systems. Remember, algorithms aren't just data and code, they've existed in various forms for hundreds of years. The only difference now is the speed at which we're able to make these decisions.
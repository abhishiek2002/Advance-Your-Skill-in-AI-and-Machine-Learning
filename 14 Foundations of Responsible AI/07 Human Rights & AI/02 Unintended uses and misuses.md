Unintended uses and misuses
- When we talk about AI causing harm, it's because of the ways it's misused and used for unintended purposes. For example, various ML tools have been used for harmful reasons, such as deep fake technology to mimic human subjects. When a machine learning tool is misused, it's often leveraged in ways outside of what developers intended. Deep fakes and audio reproduction technology are used to falsify news video from world leaders. AI has frequently been misused to socially control and manipulate millions, via propaganda and mining personal data for the sake of monetization. This kind of misuse is common and really hard to identify. We also have to recognize that many of the reasons AI is misused is because we're unsure of the purpose of a specific model. Academic papers introducing new algorithms don't always tell us how they expect the algorithms to be used. Then, we have the dual problems of open source versus proprietary algorithms. When we build ML for use in our organizations, we often don't expect those outside of the project to use our models or their insights. However, some companies later look at these ML artifacts and decide it might be a new good revenue stream and thus sell it to other organizations. What happens next is that customers who use these ML APIs now have access to tools you created without all the knowledge you collected on the way there. One way of remediating this issue is by creating documentation workflows and consistently maintain repositories of known issues and key decisions. Our issues backlog should also include concerns raised by those within the organization to further investigate areas for potential bias. Often, we have product managers to groom these items in our issues backlog and remove them as they've been addressed. Typically, these live on a platform like Jira and go hand-in-hand with the key decisions backlog. The key decisions backlog documents decisions and trade-offs made by the product team, core data ethics team, and executives that have guided the development of AI systems. When companies sell AI services, such as facial recognition APIs, to other companies, it can seem murky whose responsibility it is when models go wrong. What we see in the real world is that a company may buy an AI API, deploy it within their product, then realize it's making some not-so-great decisions. These companies usually turn to their API supplier with worry, especially that it will cost them legally or financially. Typically, when companies sell AI, somewhere in the terms it states the selling organization isn't responsible for what happens to end users. This leaves buying companies on the hook and with little expertise to fix the problem. So we have models in the wild making predictions and companies who know little about the algorithm itself to try and fix the issue. Take a moment now to consider how misuse and unintended uses might change the impact of AI systems. What should companies developing these AI systems change in the development process? First, we produce clearly-documented reasons for developing the model. Next, we provide any fairness evaluations that were done and the steps to mitigate harm in the case of unwanted predictions. And lastly, we work with security analysts to identify vulnerabilities. Development teams should create a data sheet for every data set used to train an AI system, and they should create a model card for every ML model they develop. These documentation tools require teams to think deeply about and document intentions and outline misuses the companies who may buy access to their API can rely on. Taking all of these steps for any large-scale model must be done to ensure you monitor its function, as well as identify issues that hackers may exploit. In the next video, we'll cover unethical business cases.
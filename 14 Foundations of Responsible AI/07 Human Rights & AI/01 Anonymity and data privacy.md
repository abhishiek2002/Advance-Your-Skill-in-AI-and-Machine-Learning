Anonymity and data privacy
- While there are a few laws that address data privacy and its role in ML and AI, we can look forward to new legislation in the near future. For now, the General Data Protection Regulation, or GDPR, provides a few guidelines on how to communicate ML decisions with users, but it's only applicable to companies doing business in the EU. There are several relevant concepts here. Anonymity, pseudonymization, and differential privacy, are steps we can take to protect user data. Pseudonymizating data means we remove all personal details, and put a customer number in its place. In another database, all the matches to the customer number and actual identity are stored. So if you give out just the transactions database, no one can tell who anyone is. But the data is reversible if a third party gets access to your customer database. With the frequency of large-scale hacks in the last few years, this isn't really a safe way to deal with privacy. According to the GDPR, pseudonymization and anonymization are not the same thing. Pseudonymization is not a method of anonymization, and the former merely reduces the linkability of a data set with the original identity of a data subject. And it's reversible if the original data is accessible. Data anonymization is a major part of protecting user privacy. So anonymization is the process of changing personal data, in order to irreversibly prevent identification. Anonymized data still has some of the important attributes, which allow the data to be analyzed for insights. In an anonymized data set, you can still ask questions like, "What is the most common state our customers live in?" Which wouldn't be possible using other methods like leveraging synthetic data. Synthetic data is annotated information that computer simulations or algorithms generate as an alternative to real world data. It should be noted that variables that are not identifiers can still supply context, which may lead to identification. For example, Netflix release data on movie ratings, but they removed user names and randomized ID numbers and hoped this was enough. However, MIT researchers were able to match these anonymized data sets to Amazon users, via similar ratings on Amazon. This example exposes a flaw in the use of anonymized data. A third method called differential privacy enables researchers and analysts to extract useful insights from data sets containing personal information, and offer strong privacy protections. A crucial feature of differential privacy is that it defines privacy not as a binary notion of, "Was the data of the individual exposed or not?" But rather as a matter of quantifiable privacy loss. Every time a person's data is processed, their risk of being exposed increases. Algorithms are said to be differentially private, if by looking at the output, one can't tell whether an individual's data was included in the original data set or not. As a result, the behavior of a differentially-private algorithm hardly changes when a single individual joins or leaves the data set. Differential privacy involves introducing enough statistical noise to protect the privacy of an individual, but small enough that it won't impact the accuracy of the predictions. The noise hides identifiable characteristics of individuals. So for a user who lives at 1001 Main Street, adding noise might look like they live at a completely different address. Here are three tips for increasing user privacy. First, only collect data for specified and legitimate reasons. This goes against the norm of collecting and saving everything, and hopes we can find insights in it later. Second. Process personal data in a way that protects its security and confidentiality. And finally, communicate your intentions with personal data, and inform individuals how their data is being used and collected.
Unethical business cases
- Since the advent of AI's popularity in Silicon Valley, more and more companies have adopted the idea that AI is a must-use tool for creating revenue or avoiding profit loss. However, some of the reasons we may use AI are the root cause of bias in AI. Big data, as a means to oppress people, is not that new. There are several business cases that are common in industry, but stand at odds with the goal of creating fair AI system. Let's start with the field of human resources. There have been multiple studies showing that human biases against race and gender play a large role in who gets hired. Does this mean we should use AI instead? Not really. Several AI-powered hiring tools have been proven to show high levels of disparate impact. So why are companies still buying and using them? The largest companies selling AI in HR don't spend enough time studying, re-weighting, or investigating historical data that in-turn skews the AI models they create. Plus, automated tools have shown dramatic biases, not that different from the bias exhibited without automating HR tools. So at this time, given what we know about human bias, and machine bias, HR is an area where using historical data to train AI that ranks, selects, or matches potential employees with jobs is unethical. Another area prime for reckoning with algorithmic bias is mortgage lending. Unfortunately, these biases are not indicators of a team's technical skill or intent, but often of industries rife with inequality that cannot be fixed by training pattern recognition systems on them. Even when the people writing the algorithms intend to create a fair system, AI systems still have a disparate impact on minority borrowers. Many times in industry, an 80% cutoff is used, meaning that any disparity ratio between 0.8 and 1.2 is an acceptable amount of disparate impact. However, even when models are below the disparate impact threshold, they can still have disparate impacts without breaking any laws. Remember, disparate impact is when each group does not have an equal opportunity of achieving a favorable outcome. Surveillance is another vertical where the use of AI to power surveillance tools is unethical. Surveillance has a long history of bias, unfair targeting of communities, and dubious consent practices. AI enables populations to be surveilled faster, easier, without proper regulation, and surveilled in ways that are easy to withhold from the public. Often, surveillance companies advertise the safety of neighborhoods as a reason to invest in flawed surveillance practices. But this violates the privacy and safety for many of those subject to surveillance. Whether that's for student online exam surveillance, or for monitoring employee internet activity, surveillance tools that leverage AI are almost always coming from objectives that are met when we manipulate or further marginalize certain groups of people. Earlier, I touched on how construct validity issues can impact machine learning models. When we have the intent to build a model based on an invalid construct, such as identifying distraction with eye movement, we're setting ourselves up for failure. Not all issues of construct invalidity are unethical. However, many are. What is deemed unethical depends on various aspects of the technology's context. For example, when we attempt to determine someone's gender based on their image, we're looking at an unethical and infeasible task. Gender can't be detected by photo. And by doing so, we're making harmful assumptions about gender, which is unethical. A business use case may be unethical for a variety of reasons, including the deployment context, implicit or explicit objectives, or who the use case enables. For example, a crisis text line broke the trust of many users by training machine learning models on texts people in need sent to crisis responders. While most users didn't know their messages would be used to train conversational agents businesses could later purchase, it didn't technically break the crisis text line's terms of service. There was a large backlash to this decision as users could have at least been informed their information would be extrapolated for business use. But we should also consider if using some types of machine learning are even ethical in mental health services. As you can see, there are many examples of unethical use of AI in business. Can you think of others you've read or heard about? Take a moment now and write down any examples that come to mind.
Challenge: Finish the multilayer perceptron class
(bright electronic music) - [Presenter] Are you ready for your next challenge? Now you must write two methods so we can test our multilayer perceptron. First, we have the set-weights method in line 56. For the W-init argument, you may use the organization you want, but I suggest that you make it capable of initializing a network of any size. Don't forget the bias weights. Also notice that I have written a printWeights method starting at line 61. This is for you to check if your neural network received the weights correctly, and to see the weights when you have trained it later in the course. And lastly, we have the run method starting at line 68, which feeds a sample to the network and returns a NumPy array with the output values. Let me tell you what to return, just to be clear. We simply return the last element in the values array, which is a NumPy array containing exactly the output layered values. So to recap, you must write the method to write values to the weights, and the run method to produce an output. You can test your neural network with the weights we just saw to make it behave as an XOR gate. This shouldn't take you more than 15 minutes, so have fun coding and come back to look at my solution.

Solution: Finish the multilayer perceptron class
(bright upbeat music) - [Presenter] Here's my solution. For the set-weights method starting at line 54, I'm implementing W-init as a list of lists of lists. That's three dimensions and that's because I'm specifying the layer, the neuron, and the input associated to each weight. However, W-init will have one less entry in the first dimension because I'm not specifying anything for the input layer as it has no neurons. So I implemented two nested loops starting at line 57. The outer loop iterates I through the layers in the network, and the inner loop iterates J through the neurons in each layer. Now, inside the inner loop, I'm using the set-weights method for each neuron. Notice that since W-init doesn't have anything for the input layer, I am indexing the network array at I-plus-one. Now let's look at the run method starting at line 68. So here, the first thing I do is turning X into a NumPy array. Then, I copy X into the first layer of the values array. Now it's time to run a two-level nested loop for every layer in ascending order, and every neuron in each layer. The body of the loop is simply running the current neuron by feeding it the values in the previous layer. That's it. Now we are ready to run a test on the whole thing starting at line 79. So first, we create a multilayer perceptron with the dimensions of the XOR gate design. Next, I'm assigning the required weights. These are the weights we just saw. Next, I'm printing the weights, just as a sanity check. And finally, we have four print lines to test our network. So let's run it. And here's the result in the terminal at the bottom. As you can see, it has indeed taken the weights we wrote and it's behaving as an XOR gate. Give yourself a pat on the back.
7 to 1 network GUI demo
- I wrote three applications for our segment display recognition neural network, all with a graphical user interface to allow you to see the neural network working as you tweak the input values. Please note that you cannot run these demos in the GitHub code space because of the gooey elements. So if you'd like to follow along, you'll need to have Python installed in your computer, along with Non-py and TK inter. What you are seeing right now is my local installation of Visual Studio Code. The source files are called SDRNN_7to1, SDRNN_7to10, and SDRNN_7to7.py respectively and they all import MLP.py, so you may try your own neural network with them. For your convenience, all these files are included in the code for the three demo videos. If you would like to read through these source files, be advised that they contain a lot of code to implement the graphical stuff, so you may be interested in two functions that deal with the neural network. These functions are present in all three implementations. The first one is called train callback and that's where the back propagation happens. You can see it here starting at line 73. The second one is called run.ann and that's where the neural network is run once every time the user changes the input controls. I'd rather show you the rest in the running application. Let's start with the 7 to 1 model. So first, let me tell you about the user interface. At the left, I created a set of seven sliders to act as the segments in the input. Moving them causes their color to simulate becoming brighter. At the right we have parameters and results. You may enter the number of Epochs to train next which will happen when you press the button. Then as a result, the application displaced the last reported training error, the number of Epochs trained so far, the raw output value, and finally the recognized number in a big, bold font. So let me train this network in steps of 100 epochs. Pay attention to the training error as it drops. I'll keep training until the error drops below 0.0001. In other words, when it reaches four zeros to the right of the decimal point. As you can see I have trained this network for 1700 epochs. Now, none of these applications has a valid excuse for not recognizing a trained pattern. So all numbers from zero to nine must be correctly recognized. Let me show you a few patterns. Here's number one, here's number four, and number nine. Not recognizing either of them would've been unacceptable. Okay, recall I mentioned something wrong about this model. Now let me show you what that is. Let me enter number seven. Yes, it's classified correctly but now let me change the input values from total zeros and ones to nearly zeros and nearly ones, and let's see what happens. Does this look more like a six than a seven? How about this? And this does not look like a five at all. With this behavior, we are witnessing a system that is not very good at generalizing. Now, let me show you something worse. Here we have the pattern for number zero recognized correctly but let me slowly change the middle segment to turn this zero into an eight and back. Pay attention to the output number as I move the slider. Watch closely. Did you see that? Now, ask yourself, is it necessary to go through 1, 2, 3 all the way to eight? Does changing the brightness of the middle segment make the pattern look more like a four or like a five? Of course not. Our neural network was forced to satisfy that constraint and in doing so it has sacrificed its ability to generalize. That does not look good for a neural network at all.
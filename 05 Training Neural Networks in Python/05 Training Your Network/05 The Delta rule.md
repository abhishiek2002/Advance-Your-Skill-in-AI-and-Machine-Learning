The Delta rule
- [Instructor] The simplest form of the algorithm we'll implement is known as the delta rule. It's a simple update formula for adjusting the weights in a single perceptron, that is a neuron. Yes, it's a simple formula but its reasoning is very clever. The delta rule considers the following values. The output error, this is the simple subtraction error. One input, the one affected by the weight we are going to tweak. And a constant known as the learning rate. So here's a nice equation to calculate the update in a weight i in a neuron k. Let's call it delta w sub ik. And it's the value we'll have to add to w sub ik to get the boundary closer to what we want. So to calculate this delta, we multiplied the learning rate eta times the output error. That is the label y sub k minus the neuron's output o sub k times the ith input value x sub ik. Yeah, it's very simple but notice what's happening there. The output error will be positive if the label y is higher than the output, and it will be negative if y is lower than the output. This means that when we later update w, it will contribute to making the output closer to the provided label. If we calculate all of the delta ws, and add them to the ws, our perceptron will be one step closer to having the boundary we want. So let me tell you a few things about the learning rate. First, it's a unique constant in the neural network. There's only one learning rate for all neurons. As the name suggests, it directly affects the rate of learning because higher values will result in larger leaps for the weights and lower values will result in smaller leaps for the weights. Does a higher learning rate mean faster learning? Yes. Does a higher learning rate mean better? No. The learning rate is usually initialized at 0.5 but you may have to tune it if learning is too fast or too slow. Here's why. Let me show you six updates of a weight in this error function, considering a learning rate that's too slow. So pay attention to the marble. One, two, three, four, five, six. The marble will eventually find a minimum. It will take a long time and it may get stuck at the first local minimum it finds. This could be much better. Let's see six steps again with a learning rate that's too high. One. Two. Three. Four. Five. Six. Large leaps may miss the minimum, getting stuck around it or even missing it all together. The desired situation is a learning rate that's just right. Notice that it may mimic inertia going a bit past the minimum but being drawn back into the valley. One. Two. Three. Four. Five. Six.
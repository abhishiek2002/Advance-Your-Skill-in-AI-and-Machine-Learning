The need for training
- [Instructor] This is one of the most important moments in this course. It's time to talk about the need for training. So let's look back and reflect on the following points. Throughout the coding exercises of this course, we haven't seen a useful neural network yet. True, we have seen networks that behave like gates and they may be useful. But there are much better hard-coded alternatives to perceptrons when it comes to implementing a NAND or an XOR. For example, we could simply use the logic embedded in programming languages to get away with logic. Well, it turns out that the real value of neural networks lie in their ability to learn. Sure, we just got a multilayer perceptron to behave as an XOR gate by writing the exact weights it needed. But what if we could show the neural network a lot of examples of how an XOR behaves so that it can learn from those examples? Wouldn't that be something. So I have good news. Up ahead, we'll see an algorithm to train multilayer perceptrons known as the back propagation algorithm. So sit tight and pay attention. But wait, there's another reason to train a neural network. Remember, linear separability? Well, I have bad news. Linear separability is hardly a given. Consider this example of classifying things as small or large based on their length and width. Let's say that small is represented by triangles and large is represented by dots. Moreover, notice that this is not linearly separable. There is no straight line that will divide the two categories but that doesn't mean that a single perceptron won't do a good job at classifying these samples. Take this line, for example. Notice that it will misclassify one dot and two triangles, which doesn't seem so bad. And even if we used a multilayer perceptron, we will get a nonlinear boundary like this one, which does a better job misclassifying only one triangle. It's better but it's not perfect. And that's the whole point of training. We are looking for a model that will get most of the samples correctly classified because we don't know so much about the problem and we are basing our judgment on the samples we have seen. This brings me to the problem of generalizing. This neural network may work very well for the provided data points but it still has to prove useful for new data it hasn't seen before. So let me tell you about three situations in the spectrum of misclassifying and generalizing. Here we have a different dataset for the same problem we just saw. I'm showing the same plot three times because I want you to compare these three situations for a classifier. So look at the leftmost plot and suppose we use a single perceptron with a straight line as boundary. As you can see, this one misclassifies two dots and five triangles. This situation is known as underfitting where the network misclassifies too often, so it's not very accurate. So we don't want this. This is bad. Now, look at the middle plot. Suppose we use a multilayer perceptron that ends up using an arc as a boundary. Notice that the misclassification has dropped to one dot and two triangles. Now, these numbers aren't as important as the visible shape of the trend between the categories. When we train a neural network, we are aiming for a boundary that works just right. That is it rarely misclassifies and it generalizes well. If we feed new unseen samples to this network, chances are it will get it right most of the time. Now look at the rightmost plot. Suppose we exhaustively train a very complex neural network so that it always gets it right with perfectly accurate boundary. Notice that it seems wrong to get every outlier correctly classified. It seems wrong because it is wrong. This situation is known as overfitting and you may have guessed that an overfitting neural network is bad at generalizing. If we feed new unseen data to this classifier, it will probably fail often for data points near the boundary. In the real world, outliers are inevitable and we don't need to sacrifice the accuracy of our classifier just to classify known data.
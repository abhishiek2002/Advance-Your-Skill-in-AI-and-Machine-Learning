Challenge: Write your own Backpropagation method
(upbeat music) - [Instructor] Ready to finish up your multilayer perceptron class? This time your task is to write a backpropagation trainer method, which will run one sample through the network with the backpropagation algorithm. Don't worry, you'll just have to write a few lines per step in the provided code. In fact, you'll just have to fill in the blanks. You can do this. And feel free to go back to the backpropagation videos if you get stuck. The method is called bp and it receives a feature vector x and a label vector y. Both are regular Python lists. So the first two lines convert those lists into NumPy arrays as usual to have the operations in NumPy. Now your challenge starts in line 87. Notice that I've placed a comment for each step and skeleton code for the loops. Now for steps three and four, you'll need a new NumPy array of arrays I added to the class to store the error terms or lowercase deltas. That's why I named it lowercase d. You may want to check the constructor code starting at line 36 and look for this array. So as you can see, I have highlighted all the appearance of d in the constructor. And if you pay attention, you'll see that it has the same organization as the value's array. So it's created and initialized right next to it. So this d array holds the error term for each neuron, just like the values array holds their output values. Now let's go back to the backpropagation method. As you can see in line 109, we must return the mean squared error because we'll need it the training process later. Speaking of training, at the bottom, I have included a training example starting at line 113. So notice that I'm running the bp method once for each case in the XOR truth table and that's my whole dataset. Just four samples. I'm running it for 3,000 epochs and I'm calculating the mean of the four errors in an epoch. Notice that I'm only printing the error every 100 epochs. So I'm expecting to see 30 error values in the terminal to keep it simple. In line 126, I am also printing out the weights for you to see what the neural network finally came up with. And finally, we print the truth table. So when you're ready to test your backpropagation method, you will see the error drop as the training advances. You will see the final weights and lastly, the four cases will be tested. So you'll know how your training method is doing. This may take you a while. Anything between 30 and 45 minutes. So have fun and when you're done, look at my solution in the next video.


Solution: Write your own Backpropagation method
- So here's my solution. Step one is the simplest. In line 93, we just run X through the network and assign the result to a new array called outputs. Step two is where we calculate the mean squared error. So first, I save the simple errors in an array called errors, and notice that I'm using NPI vector operations like the subtraction Y minus outputs. Then the mean squared error is the sum of the values in error squared divided by the number of neurons in the last layer. Step three is also done in vector operations. Just following the equation, notice that the result goes to the last element in our D array. Now for step four, pay attention to the loop starting at line 1 0 3. First, I calculate the weighted sum of the forward error terms and then use that sum for the current neurons error term. Notice that the outputs are not recalculated They are fetched from the values cash. All this is assigned to each element in the D array which contains the error terms. Steps five and six contain the most code but it's actually quite simple. I goes through the layers, J goes through the neurons and K goes through the inputs. That's why it goes from zero to the number of neurons in the previous layer, plus one because of the bias weight. And that's exactly what the body of the innermost loop is doing. Look at line one 14. If K is the last weight, we calculate the delta by multiplying the learning rate times the error term in that neuron times the bias term as the input because well, that's the input. On the other hand, if it's not the bias weight we are checking with K. Then we calculate the delta as the learning rate times the error term times the actual input, which comes from our values. Cash indexed at the previous layer. That's it. Finally, we return the mse. So let's see it working here. We have the results. Pay attention to the 30 error values as they go down. Next we have the weights. Notice that it came up with something other than the nan or and combination we designed earlier. Look at the values and signs of the weights. This is surely a logical equivalent of that initial ex o R. And finally, we have the truth table at the bottom. As you can see we are practically getting the ex o r behavior meaning that our artificial brain has learned it's alive. Now, this is my favorite part of the whole process. This plot shows the learning process for the Exor gate. We just saw I got this data by training an exor like we just saw and I copied the 30 error values reported in the terminal. I pasted them in a spreadsheet to finally make the plot. You should try it on your own and you'll get something very similar. As you can see, the plot shows how the error drops as the neural network learns epoch after reoc. These plots usually show a very subtle improvement in the error in the first iterations. But as the gradient dissent starts to pay off you'll see a dramatic drop after which the improvement is subtle. Again, that's just the law of diminishing returns working and that's a smart way of telling when you should stop training. You don't want to waste your time getting less significant improvements or even worse getting your neural network to over fit. So if we look at the plot you will see that the dramatic improvement happened sometime around 1000 eppo, and you'll also see that the diminishing returns started to show at about 1500 eppo. So with this in mind maybe we should have stopped training some time before 2000 to avoid overfitting.
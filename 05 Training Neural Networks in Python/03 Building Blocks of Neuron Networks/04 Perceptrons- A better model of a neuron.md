Perceptrons: A better model of a neuron
- [Instructor] Now we have our complete model known as a perceptron. As you can see, it has a set of inputs with a global bias term. This input vector will go through a weighted sum and this value will go into our sigmoid activation function. Once again, pay attention to the numbering, the inputs and their weights are numbered from zero to N minus one and the bias is treated as input number N. So how should we interpret the output values of our neuron? Well, the output comes from the sigmoid function. Notice that the output is greater than 0.5 for a positive input, that is for a positive weighted sum. This way, an output value of 0.5 seems like a reasonable threshold for firing. So before we dive into the code, let me point out some implementation notes. All values must be real numbers, not integers. The weights and inputs may be implemented as one dimensional vectors. In our case, we'll use NumPy arrays. This way the weighted sum may be calculated in one operation as the dot product between the two vectors. That's one line of code. Finally, we'll feed the weighted sum to our implementation of the sigmoid function. Before we get to the code, let me show you how to set up a code space and browse through the branches in Visual Studio Code. Here we have the GitHub repository for the course. To start a code space, you can click on the code button and go to the code spaces tab. Here you will see a list of your existing code spaces. You may start a code space by clicking on its name to continue where you left off. If this is your first code space in this repository, you can create it by pressing the create code space on main button. Let's do that. This will open a tab to set up your new code space. Setting up a code space may take a while but only the first time. Reopening an existing code space is much faster. After booting up, you will see the web version of Visual Studio Code with access to the repository. As you can see in the explorer pane at the left, there is no source folder. That's because we are in the main branch, which has no code. This is a multi-branch repository. So to access the exercise files for a specific video, you must switch to its corresponding branch. You can do that by clicking on the branch symbol at the bottom. Notice that it reads main. When you click on it, you will see a dropdown menu where you can select a branch. The branches in this list are named with the chapter number followed by the video number. This is the fourth video in chapter two so its branch name must be 0204. Now, some branches are provided in the state shown at the beginning of a video, as well as at the end of it. These have a letter B or an E at the end of the branch name. So for this exercise, I will pick branch 0204B, as in beginning. Look at the folders at the left. Now we have a source folder. Let me expand it and open the mlp.py source file. Now let me get the bottom and left panes out of the way. So here we have the first part of our perception class. First, we have the header for the constructor in line nine to create a new preceptor and object with a specified number of inputs and a bias term, which will be one by default. The only member data we need for this class are the weights and the bias term. So let me initialize the weights as a random array of floating point numbers between minus one and plus one. This random initial state will be important for the training phase. I decided to use NumPy arrays here. That's why I imported NumPy as NP at the top. So I'll do this with the random function in NumPy. Now, the size of the array must be the number of inputs plus one because of the bias input. Now, this random function returns values from zero to one. So let me add a scaling factor of two and a shift of minus one. That'll do it. And finally, let me save the bias term for later. Now, let's go to line 14 to write the run function which feeds an input array X into the perception to return the activation function's output. This is a one-liner, but I'll write it in two lines. First, let's calculate the weighted sum with NumPy's dot product between the input and the weights. Notice that I'm inserting the bias at the end of the X array because I'll use it as the last input. And finally, let's plug the sum into the activation function and return that.
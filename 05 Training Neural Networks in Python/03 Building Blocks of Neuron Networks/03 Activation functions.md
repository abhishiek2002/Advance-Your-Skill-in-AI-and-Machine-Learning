Activation functions
- [Instructor] We are almost there but our neuron is still missing something. So let me tell you what's wrong with weighted sums. There are two inconveniences I'd like to mention. First, values aren't constrained so a sum may sometimes result in a very large value or a very small value. Second, a weighted sum is a linear function. So the threshold to fire is not very well defined. That is, a change between true and false is not very notable. And most importantly, it's not easily trained. It turns out that other functions that make learning easier are non-linear. This is the real reason to add an element to our neuron. So what's wrong with having a very large and a very small value? Consider this example where we have a two input neuron and we are feeding 1000 to X0 and two to X1. For now, let's leave the bias weight at zero so the bias is not shown to keep the diagram simple. If we run the neuron, we'll have a result of 2006. So notice that although the weights are very similar, two and three, the big difference in the input values has made the neuron very sensitive to X0 and insensitive to X1. That's the job of the weights, not of the inputs. And what's wrong with linear functions? Well consider this neuron with one feature input and one bias input. At the right, we have a plot of the output as a function of X0. Now, this is not the usual 2D plot we have seen so far. The line is not the boundary. This plot is showing the output sum Z as a function of X0, so the boundary is the horizontal axis. Remember, this neuron will classify the input values as one of two categories based on the sign of the output. That means that whenever X0 is greater than -1.25, the output will be positive, putting X0 in the positive category bucket. On the other hand, whenever X0 is less than -1.25, it will go to the negative category bucket. A somewhat inconvenient detail of these type of function is that close calls don't have a very dramatic difference. For example, if X0 is -1.26 and increases by a small amount to -1.24 the category will change from negative to positive but the value of the function will be close to zero either way. A more dramatic change in these cases would give us a bit more confidence in the classification. Once again, the real reason why linear functions are inappropriate for a neuron is that this dramatic distinction between the categories near the boundary will make it very easy for this neuron to learn. So in conclusion, we need a non-linear element in our neuron that's achieved by adding a so-called activation function to our model, and we'll do that by simply plugging this function to the output of our model so far. This means that the sum will be sent as an argument to the function G. So let's do a quick recap on activation functions. These functions model the desired dramatic threshold behavior of the boundary. They usually constrain the output values and most importantly, they provide non-linearity to the neuron, thus enabling training by backpropagation, which is the method we will use. This requires activation functions to be differentiatable. So let me show you some popular activation functions. First, we have the binary step function, which limits the output values between zero and one. Actually outputs are exactly zero or one. Here's the expression for the function. So as you can see in the plot, the function returns one for positive values of X and zero for negative values of X. There's the dramatic distinction in the boundary. The function jumps between zero and one. Next we have the logistic or sigmoid function. This is the one we'll use in our neuron. This one also limits the output values between zero and one. However, output values are real numbers between zero and one. Here's the expression for the sigmoid function. As you can see in the plot, this function has a smooth change in the boundary, and it turns out that its derivative is a very simple function which we will need for the learning part. This is definitely a keeper. We also have the hyperbolic tangent function. This function limits the output between -1 and plus 1. Outputs are also real numbers in this range. The expression is a well known function composed of exponentials. Now looking at the plot, you may think of the sigmoid function we just saw, and that's because the hyperbolic tangent is the same sigmoid function with a scaling factor and a vertical shift. That's why these two functions behave in a similar way, and you may want to choose one or the other depending on how you want to represent your outputs. For example, if you want to represent false with zero and true with one, use the sigmoid. And if you want to use -1 for false and plus one for true, then use this hyperbolic tangent function. Lastly, we have the rectified linear unit function, often called ReLU. This function limits outputs to positive values, but it's unbounded for positive values. As you can see in the mathematical expression this function leaves the value of X unchanged for positive values and outputs zero for negative values of X. All of these traits are visible in the plot, and although the function is unbounded it's a very popular activation function for neural networks.
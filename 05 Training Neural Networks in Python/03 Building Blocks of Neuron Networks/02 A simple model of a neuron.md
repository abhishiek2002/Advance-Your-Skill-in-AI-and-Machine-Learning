A simple model of a neuron
- [Instructor] So based on the roles of the dendrites, nucleus, and axons, let's just assign those roles to a set of inputs a calculation unit, and an output respectively. To come up with the block diagram, you can see at the right, we'll call the inputs X0 through Xn minus 1 for a neuron with n inputs. Now, biological neurons seem to react in different sensitivities to different incoming signals, so we'll model that mathematically by assigning a weight to each input channel. Let's call them W0 through Wn minus 1. Next, the computation performed in order to fire or not is a simple weighted sum as shown inside the circle and that's exactly what our output will be. Here's an example of how a neuron would help us classify. Let's suppose we have a two input neuron and we feed data with two features into it. These features are shown in the plot at the right, which shows the risk of getting some medical condition X based on two factors, A and B, as the horizontal and vertical axis. Each point represents a person and its shape represents that person's risk of getting that medical condition. Triangles are at risk while squares are not at risk. Let's say these points come from a number of observations in a research study. Now, let's suppose we want to predict if a person is at risk of getting that medical condition X based on factors A and B alone. We could feed A through X0 and B through X1 into our neuron and it will give us some useful information. Notice that the weights I assign to the inputs are 1 and minus 1. This means that the boundary that divides positive and negative output results would be this line. Take a moment to look at this line and notice that whenever A is greater than B, that is for a point above the line, the result of the weighted sum will be positive. And whenever B is greater than A, meaning that the point is below the line, the weighted sum will be negative. So this neuron model is indeed capable of classifying if we interpret the sign of the weighted sum as the category. This way, a positive result means that the person represented by A and B is at risk, while a negative sum means that the person is not at risk. Now although this neuron is capable of producing a boundary line, it's very limited. That's why we need to talk about bias inputs. So as you noticed, a weighted sum is a linear function. It has a weight assigned to each input which will provide a slope to the boundary. However, an additional independent input is needed to move the line vertically. And this extra input is usually fed a constant value of 1, but it has its own weight. Now let me show you why we need a bias term. The neuron we've considered so far is only capable of producing boundary lines that pass through the origin or the coordinate zero,zero. So consider this plot where we have a set of students who passed or failed some class and we want to classify other students based only on their grades on two exams in that class, even though there was more graded work. The squares passed and the triangles failed. Our first attempt of a neuron could produce any line with a positive slope like this one. But notice that, try as we might, there's no way of dividing the two categories with a line like this one. How about inverting the slope by switching the signs of the weights? Well, we would get a little closer because the triangles and squares seem to be separated by a negative slope line. But again, stick to the origin and there's no way of placing the line between the categories. As you may have already guessed, a simple solution is to add a biased term. A term that's independent of the grades of exams one and two. Technically, this is the intercept in the equation of a straight line. Notice how this line does the trick for us. None of the previous attempts would've divided the categories like this one does. So this will be a refined model of a neuron. It produces a linear function that can place the boundary wherever it's needed. Also, notice that the examples we saw have only two inputs. This was done to keep the explanation simple and in two dimensions. But our neurons can have any number of inputs making the boundary go from a line to a plane or in general to a hyperplane. Pay attention to the notation I'm using here. This is an n input neuron with inputs numbered from 0 to n minus 1. It has an implicit bias input, which doesn't count as an input because the bias is usually a part of the neural network. It doesn't come from the outside. So technically the neuron will have n plus 1 inputs, but we'll say that it has n inputs based on the outside world standpoint. All of these features of a neuron are going to help us classify, but there's still one missing piece if we want to give this neuron the ability to learn.
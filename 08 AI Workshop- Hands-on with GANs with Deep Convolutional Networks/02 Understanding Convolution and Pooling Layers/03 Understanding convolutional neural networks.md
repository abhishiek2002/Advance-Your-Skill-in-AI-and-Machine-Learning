Understanding convolutional neural networks
Let's get a big picture understanding of how convolutional neural networks or CNNs work. CNNs are a class of deep learning models primarily used for computer vision tasks. They are designed to automatically and adaptively learn hierarchical features from input images, making them well-suited to tasks such as image classification and object detection. The CNN architecture, like I mentioned before, is meant to mimic the visual cortex of the brain, essentially how humans perceive images. CNN are able to extract data from images in two dimensions, so they also get the spatial information present in an image. Dense neural networks with fully connected layers can also be made to work with images. Essentially, you flatten the image data that you will feed into a dense neural network. However, flattening the image data does not let the network capture the spatial features of images. The location of an object at the top right or the bottom left of an image, that information is lost. Now, training dense neural networks on images leads to parameter explosion. There are millions of parameters for even very simple images. And if you have a fully connected neural network, well, training that is just very, very tough. Convolutional neural networks, on the other hand, are sparse neural networks. They have far fewer parameters to train, and they work very well with image data. CNNs are able to capture location invariant spatial features that are present in the input. So let's say you're trying to detect an object. That object could be in the center of the image or to the top left or the top right. It doesn't matter. CNNs will be able to detect this. Convolutional Neural Networks are made up of two types of layers. The convolutional layer applies learnable filters or kernels to input data, and these filters detect various features like edges, textures, or patterns within the input data. Pooling layers are often referred to as subsampling layers that reduce the spatial dimensions of the input data. Pooling layers help reduce the computational complexity of the network and focus the network on the most important features of the input. Lets understand how each of these layers work, starting with the convolutional layer. The convolutional layer involves a learnable filter, also called a kernel, that is slid in a horizontal and vertical fashion over the input image. This kernel is trainable in that the weights of the kernel are found during the training process of the neural network, and this kernel is responsible for extracting features or information from the input data. The sliding kernel learns hierarchical representations that exist in the input image, and the output of the convolutional layer is feature map representations. A single input image can generate several feature maps, where each feature map contains different information that has been extracted from the input image. Here is a visual overview of how convolutional neural networks work. I'm going to break down this image step-by-step so you can see what's going on. Let's say this is the input that you feed into a convolutional layer. The input is typically an image, but it can also be a feature map generated by a previous convolutional layer. A k x k learnable filter or kernel is slid over the input image, and this kernel performs elementwise multiplications and sums the results over and over again over small regions, and this is what helps the layer learn hierarchical representations. The kernel may slide over the original image, or you may choose to add some zero padding around the edges of the image before the kernel actually slides over the image. So the input image may be zero padded with p zeros. Now the kernel moves over the image with a stride represented by s. The stride refers to the number of pixels the kernel moves at each step. The kernel will have a horizontal stride for horizontal movement and a vertical stride for vertical movement. The elementwise multiplication that the kernel performs on the input produces an output feature map, and this feature map has extracted some kind of representation from the input image. Maybe it's detected horizontal edges, vertical edges, maybe it's blurred the input or sharpen the input, anything. The size of the output feature map, that is, the width and height of the output feature map is computed using this formula. Now the output over here can be the width or the height. p refers to the padding that you've added to the input image, the zero-padded layers. k refers to the size of the kernel, and s refers to the stride either in the horizontal or vertical direction. If you're using the stride in the horizontal direction, the output will be the width of the image, and if the stride s refers to the vertical stride, the output here will be the height of the image. Convolutional neural networks have a second kind of layer, the pooling layer, which is responsible for subsampling or down-sampling of the input. The objective is to reduce the sample size. The pooling layer typically computes a summary statistic on the input image. It performs some kind of aggregating operations. Now the aggregations can include average pooling, max pooling. There are different pooling layers you can use for different use cases. The objective of the pooling layer is to preserve the important representations in the input data, while reducing the number of parameters the network has to deal with. It reduces the computational complexity of the network. Here is what the general architecture of a CNN looks like. Convolution and pooling layers are interleaved with one another, they alternate. After every convolution layer, you generally have a pooling layer. And at the very end, after the convolutional and pooling layers are done, there's generally a dense, fully connected layer which produces the final output, whether it's an image classification or some kind of object detection.
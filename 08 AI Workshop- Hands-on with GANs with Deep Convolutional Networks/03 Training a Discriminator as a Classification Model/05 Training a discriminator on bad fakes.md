Training a discriminator on bad fakes
We have the discriminator set up, now it's time to train it. The loss function that we'll use is the BCELoss or the Binary Cross Entropy loss. The BCELoss is a loss function that measures the difference between two probability distributions. In the context of binary classification, the two probability distributions are the predicted probabilities of the positive class and the negative class, as compared with the actual labels in the data. Next, we use the Adam optimizer to train the discriminator. The Adam optimizer has an adaptive learning rate that has been shown to work well for a GAN. We're only training the discriminator, but we are using the same optimizer and the same parameters like we would for the discriminator when it's part of a GAN. Next, let's set up a training loop to train our classification model. Now we'll train just for two epochs. You can see the for loop for epochs on line 1. We only train for two epochs because that's more than sufficient for the discriminator to learn from the data. That's because the discriminator's task is super easy in this example. Our fake images are pretty terrible, and it's going to be very easy for the discriminator to tell them apart from the real images. At the beginning of GAN training, this is what is true. The generator generates pretty bad-looking fake images, and the discriminator finds it easy to tell them apart. On line 5, we have the nested for loop to iterate over the batches in our training data. We access the input data and the corresponding labels and move them all to the GPU where we are training. This is on lines 8 and 9. We call optimizer.zero_grad() to zero out the parameter gradients on the discriminator, and then we make a forward pass through the discriminator on line 15. This will give us the predictions from the discriminator that we then use to compute the loss on line 17. We compare the predictions from the discriminator with the actual labels on the data. loss.backward() will make a backward pass through the discriminator model to compute gradients and optimizer.step() will use those gradients to update the model's parameters, and this is, of course, done for every batch and every epoch. We compute the running loss, and for every 20 batches, we print out the epoch, the current step, and the current loss of the classification model. Let's start the training process. The training will go through fairly quickly because we are only training for two epochs. We have very little data, and we are also using a GPU. Once training is complete, let's take a look at some of the predictions made by the discriminator classification model. We get the first batch of data from our testloader. I'm going to display the ground truth labels first, the actual labels for all of the images in this first batch of test data. Let's take a look at the output that we'll display in the form of a grid. Now here you can see the original labels for the real and fake data. Next, let's get predictions from the discriminator. Make a forward pass through the discriminator using this first batch of images. The output here will be probability that a particular image is real. Let's use torch.round to get the predictions in the form of 0/1 values. Once again, we'll display a grid of images from this batch and output the predictions of the discriminator. Let's take a look. And here you can see that the discriminator gets everything correct. The first image on the top left is a bad fake. It has been labeled as such. The second image is real. It has been labeled as such. The third image and fourth image both bad fakes. And again, all of the labels are correct. Let's compute the accuracy of the discriminator on the test data. We'll keep track of the number of correct predictions and the total number of predictions in the variables initialized on lines 1 and 2. Turn off gradients for the discriminator network with torch.no_grad(). Iterate over batches of data in the test loader. Get the images and labels, move them to the GPU by calling to(device). This is the code on line 7 and 8. Get the probability scores from the discriminator on line 11, convert that to predicted labels on line 13, and then basically, keep track of the total predictions and the correct predictions. The correct predictions are where the predictions from the model match the ground truth labels. We compute this on line 16. And finally, print out the accuracy of the network. And here you can see that it's 100 percent. This shouldn't have been surprising to you. We made the discriminator's task way too simple by using fake images that were pretty terrible. Next, I set up some code to compute the accuracy of the discriminator on real labels and fake labels. Initialize two dictionaries called correct_pred and total_pred and initialize the value 0 for each of the classes in correct_pred and total_pred. So these two dictionaries will have values of zero for the keys, bad fake images, and real images. You already are familiar with the code that you see on line 6 through 15. That's when we get the predictions from the discriminator. On lines 18 through 21, we compute the correct predictions for each output class or category, and on lines 24 through 26, we print out the accuracy score that we've computed for each output class or category. And once again, this is 100 percent. Our discriminator has superlative performance. Well, that's what happens when you give it too easy of a job to do. And this is how the discriminator works in the early stages of GAN training, when the generator basically is not capable of generating very good fakes.
Loading and transforming training image data
In this movie, we'll load in and take a look at the data that we are going to be using to train the discriminator. We've already mounted Google Drive that contains our training data to this notebook. The next step is to indicate what device we are going to be using for training. Now the device will be a GPU device if one is available, otherwise, it will be CPU. Now, because we have set the runtime type to have GPU, notice that a CUDA device is available for us to use. Next, we'll load in the Anime Faces dataset and create a dataset by pointing to the image folder that contains our data. The batch size that we'll use for training will be 16. The size of each image is 64 x 64. And remember, these are color images or multi-channel images. The route points to the path on drive where the data is located under /content/drive, we have My Drive, that is the home folder of Google Drive. And then within that, we have the ai_workshop_dcgans subfolder. And within that, the anime_classification subfolder. We then apply a number of transformations to the input image. Each image will have a height of 64 pixels using the resize operation. And then we perform a center crop so that every image is 64 x 64. We convert the images to a tensor format and normalize using 0.5 as the mean and 0.5 as the standard deviation. Here I kind of approximate the mean and standard deviation values for the input data. The ideal thing would be to actually take a look at your data and compute the mean and standard deviation you should use to normalize those images, but 0.5 should work well for this demo. This should ensure that all of the RGB channel values are centered around zero, and that will actually improve the training of the model. Once this is done, let's take a look at the classes in this data set. If you look at the classes, you can see there are two; bad_fake_images and real_images. The classes correspond to the folders that we had in our Google Drive. This is what we're going to train the discriminator to identify. Now the total number of images that we have is 2048; 1024 bad fakes and 1024 real images. I'm going to randomly split the anime_faces dataset into a training set and a test set. 80 percent of the records we'll use to train the discriminator and 20 percent to evaluate the discriminator. The random split will ensure that the training data is shuffled. Next, let's set up the data loaders for the training and test data. Data loaders in PyTorch allow us to iterate over the training and test data in batches. Note that I've set shuffle = true in the train data loader, so the training data will be shuffled when we train the model. The testloader has shuffle = false. We'll use that only to evaluate how the discriminator does once it has been trained. Next, let's take a look at some of the images that we'll use to train our model. I set up an iterator over the train data loader and extract the first batch of images. Observe here the shape of the PyTorch tensor of a batch of data. This will be the shape of each batch that we use to train our discriminator model. The first dimension here, 16 refers to the batch size, 3 is the number of channels in each image, and every image is 64 x 64. Next, I'll set up a utility function to display the images that we are working with. This function is called display. It takes in an image tensor and then displays it using matplotlib. The mathematical operation on line 2 unnormalizes the data. If you remember, we had normalized our data by subtracting a mean of 0.5 and dividing by a standard deviation of 0.5. This is just the reverse operation. The permute function swaps the dimensions of the image, so the height of the image comes first, then the width, and then the number of channels. PyTorch accepts a number of channels first, then height and width. The rest of the code in the function is to just convert the image to a NumPy array and display it using matplotlib. and then on line 13, I invoke the display function by invoking make_grid on a batch of images. make_grid is just a PyTorch utility that will take in the images in a batch, and lay them out in a grid format. The print functions on lines 11 and 15 will essentially display the labels associated with the images; real images are bad fakes. So let's take a look at this first batch of images in our training data. You can observe the bad fakes interspersed with the real images. It's very easy to tell those apart. The discriminator is going to have a very easy job of it. That's pretty clear.
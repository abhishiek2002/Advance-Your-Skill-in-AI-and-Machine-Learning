Setting up the GAN training loop
We are now ready to train our deep convolutional generative adversarial network. While training a GAN, the generator network and the discriminator network have to be trained together and we'll train them alternately in the same iteration. Let's set up some parameters for training. The loss function that we'll use is the binary cross entropy loss function, which will essentially mimic the minimax loss used to train a generative adversarial network. We'll set up the loss functions in such a way that the discriminator will maximize the probability of classifying real images as real and fake images as fake, whereas the generator will maximize the probability of having the discriminator classify fake images as real. I've initialized a batch of 64 fixed noise latent variables. Now, every so often during the training of the generator, we'll look at some sample images to see how the generator is performing at that point in the training. We'll use this fixed noise that I've set up here to generate images from the generator. So it's the same noise variables, and we'll see how the generator performance changes over time. Next, we set up the categories or classes that the discriminator uses to identify reals and fakes. Real images will be labeled as 1, fake images will be labeled as 0. We also set up Adam optimizers, one for generator training and another for discriminator training. I'd mentioned just a bit ago that at periodic intervals during the training process, we'll have the generator generate images from fixed noise. We'll display those images in the form of an image grid using this utility function here. This function contains all the basic little transformations that you need to do to display the image grid in matplotlib. Next, let's set up the training loop. The list that I've initialized, G_losses and D_losses, will track the generator and discriminator losses over the iterations of training. real_score_list and fake_score_list will keep track of the scores that were achieved by the discriminator on the real data and the fake data, respectively. On lines 12 and 13, I make sure that both the generator as well as the discriminator are in training mode, so that the gradients will be updated. I have two for loops running. The first for loop on line 15 is to iterate over the number of epochs of training. The second for loop on line 17 is to iterate over the batches in each epoch. Every iteration will train the discriminator as well as the generator, starting with the discriminator. On line 22, we zero out the gradients of the discriminator. We get the first batch of real images on line 24 and generate the labels for those images. Real labels, remember, equal to 1. We'll make a forward pass with the real images through the discriminator to get the predictions from the discriminator. This is on line 28. And then we compute the loss of the discriminator on the real images. The code on line 32 is our loss function. The discriminator will try to minimize this loss. Discriminator wants to classify all real images as real, and we'll compare the predictions of the discriminator with the real labels. We then compute gradients in the backward pass through the discriminator. This is on line 35. And then we compute the score of the discriminator on the real images, D_x. The discriminator wants the score to be as close to 1 as possible. Next, in the same iteration, we generate a batch of fake images using the current parameters of the generator. We initialize the noise to be used for the generator on line 40. We pass this noise through the generator on line 42 and get the fake images generated using the current generator parameters. We set up fake labels that will be all 0s for this batch, and on line 46, we pass the fake images through the discriminator to get the discriminator's predictions. And then we compute the loss of the discriminator or the error of the discriminator on these fake images. The objective of the discriminator is to classify fake images as fake, and the error that it will try and minimize is that its predictions are equal to the fake labels. On line 53, using the loss on the fake images, we make another backward pass through the discriminator to compute gradients. The gradients computed will be accumulated or summed up with the previous gradients that we had computed on the batch of real images. And on line 55, we compute the score of the discriminator on the fake images. The discriminator wants the score to be as close to zero as possible. Our discriminator training is almost complete. We compute the total error, which is just the sum of the error on the real and fake images. And then we call optimizerD.step() to update the parameters of the discriminator. Next, in the same iteration, we train the generator. We zero out the gradients for the generator, and then we pass the fake images that the generator had previously generated through the newly updated discriminator. Remember, the discriminator's parameters have just been updated in this iteration, and we use the new parameters to see what predictions the discriminator makes on the fake images that were previously generated. Next, we compute the loss of the generator. The objective of the generator is to get the discriminator to classify fake images as real. On line 73, we compare the predictions of the discriminator with the real labels, and this is the error that the generator tries to minimize. We compute gradients for the generator by making a backward pass through the generator, and we compute the score of the discriminator on the generated images. This is on line 78. The discriminator wants the score to be close to zero, the generator wants the score to be close to one. And then we update the generator parameters using optimizerG.step(). At certain intervals during training, we'll print out the epoch, the current step, the losses from the generator and the discriminator, and the discriminator scores on real and fake data. In addition, we'll use the fixed noise and pass that through the generator, this is the code on line 97, to get a batch of fake images at this point in the generator training. We'll then display those fake images in a grid format. And for each epoch of training, we keep track of the generator and discriminator losses, code on lines 100 and 101, and the discriminator's scores on real and fake data. We'll visualize this as an interesting plot after the GAN training is complete.
Setting up data for GAN training
Now that we've seen how the discriminator performs on good fakes and bad fakes, it's time for us to train our Generative Adversarial Network. Let me show you the data that we are working with. Here I am in my ai_workshop_dcgans folder on Google Drive. This is right under the home directory, My Drive, and I'm within the anime_classification subfolder. And here I have a zip file, anime_images.zip. This is a file with a much larger corpus of anime images that we'll use to train our GAN. Now let's head over to our Colab notebook, TrainingDCGANS OnAnimeFacesData. This training has to be performed on a GPU, otherwise, it'll take a very long time on a CPU. Let's make sure that our runtime type is set to T4 GPU. Make sure you have the selection on in your notebook before you run any code. With that done, let's set up the imports for all of the PyTorch libraries that we'll be using. You're familiar with most of the libraries here. One thing that might be new is on line 11, the tqdm library. This is just a library that shows a progress bar. Very useful if you want to visualize the progress of training. We'll, of course, have to mount Google Drive so that it's accessible from our notebook. You're familiar with this code. You'll have to authenticate yourself with your Google account in order to be able to mount Drive. Once Drive is mounted, we'll need to extract the zip files to access the images training data, and you can do this using the unzip command. The path that you see here is the absolute path to the anime images.zip file that we had uploaded to Google Drive. The -d '/content/images' will extract all of the contents of the zip to this folder on my cloud-hosted notebook. Running this command will extract the files. It will inflate the zip file so that it's available right here within your notebook. And you can actually view these images if you click on the little folder icon off to the left. This will show you the folder is accessible from here in your notebook. Notice that we have a separate images folder here. Under that, there is another images subfolder. If you expand that, that's where you'll find the images of our training data. Let's get started with the setup that we need for training. First, we need to initialize the device that we'll be using. Because we have access to the GPU, the device will be a CUDA device. Next, we instantiate the dataloader for the training data. For each batch of training, we'll use 64 images. That's the batch size. Image size will be 64 x 64. On lines 5 through 11, we create the anime_faces_dataset. We load in the data from our image folder, /content/images, and apply transformations to resize the images, convert the images to a tensor representation and normalize the images. These transforms are the same ones that we used earlier when we were training the discriminator as a classification model. Once we have the data set, we set up the dataloader. This is a code on lines 14 through 20. Shuffle = True for this training dataloader, and drop_last = True will drop the last batch of images if the batch size is not a full 64, for the last batch. If you compute the length of the training dataloader, that will tell us how many batches we'll have for each epoch of training. And you can see it's 993. Before we start the training process, let's quickly look at one batch of images. We'll use the make_grid utility once again to display one batch in the form of a grid. And you can see that we have to perform that little transpose of dimensions so that the images are in the form that can be displayed by matplotlib. And here is our anime faces training data. We are familiar with this data. We'll now use it to train a GAN, which is much more exciting than just training a discriminator.
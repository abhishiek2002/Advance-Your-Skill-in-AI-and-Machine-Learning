Setting up the generator and discriminator
We'll now define the neural networks that make up the generator and the discriminator. But before that, here is the weights_init function that we've seen earlier. It takes in a model as an input argument, and it initializes the weights of the model parameters using the criteria specified in the original DCGANs paper. The author suggests that model weights should be randomly initialized from a normal distribution with mean equal to zero and standard deviation equal to 0.02. And that's what we do with every convolutional layer. That's the initialization on line 5. On line 7 and 8, we initialize the weights for the batch normalization layer. The model weights are initialized using a normal distribution with a mean of one and standard deviation of 0.02, and the biases are initialized with the zero constant. Next, we set up a bunch of variables for the various parameters that we'll use to construct our generator and discriminator networks. nc we've seen earlier, number of channels in the training data. nc = 3 to represent color images. nz is the size of the latent noise vector that is fed into the generator so that it can produce an image using that noise. nz = 100. The size of the feature maps in the generator, ngf is 64, and the size of the feature maps in the discriminator is ndf = 64. We'll train the Generative Adversarial Network for 50 epochs of training, using a learning rate of 0.002. Now I've specified the number of GPUs that we have. It's just one. Let's set up the device that we'll use for training. If CUDA is available, that is, if GPU is available, the device will be set to cuda, otherwise, the device will be a CPU. Since we have a GPU, the device is cuda. The GAN structure that we'll use for both the generator architecture and the discriminator architecture is from this PyTorch tutorial here at this link. And this tutorial is basically inspired from the GAN network used in the original deep convolutional GAN paper. We are now ready to define the generator neural network. Remember, the generator takes in latent noise as an input and produces an image at the output, and the generator uses the transposed convolutional layers that we discussed earlier. The generator class inherits from the nn.Module base class, and within the init method is where we define its layers. The layers are just a sequential stack of layers. Each stack of layers include a transposed convolutional layer, followed by batch normalization, followed by ReLU activation. The in_channels to the first transposed convolutional layers is nz, that is, the size of the latent noise variable. We had initialized this to 100. The out_channels represent the depth of the feature maps generated by this transposed convolutional layer. The input noise variable is projected and reshaped into a three-dimensional convolutional spatial representation. The out_channels produced by this first layer is very, very deep, ngf * 8. We've specified a kernel size of four, stride of one, padding of zero, and a bias = false. The transposed convolutional layer is used to upsample the input that is fed to it. The batch normalization layer will center the output of the transposed convolution layer to be at zero with a variance of one. And as we've discussed earlier, this mitigates a lot of problems that we encounter with GAN training. And finally, we have the ReLU activation. Now, the comments before each stack of layers tells you the shape of the input going into that layer. So the first layer, we just have the noise variable, Z. This is the comment on line 6. The output of the first layer is fed into the second layer, and that has dimensions ngf x 8 and 4 x 4 feature maps. The in_channels of the second layer correspond to the out_channels of the previous layer. in_channels here is equal to ngf * 8. Observe that the out_channels is ngf * 4, that is, the out_channels is shallower. The output of this layer pass through batch normalization, and ReLU activation is fed into the next transposed convolutional layer. The in_channels on line 31, ngf * 4, matches the out_channels of the previous layer defined on line 21. You can see the other layers that have been defined here. Observe that the inputs to each layer gets larger and larger and shallower and shallower till finally we have the last transposed convolutional layer. This layer does not use batch normalization and has the Tanh activation function. The Tanh activation function produces an output in the range -1 to 1, and these lie in the pixel intensity ranges that we want for the generated image. The output of the generator produces an image and the state size here is a number of channels by 64 by 64. That is essentially the dimensions of a single image generated by the generator. And it is this image that will be fed into the discriminator. Let's now instantiate our generator network and move all of its model parameters to our GPU device. Also, apply the weights_init function to initialize the model weights. Once the generator has been initialized, we'll set up the discriminator. We've already seen the architecture of the discriminator model in a lot of detail, so I won't belabor the point here. Notice the strided convolutional layers with LeakyReLU activation and batch normalization. The discriminator is the same discriminator that we've used before, and the output of the discriminator is a probability score indicating whether this is a real image or a fake one. I'll now instantiate the discriminator and initialize the weights of the discriminator as well.
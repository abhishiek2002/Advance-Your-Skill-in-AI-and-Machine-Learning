Semantic coding for cost
- [Instructor] Okay, we're back in the code, and we're worried about cost, and cost correlates to a lot of things in the AI world, especially in the model usage because it costs tokens to use them. You may be using an open source modeling locally, so you're like, ah, no problem. But it is these incredibly powerful foundation models that you really like using, and they cost per token. So you're sitting there, maybe you're by yourself. You don't have like an architect to work with you. That's most people, right? And wouldn't you like to have help? So help me deal with the cost. I was sort of hoping someone out there will help you. Engineering can be lonely, and this is a fundamental issue with LLMs cost. Okay, so there's a bunch of tricks. We call them tricks still, but they are harder knowledge. Caching is a proven means to reducing costs. Therefore, we want to cache the completions, ie GenAI, I ranked 'em myself. You like, in this world you can like just talk to yourself, and someone's actually listening, actually not someone, again, don't want to humanize AI ever. This is just like human talking to human. You're out there listening hopefully, and you are a human. So we're going to cache completions, and the Python code that's being used is able to generate embeddings from the text you give it. And of course, you can generate completions as well. Goal is to cache the completions and generate embeddings for the text to cache the outputs with the key as the input. We'll use vector similarity to compare a new piece of text to the available cached completions. Wow. If you know what this is, you're like, ah, that makes sense. I'll tell you like six months ago, I had no idea what I'm typing just now. (chuckles) So actually, in the spirit of that, let's say, let's actually do something different here. Let's use the Semantic Kernel extension to summarize the previous text in plain language. Okay, we're going to ask GPT-3 to translate my gobbledygook into different like, okay, result, caching, save, store results, cache completions, use vector similarity. Yeah, that's much simpler. So that's what I'm trying to say. We're going to increase the number of tokens available. We can also do it here from the text file config.json. And okay, that's our strategy. There is a function to convert text to embeddings called text to embeddings. And there's a function to complete a text called text completion. They are both in Python. Decode to construct rudimentary cache, will read as, and you know, if you have GitHub co-pilot installed, this happens automagically while you're typing. But just imagine you're a developer somewhere out there trying to figure stuff out. Yes, you can have it generate code in from GitHub. I would go with the big GitHub co-pilot extension within VS code. But you can also have these conversations to kind of work with an architect to really kind of like suss out things. And I want to note that this is using GPT-3. And if you try GPT-4, you'll quickly discover that the code it generates is probably like three to four times better. But that's the kind of quick way to sort of play with ideas, like, oh, it costs a lot to use the model. Let's use caching. Oh, it costs a lot to use that model. Let's use a cheaper model. So you can have a conversation with the systems. You can have a conversation with the model to be able to basically think out loud while it thinks out loud. And it's a wonderful way to not just reduce cost but play with more sophisticated engineering ideas. This is an example of how by working with AI, our slow thinking muscle, which we never get to use in daily life, we get to accelerate it.
Semantic coding for responsible AI
- [Instructor] So let's get coding. If you remember, we use the semantic kernel extension. This is always available on the left-hand side and let's make a function called make_me_responsible. If that, if they were that easy, right? This is a function that improves my critical thinking in building AI technologies. So make_me_responsible's right here. Okay this is that moment where you're like uh I got to write something. How do I do it? No problem. I have gathered information online, open source, from Microsoft's harms taxonomy that lays out the different risks of AI and what we want to be concerned about. Things like infringement on human rights, things on privacy laws, things that we should be worried about. Let's just take a piece of one of them. Let's see here. Let's take the economic loss issues. Okay, copy that. Go back into my make_me_responsible function and I'm going to add that context here that this is a denial of consequential service, harm description consideration example. I want to say this using the above framing for a feature described as the concerns that we should have regarding financial instruments are okay. And then we're going to run that function. So now let's say we are building a way to take credit card information quickly. So now what it's doing is it is computing based upon this context output. So it's saying given that consideration, the concerns the ones we should have and it's based upon the input. So let me read it here. Wow, we're going to deny people based on socioeconomic status not good. We're going to charge different rates. Doesn't sound good too either. They're going to have to work for low wages. So this is like, this is like gloomy dark things that if you're waking up in the morning saying I'm going to make a cool way to do this, these are the consequences, the unintended consequences. Now let's actually improve this reflection or essentially the critical thinking behind this. Now it isn't getting too creative so let me give it a little bit of latitude. Kind of like unbutton its top shirt collar button. And I'm going to say are including examples in the context of the feature. Let me give it a little more. Okay, let's now run it. Okay, a way to take credit card information quickly. You know fast is always great, right? But you got to ask questions when it's happening too quickly. Look at it here. Okay, okay. Now it's giving me the for example. Someone once told me always say for example, what before when you're opening your mouth and it'll make sense. For example, this is what happens when you write a prompt like this. It produces something like this. Clear discrimination. For example, charging higher rates for people with lower socioeconomic codes. Differential pricing. For example, charging more. This is sort of a more in plain English which is great. Paying destitute people for biometric data. These examples kind of really elicit the feeling inside you. And as a critical thinker, you don't solve the problem immediately. You understand the problem. So this is one example of using expert information about unintended consequences. Responsible AI thinking in the context of a feature input and this can get as complex as you want. And when you think about it, this ability to enable the AI that could enable you to do irresponsible things could also be used as a calculator, a responsible AI calculator. I think it's a question of feature, how we build features, we'll want to be able to write new code but also write this kind of reflective code. As in the software industry, we have software testing. We may as part of deploying this kind of AI-based software. Instead of unit tests, we may write responsible AI tests. And not just for the output of AI but for the human pilots who are developing these co-pilots for others.
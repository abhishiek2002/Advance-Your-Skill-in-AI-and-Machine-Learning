Training the adversaries
We have a big picture understanding of the GAN architecture, but how do you actually train a GAN? How do you get the generator and the discriminator to compete as adversaries? Now, when you train a GAN, you have to train the generator and discriminator together, alternating training for one and then the other. You need to juggle the training of the generator and discriminator. It's not possible to train the generator separately and then the discriminator because they have to work together. They have to compete with one another. The way this typically works is that you'll train the discriminator for one or more epochs of training and then you'll train the generator for one or more epochs and then you continue this till training is complete. So you rinse and repeat the previous two steps, training the discriminator and then the generator, and so on until you feel that training is done for your GAN. Let's go back to the architectural overview of the GAN and understand how training will actually work. Now, at the beginning of training, you can imagine that the generator does very, very poorly. The generator weights are not trained. They would have been initialized at random. So any noise that you feed in should generate an image that is just pure noise. The generator's output is obviously fake, which means the task of the discriminator is very, very simple. The discriminator at this point will perform extremely well. It will find it very easy to classify what images come from the real database, and what images are the fake images produced by the generator model. So the discriminator starts off as performing really well, but gradually over time, as the generator receives feedback from the discriminator, the generator slowly starts improving the quality of the data that it produces. Remember that the discriminator heavily penalizes the generator for generating implausible fakes, so the generator gradually improves over time. In the zero-sum game, as the generator improves its output, the discriminator starts getting worse. The discriminator finds it more and more difficult to identify fake data from real. The better the generator gets, the worse the discriminator performs, and the accuracy of the discriminator in classifying the input that's fed to it steadily gets worse until the discriminator is making predictions completely at random. As model training progresses, the discriminator will be no better than a coin flip, and this, in essence, is the training process of a generative adversarial network. Now, some other details to keep in mind. When you train the discriminator, the generator weights are not updated. The generator is kept constant while training the discriminator, and this allows the discriminator an opportunity to recognize the flaws of the generator. We do the reverse while training the generator. The discriminator weights are kept constant, and this is important because if the discriminator weights are updated when a generator is being trained, the generator cannot improve with a moving target. Now, one thing I want to point out to your attention here. It's very hard to tell when GAN training is complete or when the model converges. That's because as you keep training and the generator constantly improves, the discriminator steadily gets worse and the feedback from the discriminator to the generator also steadily gets worse. At some point, the bad feedback from the discriminator will cause the generator to also get worse. If it's not getting good feedback, it will stop improving and start becoming worse, and that's not really what we want. So you will stop the training of the GAN when you feel that the generator produces images that are realistic enough for you.
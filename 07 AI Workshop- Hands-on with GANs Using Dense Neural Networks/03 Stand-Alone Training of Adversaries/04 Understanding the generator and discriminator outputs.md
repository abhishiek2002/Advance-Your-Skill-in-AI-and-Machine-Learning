Understanding the generator and discriminator outputs
At this point, we have a generator and a discriminator, but those two neural networks are untrained. Their weights and biases have been initialized using a random values. Now, let's say you try to get the generator to generate an image. Well, you're going to get nothing like what you'd expect. You'll just get random noise at the output, as well. Let's see where we start off with in the initial state of the generator. Now, this first variable, z contains a batch of 64 latent noise variables. The first dimension 64 here is the batch size. And latent size is the size of the noise input, which we had initialized to 100. So the first line of code will generate 64 latent noise variables, which we then pass through the generator on Line 3. So netG of z will make a forward pass through the generator neural network model and we get a sample generator output. Let's take a look at this generator output. This output is just a tensor where the first dimension is 64. So it's a tensor with 64 vectors. And every vector corresponds to a generated image. It's, of course, a flattened form of the image. Observe that the pixel values of each image are in the range -1 to 1. That's exactly what we'd expect with the Tanh activation at the output of the generator. Let's take a look at the shape of this sample generator output. And you can see it's 64 by 784. We fed in 64 latent noise variables and 64 images were generated at the output. Let's take a look at the minimum and maximum values of the pixels in the generated output, and you can see that they are in the range -1 to 1. So far, everything about the generated image makes sense. Let's actually take a look at a generated image. And you'll see that with an untrained generator, we just get an image that is just noise. What I've done on Line 1 here is reshaped the sample generator output to be 64, 28 by 28 images. The reshape function converts the generated output tensor to a format that PyTorch expects; batch, channels, height, and width. And then I use plot.imshow to display the first image, the first 28 by 28 image, and you can see the output is pure noise. Well, you shouldn't have expected any better. Remember, we haven't trained the generator. The weights and biases are initialized using random values. If you use random model parameters to make predictions, well, you get random predictions at the output. I'm going to use the make_grid function that we've encountered earlier, remember this is a part of the PyTorch utilities, and display a grid of all generated images using the display utility function that I set up earlier. And you can see that all 64 images contain pure noise. It's not just the generator that generates random output, the discriminator also cannot make any predictions. The discriminator has also not been trained at this point in time and has been initialized with random weights. netD.eval will basically move the discriminator into evaluation mode so that gradients won't be computed, and then we turn off gradients and get a prediction from the discriminator for all generated images from the generator. We should get 64 predictions for the 64 images that we are passing into the discriminator. The reshape(-1, 784) will flatten the image tensor so that each image is just a one-dimensional vector, -1 just gets the reshape function to infer the size of the batch dimension. We know that there are 64 images in a batch. Let's take a look at the first 20 predictions. Every predicted value here is a probability score indicating whether the generated image was real, and you can see that all these probability scores are almost identical and they are close to 0.5. The discriminator basically can't tell anything about the images. It's just outputting random probability scores.
The generator architecture and discriminator architecture
At this point, you have a conceptual understanding of how the generator and the discriminator work in a generative adversarial network. You know the objectives of each of these components. With that in mind, let's set up the neural networks for both the generator and the discriminator. Now, the generator takes in latent noise as an input and generates an image at the output. The size of the noise input that you feed into the generator network is a hyperparameter, something that you can configure. I've chosen to have a latent noise size of 100. The noise input will be a single dimensional vector of 100 pixels. The objective of the generator is to generate an image at the output. Now remember our generator and discriminator are both dense neural networks, so they don't work with two-dimensional data. That is, they can't work with your images in the original form. We'll have the generator generate flattened versions of the images that is in the form of a 1D vector, which we can then reshape to be a 28 by 28 image. That means the image size is 784 pixels, 28 multiplied by 28 gives us 784. So every generated image will be a single vector of 784 pixels that we'll reshape to get a 28 by 28 image. Next, we'll define the neural network for the generator. This is the generator class that derives from the nn.Module base class in PyTorch. Within the init method is where we've defined the neural network layers. The neural network is straightforward. It's a simple dense neural network instantiated using the nn.Sequential class. Observe that the input dimensions of the first linear layer is 100. This corresponds to the size of the latent noise that we feed into the generator. The output of the first linear layer is 64, and that dimension corresponds to the input dimension of the next linear layer, 64. The output of the second linear layer is 128, and then that corresponds to the 128 that is the input of the third linear layer. The dimensionality of the output of the third linear layer is 256, which connects to the input of the fourth linear layer, 256. The output of the final linear layer is 784 that corresponds to the size of the image. Remember 28 by 28 images. This last layer will output the generated image. Now, let's take a look at the activation functions for each layer. The first three linear layers all have ReLU activation. The generator output has been found to perform the best with Tanh as the final activation. Because the Tanh activation functions for scales, the output produced to be a value between -1 and 1. This allows the generator output to match the pixel values in real images. Remember, we normalize them to be in the range -1 to 1. The forward function in the generator is simply a forward pass through the generator. Latent noise is read in and an image is produced. Now that we've defined a generator class, let's just instantiate the generator neural network. I've referenced it as a netG, and let's print out the netG variable so that you can see the layers that make up the generator. Next, let's set up the second component within a generative adversarial network the generators adversary, the discriminator. Once again, I define a class discriminator deriving from the nn.Module base class. Within the init function, we have the layers of the discriminator. The discriminator is just a simple classification model. It takes in an image as an input and outputs a probability score which determines the discriminators probability that the input was a real image. The network that makes up the discriminator is a simple, dense neural network, once again, instantiated using the nn.Sequential class. The first linear layer has an input dimension of 784. Remember, this is where we feed in images, whether the real or fake, so that layer's input dimensions have to match the size of the images, 28 by 28 pixels. The linear layers of the discriminator are on Lines 6, 10, 14, and 18. You can see that the output of each linear layer is fed to the next one in sequence. Every linear layer, except for the last one, has the LeakyReLU activation function. In practice, the use of the LeakyReLU allows gradients to flow better through the neural network architecture. For negative values of input, the LeakyReLU activation returns a slight negative value rather than a value of zero. This is the leak that greatly mitigates the problems of dying neurons in your neural network, and improves the ability of your neural network to learn from the data. The value of 0.2 that we passed into the LeakyReLU function is the magnitude of the leak that we can control. Every linear layer is followed by a dropout layer on Lines 8, 12, and 16. This dropout turns off neurons during the training process of the discriminator, thus allowing the network to learn better from the input data and mitigating overfitting on the training data. On Line 19, observe that the activation function of the final layer is the sigmoid activation. Remember, the discriminator is just a classifier and the output is a probability score. That is a score between zero and one that the generated image is real. The sigmoid activation function will allow the discriminator to generate prediction probabilities in the range zero to one. And then, of course, we have the forward function, which is what is invoked when we make a forward pass through the discriminator. Now, that we've understood the architecture of the discriminator, let's instantiate a discriminator neural network, as well. And you can take a look at the layers that make up the discriminator.
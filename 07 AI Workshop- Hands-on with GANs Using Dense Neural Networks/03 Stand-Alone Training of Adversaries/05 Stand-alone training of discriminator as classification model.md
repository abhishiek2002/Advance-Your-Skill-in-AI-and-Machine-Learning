Stand-alone training of discriminator as classification model
In order to understand how the generator and discriminator work, what we'll do first is train each of them independently without training the other. So first we'll train only the discriminator with real and fake images, and then we'll train only the generator. And then we'll train the generative adversarial network where we'll train the generator and discriminator turn by turn. We're training the discriminator and generator separately only for learning purposes. The actual generative adversarial network requires that both of them be trained together. We'll start off by training the discriminator first. Discriminator is just a classification model that has to learn to differentiate between real images and fake images. So the loss function that we'll use to train the discriminator is one that's commonly used for a classification model. The binary cross-entropy loss function or the BCELoss. The BCELoss function is a variant of the cross-entropy loss for binary classification or categorization. The cross-entropy measures the difference between two probability distributions, and we'll use this to measure its difference between the prediction probabilities output by the discriminator and the actual values in the labeled data. Next, we'll instantiate the optimizers that we'll use to train the generator and the discriminator. I'm going to instantiate both optimizers, and notice I have specified a learning rate of 0.0002. Both of the optimizers are Adam optimizers. We'll make use of the D optimizer or the discriminators optimizer when we train the discriminator, and later on when we train the generator separately, we'll make use of the G optimizer. The Adam optimizer adaptively adjusts the learning rates for each parameter during training, making it well-suited for non-convex optimization problems typically encountered in deep learning models. The Adam optimizer was found to work well for GANs, and that's why we are using the Adam optimizer for both the discriminator and the generator. Next, let's set up the values that we'll use for real images and fake images. Real labels correspond to one. Fake labels correspond to zero. Next, let's take a look at the code where we only train the discriminator. We want the discriminator to be able to identify real images as real, and fake images generated from the generator as fake. Since we're only training the discriminator and not the generator, you'll find that during the training process, the generator does not improve at all, which means the discriminator very easily figures out what images are real and what are fake, and achieves 100% accuracy. netD.train will make sure that the discriminator neural network is in training mode. netG.eval will ensure that the generator is in the evaluation mode and its weights are not updated while we train the discriminator. Now, I'm only going to train for one epoch, and this is more than sufficient for the discriminator to get amazing at what it does. The generator we know is not being trained, so it doesn't improve at all. I run a for loop for each epoch on Line 9, and then a nested for loop on Line 10, iterating over the batches of data in each epoch. The training data is, of course, our Fashion-MNIST images. For each iteration of training the discriminator, remember, it's just like training a simple classification model, zero out the gradients of the discriminator. This is the code on Line 12. On Line 15, we access and reshape the current batch of images from the training data. Remember, each image has to be flattened to be 784 pixels. On Line 17, we get the labels for this real data. Remember, real labels will have a value of one. Now on Line 20, we make a forward pass through the discriminator with the real images. The view -1 flattens the tensor received at the output of the discriminator. Remember this tensor will contain probability scores. We then compute the loss for this pass through the model. This is computed on Line 24. This is the loss on the real data. Remember that the discriminator tries to maximize the probability of classifying real images as real. On Line 27, we make a backward pass through the discriminator network to compute gradients and then we compute D_x. D_x here is the score of the discriminator on the real data. Next, we have to train the discriminator on fake images generated by the generator. On Line 32, we initialize the latent noise variable that will feed into the generator to generate fake images. We make a forward pass through the generator on Line 34 and get the fake images. On Line 35, we set up the labels for this batch of images. Remember, these are all generated images. They'll all be categorized as zero or fake. Next, we make a forward pass through the discriminator on Line 38 with the fake images. The output will now have the predictions of the discriminator on the fake images, and we compute the error on these fake images on Line 42 using BCELoss. Note here that the objective of the discriminator is to classify fake images as fake, and that's the probability that the discriminator tries to maximize. We make one more backward pass through the discriminator on Line 45, computing gradients for the loss that was computed for the predictions on fake images. And on Line 47, D_G_z1 is the score of the discriminator on the fake images. On Line 50, we compute the total error of the discriminator, the error on the real images categorization and the fake images classification. And then finally on Line 53, we call optimizer.step to update the model weights of the discriminator. After every 100 batches on Lines 56 through 61, we print out the current epoch, the discriminators loss on the real data, the discriminators loss on the fake data, the total loss, and the score of the discriminator on the fake data and the real data. And this is the simple discriminator classification model that I'm about to train. Let's run training. It should run through very quickly because it's just for a single epoch. And let's evaluate the results. Observe that after the first 100 batches, the loss on the real images is 0.73, the loss on the fake images 0.64. Both the fake score and the real score are around 0.5, indicating that the discriminator is guessing at random what image is real, what image is fake. But notice immediately after that, after the next 100 batches of training, the real loss goes to 0.001, a very small number, fake loss goes down to 0.015. The fake score is very close to zero, indicating that it's accurately identifying fakes, and the real score is very close to one. Basically, after 300 batches of training, the fake score is 0.002 and the real score is one. And if you scroll down at the end of one epoch, the fake score is exactly zero and the real score is exactly one. The discriminator had two easier job to do because the generator wasn't improving and its fakes were very easily identified.
Problems with GANs and potential mitigations
Though generative adversarial networks have been around for a while, they are still an active area of research, and there are many problems that haven't entirely been solved, yet. Here are some of the common problems encountered by GANs. GANs are prone to vanishing gradients, which will essentially affect the training of the GAN. They are prone to mode collapse, and they may also fail to converge. Now, all of these common problems are areas of active research, and you can't say that any of these problems have been completely solved. If the discriminator in the GAN works too well and is always able to identify real images and fake images well, the generator can fail to improve due to vanishing gradients. If the discriminator performs too well, it means that the discriminator does not give the generator enough information so that it can improve the samples that it produces. The generator can't tell what kinds of samples will actually fool the discriminator. This problem in the original paper was greatly mitigated by using improved loss functions, such as the modified minimax loss and the Wasserstein loss function. Another problem commonly encountered in GANs is that of mode collapse. Now, ideally, the generator in the GAN should produce different outputs for the random noise that is fed into the generator. For example, if the generator generates handwritten digits, then it should produce ones and twos and threes and all digits one through nine. It shouldn't constantly just produce fives or sixes. However, mode collapse means that the generator learns to produce just one or two plausible inputs and always produces the same set. It might see that nines are fooling the discriminator well, so it will constantly produce nines, or it will constantly produce threes. It may not produce the whole range of possible outputs. This occurs when the generator over optimizes for the discriminator and produces a small number of outputs. If the generator feels that when it produces nines that consistently fools the discriminator, it might always produce nines, and this failure is referred to as mode collapse. Failures due to mode collapse can be mitigated by using improved loss functions such as the Wasserstein loss function. Another technique is to use a GAN variant called the unrolled GAN where the generator loss incorporates the outputs of future discriminators. Another major source of GAN failure is the failure to converge. Now, what does it mean for a GAN to converge? Think about what happens during the training process of a GAN. As the generator improves with training, the discriminator steadily gets worse and it's unable to distinguish real images from fake. At some point, the discriminator is just guessing at random. If the discriminator produces just random feedback, the generator might train using this random feedback, and then the generator's quality will also degrade. The generator that had improved to generate good quality samples might start getting worse beyond the point. Now, how is this solved? You can solve this by adding noise to discriminator inputs and by penalizing discriminator weights.
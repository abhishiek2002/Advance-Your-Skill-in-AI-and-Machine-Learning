Understanding the minimax loss function
Let's dig a little deeper into this minimax loss function, because this is the loss function that we're going to be using to train our generative adversarial network. And in this movie, I'll show you how the minimax loss function is just derived from the binary cross-entropy loss which you might have encountered when you train classification models. Now, here is a formula for the minimax loss function. Now, when you look at this formula, I must say it looks pretty daunting. There's a lot going on in here, but I'm going to break this down bit by bit. This is the loss function that the generator tries to minimize and the discriminator tries to maximize. That's why you see the minG maxD off to the left. Let's look at this D(x) here in this first component. This refers to the discriminator's estimate of the probability that a real data instance x is indeed real. As you might imagine, the discriminator tries to maximize this probability. Now, D(x) is operated upon this term. This term that you see here on the left is just the expected value of this log probability over all of the real data instances that are fed into the discriminator. Now, let's look at this G(z) term in the second component of this loss function. This is the output of the generator for a given noise input z. G(z) here represents the fake generated sample. D(G(z)) is just the discriminator's estimate of the probability that a fake data instance is real. This is something that the discriminator would like to minimize and the generator would like to maximize. Similarly, E(z) is just the expected value over all random inputs to the generator, so the expected value over all generated fake instances G(z). Now if you remember, the discriminator tries to maximize this loss function. The discriminator tries to maximize the probability that it classifies real instances as real, that is, D(x), and minimize the probability of classifying fake data instances as real. So D(G(z)), the discriminator tries to minimize, so it tries to maximize 1 - D(G(z)). Overall, the discriminator tries to maximize this loss function. Now, the generator tries to minimize this loss function. The objective of the generator is to get the discriminator to classify fake data instances as real, so the generator tries to maximize D(G(z)), and thus tries to minimize 1 - D(G(z)). Now, this formula that we just broke down for the minimax loss function actually derives from the cross-entropy between a real distribution and the generated distribution. The binary cross-entropy loss function is one that you may have used while training a classification model. The cross-entropy is the measure of the distance between two distributions, and essentially we are trying to see how far the distribution of the predictions from the model is from the real distribution in the data. Let's start with the formula for the cross-entropy loss function. I'm afraid we can't really derive this formula here, that's out of scope for this particular course, but this is the formula and you can accept it as a given. Y here refers to the ground truth table, and y-hat refers to the prediction from the model. The label associated with the real image is one, and the prediction from the discriminator for a real image is D(x). Let's replace those terms here in this formula. The right-hand side is updated with D(x), representing the predictions from the model, and the label Y replaced by one. Now, the 1 - 1 in the second component essentially means that the second component becomes zero, and the formula reduces to what you see here on screen. This component is the loss computed for the discriminator on the real data. I'm now going to put in zero as the label for fake images in the same cross-entropy formula. The prediction from the discriminator, that is, the output of a discriminator for a fake generated image is D(G(z)). Replacing Y with zero essentially causes the first term to be multiplied by zero, and that disappears and we are left with the second. So the formula on the right-hand side reduces to this single term. This is the loss of the discriminator on the fake data. The total loss of the discriminator is the sum of the losses on the real data and the fake data, and that gives us the loss of the discriminator using this formula. This loss function has a negative sign, and this is the loss that the discriminator tries to minimize. If you get rid of the negative sign, you can change minimize to maximize, which means that this is the loss function that the discriminator tries to maximize during the training process. Now, compare this with the minimax loss function that we saw earlier. The minimax loss function includes the expected value of these terms over the real instances and the generated instances. But essentially, the loss functions are exactly the same. What we've derived here from the binary cross-entropy loss is just the minimax loss function for the discriminator. Now, let's talk about the generator. The generator cannot really affect the discriminator predictions on real instances, so it can't really affect this first term. The generator has to focus all of its attention on the second term here. The generator can only attempt to minimize this second term, where it tries to get the discriminator to classify generated images as real. If D(G(z)) is equal to one, then that term will be minimized. And this is what gives us the generator loss.
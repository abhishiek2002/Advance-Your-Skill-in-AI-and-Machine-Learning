Setting up GAN training
At this point, we have a good understanding of the generator component and the discriminator component of a generative adversarial network. We are now finally ready to see our GAN in action by training both of them together. Let's start completely afresh. I instantiate a new generator network, a new discriminator network, and set up the Adam optimizer to train both of their parameters. Next, I'll instantiate a number of lists that we'll use to track different metrics. G_losses will track the generator's losses through the iterations of training. D_losses will track the discriminator's losses. The real_score_list and the fake_score_list will keep track of the score of the discriminator on real images and fake images. We'll train for 40 epochs. So we'll see how the generator improves in that period. As we proceed with the training at fixed intervals, we'll have the generator generate images using its current parameters. This will allow us to see how the generator improves over a period of time. We'll use the same fixed noise to generate those images, and I've initialized that on Line 9. And on Line 11 and 12, I ensure that both the discriminator and the generator are in training mode, so that their model parameters are updated. Execute this code cell. We are now set up to see the training loop for our generative adversarial network. You'll see that this training process simply brings together the discriminator training and generator training in one loop. The outer for loop iterates through the number of epochs of training. This is on Line 1. The inner for loop on Line 3 iterates over the batches of training data. First, we'll train the discriminator on a batch of real images and on a batch of fake images. Remember, the discriminator tries to maximize the probability of classifying real data as real and minimize the probability of classifying fake data as real. On Line 8, we zero out the gradients of the discriminator. On Lines 10 and 11, we access the batch of real images and the corresponding real labels. Remember, real images are labeled with one. On Line 13, we make a forward pass through the discriminator to get its predictions on the real images. On Line 15, we compute the loss of the discriminator on real images. The discriminator tries to minimize this loss. It tries to maximize the probability of categorizing real images as real. We compute the gradients on the discriminator on the real images on Line 17 by making a backward pass, and D_x will give us the discriminator score for the real images. The discriminator wants the score to be as close to one as possible. Next, we train the discriminator on fake images generated by the generator. We initialize the noise for the generator on Line 22. Make a forward pass through the generator on Line 24 to get the fake images. The fake labels comprise of all zeros. On Line 27, we pass the fake images generated by the generator through the discriminator. Now, the discriminator will try to categorize the fake images as fake, and that is the loss we are computing on Line 29. The discriminator will try to minimize this loss. That is, increase the probability of categorizing fake images as fake. On Line 32, we make another backward pass through the discriminator and compute gradients. Now, these gradients will be accumulated or summed with the previous gradients that were computed when we made a backward pass with the error on the real images. That's the code on Line 17. On Line 34, D_G_z1 will give the score of the discriminator on the fake images. The discriminator wants the score to be as close to zero as possible. And d_optimizer.step will update the parameters of the discriminator for this iteration. That completes the training of the discriminator for this iteration. In the same iteration, we train the generator, as well. On Line 44, we zero out the gradients for the generator. Then on Line 47, we take the fake images that the generator previously generated when we were training the discriminator earlier on in this iteration itself, and pass that through the discriminator again. The reason we do this is because the discriminator parameters have been updated in this iteration, which means that the discriminator has likely improved. So we get this improved discriminator predictions on the fake images. Next, on Line 51, we calculate the loss of the generator. The objective of the generator is to get the discriminator to classify its fake images as real. So the loss function is essentially comparing the output of the discriminator with real labels. And this is the loss that the generator tries to minimize. And this error will be minimized by the generator trying to maximize the probability that the discriminator categorizes fake images as real. We make a backward pass to compute gradients on the generator on Line 53, and on Line 55, we compute the discriminator score on the fake images. Now, the discriminator wants the score to be close to zero, whereas the generator wants the score to be close to one. And we update the generator's parameters in this iteration by calling g_optimizer.step. Every so often, we'll print out the training metrics of the model to screen. This is the print on Line 60, and every so often, we'll turn off gradients on the generator and discriminator. On Line 73, we'll have the generator generate fake images using the fixed noise that we had initialized earlier, and we'll display those fake images as a grid. This will allow us to see what the generator images look at at this point in the training process. And on Line 76 through 80, I compute and append the generator, discriminator losses and the real score and the fake score of the discriminator. In the next movie, we'll start the training process for the GAN and see how things work.
Introduction to artificial intelligence (AI)
You interact with artificial intelligence, AI for short, a lot more than you might realize. AI is becoming more pervasive in our workplaces, in government offices, hospitals, and schools. It is being used to automate repetitive work tasks, to help government officials determine who needs resources and how to allocate them, to aid doctors in early cancer detection, and to help review students' homework and recommend training to hone their skills. While AI can improve efficiency and productivity, it can also undermine efficiency, effectiveness, and equity. In this video, you'll gain a deeper understanding of what AI is and how it is being used. The term artificial intelligence was coined in 1955 by a team of researchers proposing a first-of-its-kind summer research project to be held at Dartmouth College. One of those researchers, Professor John McCarthy went on to further define AI as the science and engineering of making intelligent machines. As the field has developed, AI has come to refer to a broad branch of computer science concerned with developing algorithms and machines capable of automating analytical tasks and decision making. The capacity of AI systems is described in ranges. Narrow or weak AI can perform simple tasks, while general or strong AI can cope with generalized tasks, much like a human. Apple's Siri is a form of narrow AI, limited to simple tasks of question-and-answer and basic calculations. General AI is more associated with the concepts we see in science fiction films, including the movies Her and Terminator. The most common form of AI used today is machine learning, which is a form of narrow or weak AI. Machine learning uses statistical models and algorithms to draw inferences from patterns in data. These inferences are used to predict an output variable, for example, an action that should be taken or a decision that should be made. Examples include Netflix's recommendation algorithm that determines what types of movies to suggest, and a self-driving car that adjusts its speed to match traffic flow. Generally, there are two types of machine learning, supervised and unsupervised. Supervised machine learning infers from a set of example input-output pairs how to map an input to an output. Labeled data can be used, for example, to train an AI system to recognize an object that is round and red, with a stem, as an apple. Unsupervised learning does not use labeled data. Instead, the algorithm identifies distinguishing characteristics of an apple. That is, unsupervised learning tries to find hidden structures or patterns in unlabeled data. Perhaps it's the small stem, the striated coloring, or the shape and size that makes an object an apple. While the majority of AI in our lives today uses machine learning or narrow AI, the advent of more complex, general AI is on the horizon. With each passing day, AI is becoming more capable and more integrated into our lives. This is due to the exponential pace of growth in the volume of data collected, advancements in algorithms, and significant improvements in computational speed. But even with significant advances, current capabilities of AI are nowhere close to depictions in science fiction novels and movies. In the next video, I'll describe common myths and misunderstandings of AI to reveal why we need to carefully balance the real benefits and risks of artificial intelligence.
AI in HR and hiring
Before we take a look at the issues with AI for human resources, or HR for short, let's hear from an expert in responsible AI governance, Dr. Brandy Nonake, Director of the Citrus Policy Lab. Access to jobs is a critical equity issue. Applied appropriately, AI-powered human resource tools can improve efficiency, effectiveness, and equity. However, despite efforts to ensure equal employment opportunities for protected classes, laws are not keeping up with the advances in AI technology. The European Union has designated human resources as a high-risk area for the application of AI. This is because AI can automate and amplify bias in the workforce, especially if decisions are based on flawed or biased historical employment data. As an example, Amazon, using data sets that skewed heavily male, ended up scrapping their hiring tool because it started discriminating against women and the AI could not unlearn the bias it developed. To their credit, they shared their experiences and were transparent, which is critical for developing responsible AI. AI can help hiring managers write better job descriptions using tools like Textio that reduce biased language. Chatbox can be useful for collecting information, answering questions 24-7, and scheduling interviews. AI also can be used to anonymize applications at scale, which could be a boon for achieving equity, given that hiring discrimination against Black Americans hasn't declined in 25 years, and that resumes with candidate names that sound male and white are significantly more likely to result in an interview than those with candidate names that sound Asian, Black, Latinx, or female. With staff turnover in the US at an all-time high, the high cost of hiring, and an increase in the number of applications, companies are necessarily turning to AI to filter job applications. AI can make this process more efficient and can help hiring teams discover hidden talent by finding applicants who have the skills to do the job, but who might not otherwise exhibit historical indicators of success. For example, the US military is testing a voluntary AI app to surface talent in the reserves for specific projects that require skills like cloud computing or cybersecurity. Today, the military has no way of knowing who among their reserves has the relevant work experience, but an app called GigEagle, powered by AI, is being developed to identify that talent. The prototype focuses on voluntary staffing for short-term project needs. A problematic application of AI in HR is its use to monitor candidates' facial features and expressions during video interviews. Research analyzing the effectiveness of these AI systems identified unintended biases. For example, candidates who wore glasses or who had a bookcase behind them impacted the algorithm's personality assessment, potentially helping or costing someone a job. Leading AI researchers like Joy Boulawini, founder of the Algorithmic Justice League, have warned about these applications, finding that commercial facial recognition systems make less accurate assessments of women, and especially women of color. According to a recent Harvard Business Review article, candidates are increasingly finding themselves in front of a black box with companies failing to provide them with an accurate expectation of what data the technology was going to use and the limits of the data analysis. To avoid these issues, Dr. Nanaki suggests that companies be more transparent about and held accountable for using AI in HR. They should have diverse development teams, must acknowledge implicit and explicit biases in their data and models, and should put in place safeguards. Companies need to evaluate and mitigate bias in the data, identify any biases in the design of the model, and audit for bias in the final decisions or outcomes. It is also important to understand that the criteria themselves can be a source of bias. With a large number of US employers using AI for automating hiring, we are seeing that AI may inadvertently be filtering out qualified talent because of overly simplistic criteria to divide good and bad applicants. For example, by only considering candidates' credentials rather than their capabilities. The algorithms reflect only one slice of the data available on candidates. And since systems are not objective, responsible, or sentient, human oversight when designing and deploying AI in HR is essential. The World Economic Forum has produced a toolkit on human-centered artificial intelligence for human resources. And the Haas Center for Equity, Gender, and Leadership at UC Berkeley developed the Mitigating Bias in Artificial Intelligence playbook. Also, the University of California has developed guidance on responsible procurement and the development of AI in HR. I encourage you to check out these resources before continuing to the next video.
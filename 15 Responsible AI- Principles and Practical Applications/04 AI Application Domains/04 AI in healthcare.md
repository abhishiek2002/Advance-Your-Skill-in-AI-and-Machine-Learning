AI in healthcare
Before we take a deep dive into issues with the use of AI in healthcare, let's hear from Dr. Ziad Obermeier, Blue Cross of California Distinguished Associate Professor of Health Policy and Management at the UC Berkeley School of Public Health. His research focuses on the intersection of machine learning, medicine, and health policy. We want to find people who are going to get sick, like people who are going to have an exacerbation of their heart failure or their emphysema or something like that, so that we can get them on the right medications. And so we want to find people who are going to get sick, but there's no variable in our data set called get sick or health or anything like that. And so when you're building an algorithm, algorithms are very literal, a variable in a data set. They looked at who generated a lot of healthcare costs, and they use that as a proxy for who needed things for their health. And the problem is that not everybody gets care when they need it. And certain populations, non-white, poorer people with linguistic barriers, those people are less likely to get care when they need it. But the algorithm doesn't see like, oh, they should have gotten care, but they didn't. The algorithm just sees that this person was more expensive than that person. And so that person must have needed more help because we gave them more help. And the algorithm automates that differential attention of the healthcare system to different populations of people. And that's how these algorithms can scale up racial and other kinds of biases. Let's discuss some ways to address the issue Dr. Obermeyer raises so that we can better use algorithms to promote greater equity and improve health outcomes. To begin, we need to understand why we shouldn't always take data at at its face value. Consider how the data was collected and what sorts of social and economic parameters are hidden within that data. Factors include unequal access to the healthcare system, differential insurance, and disparate treatment of patients by doctors. For example, if a doctor doesn't take a patient's reported symptoms seriously, they're going to be less likely to order a diagnostic test to obtain data. And rather being diagnosed as a heart attack, A condition may be classified as indigestion or anxiety so that patient is sent home. These factors lead to problems with the data and disproportionately affect women, black people, and those from historically marginalized communities. By paying attention to problems in the data, we can improve algorithms and retrain them on variables that are much closer to patient's health and that are less prone to bias. By utilizing variables such as markers for chronic conditions and objective metrics of health that are grounded in laboratory tests rather than human judgment, AI can help predict which patients need treatment and recommend appropriate interventions. While AI poses risks, it can also lead to significant improvements in health. Let's explore some examples. Remember the expert we heard from in the beginning of this video? Dr. Obermeier's research found that AI can both cut down on unneeded tests and recommend testing for individuals who have been historically under-tested, such as people of color and lower-income patients. Dr. Obermeyer also conducted research using AI-trained on-patient data to improve the reading of X-rays and diagnosis of arthritis. The algorithms trained on patients' reports did a better job than doctors at accounting for the pain experienced by Black patients, apparently by discovering patterns of disease in the X-ray images that humans usually overlook. Medical device studies often involve trials on primarily white individuals. This is the reason why pulse oximeters that measure oxygen saturation are less accurate for people with darker skin pigmentation. It's critical to be aware of biases in testing and how homogenous data might affect the outcomes before using that data to train algorithms. The use of AI in diagnosing and treating mental health is even more challenging. Many of the issues with AI arise from replacing human judgment. Psychology relies heavily on human perception and has a history of being problematic and unfair to different populations. Despite these and privacy issues, the lack of availability, access, and affordability of mental health care has driven many startups to use AI to fill the gap. AI is being used to detect mental disorders based on web posts by looking for emotional patterns. AI can detect depression in your voice, and AI can flag those at risk of suicide. The Veterans Health Association's ReachVet pilot helps identify veterans at the highest risk of suicide by using AI that checks more than 140 variables and weights them to estimate someone's overall suicide risk at that moment in time and triggers outreach before a crisis strikes. Crisis Text Line, a prominent mental health support line and nonprofit, uses an AI chatbot to help people cope with traumas such as self-harm, emotional abuse, and thoughts of suicide. Google's multi-task Unified Model, or MUM, can detect, based on search patterns, who might be in crisis and at risk of suicide, and will place the National Suicide Prevention Lifeline and other resources for people in crisis at the top of their search results. So much of how algorithms perform, and whether they're fundamentally good and just algorithms, or whether they're flawed biased algorithms, so much of that comes from the data. And I think one of the huge challenges right now for people who want to build algorithms, whether it's for research or for kind of improving clinical care, is just how hard it is to access data. There was a paper a couple years ago that showed that of all of the AI products that were being used or had been FDA approved, like 60 or 70% of them were trained on populations from Palo Alto, Cambridge, or around the Mayo Clinic. And so I think that really speaks to a lot of the themes that we've touched on, which is that if algorithms are learning from these specific kinds of patients and from what those patients need, they're going to serve the needs of privileged people who happen to be in these data sets that are accessible to researchers and to product developers. And I think that's a huge problem. To address the lack of representation in datasets and to help developers create more inclusive products, Dr. Obermeier is providing access to better data with a non-profit organization called Nightingale and a company called Dandelion. Nightingale works with healthcare systems to produce datasets that can be used to train machine learning algorithms and makes them available for non-profit research anywhere in the world. Dandelion works with big health systems to to unlock their data and make that data available to people who want to build clinical AI products in a way that enhances patient care. For guidance and to learn about best practices to avoid AI bias in healthcare, I'd like to refer you to the Algorithmic Bias Playbook linked in the resources handout. In the playbook, you will find tips for people in the C-suite, technical teams, and regulators, along with four steps to diagnose causes, measure, and mitigate bias in commonly used healthcare algorithms. Related to human health and the health of our planet is climate change. Join me in the next video for a look at the relationship between AI and climate resilience.
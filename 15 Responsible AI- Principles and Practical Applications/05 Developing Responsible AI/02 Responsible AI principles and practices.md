Responsible AI principles and practices
In this video, you'll be introduced to emerging governance and technology strategies to support responsible and trustworthy AI development and deployment guided by responsible AI principles. Let's explore three strategies, AI documentation, fairness evaluation tools, and responsible AI training toolkits. First, the private and public sectors have promoted documentation of AI development. For example, IBM has developed fact sheets that encourage AI developers to record the purpose, performance, safety, security, and provenance of AI models. Google's model cards provide a framework for recording the provenance, usage, and ethics-informed evaluation of AI models. And Microsoft's data sheets for data sets and transparency notes encourage documentation of data provenance, composition, collection processes, motivations, and recommended uses. In the public sector, the cities of Helsinki and Amsterdam have launched AI registries to publicly record the data sets used in government-run AI models, how model predictions are used to inform decision-making, and how AI models are assessed for bias and discrimination. These documentation strategies support the responsible AI principles of transparency and explainability, accountability, reliability and safety, fairness and non-discrimination, and professional responsibility. Second, the private sector has also created a variety of fairness evaluation tools to check data and AI models for bias. For example, IBM's AI Fairness 360 Toolkit tests data sets and models for bias and provides recommended algorithms to mitigate identified biases. Microsoft's InterpretML can be applied to both Glassbox machine learning models, such as linear regression models, and to Blackbox machine learning models, such as deep neural networks. Google's What If tool enables developers and the public to identify and test, without needing deep technical expertise, predictive features in machine learning models and whether these features contribute to bias. These fairness evaluation tools support the responsible AI principles of transparency and explainability, reliability and safety, and fairness and non-discrimination. Third, responsible AI training toolkits, guides, and courses like this one can help organizations train their staff to identify and mitigate potential risks of AI. Numerous responsible AI training tools have been established, such as Google's People and AI Guidebook, Omidyar Network's Ethical Explorer, and the Ethical OS Toolkit developed by the Institute for the Future and the Omidyar Network. These toolkits introduce stakeholders, such as developers, product managers, and end users, to the benefits and risks of AI and encourage dialogue that can help shape the responsible design and deployment of AI. For example, Omidyar Network's Ethical Explorer contains a set of discussion cards on algorithmic bias. These cards provide questions meant to spark discussion and action, such as, do we have systems in place to limit algorithmic bias? What might we do if we discovered our users have been profiled or discriminated against? When considering appropriate governance and technology strategies to implement, your organization should consider the following questions. Do the AI governance and technology strategies align with established development and deployment practices within our organization? If not, what needs to change to support successful implementation? If we're procuring an AI system from a third party, what governance and technology strategies will we require the third party to implement before we acquire the technology? I encourage you to take a moment now to consider how your organization might implement one of the strategies presented in this video. What are the opportunities and barriers within your organization that would affect successful implementation of responsible and trustworthy AI? you
Responsible and trustworthy AI
As the benefits and risks of AI-enabled technologies become better understood, companies, non-profits, academic institutions, governments, and intergovernmental organizations are increasingly developing principles to guide the responsible development and use of AI-enabled technologies. In this video, I'll give you an overview of these principles and provide examples of how they are being applied in AI development and oversight. Watch, a nonprofit research and advocacy organization, has taken stock of responsible AI principles and guidelines established globally. Its AI Ethics Guidelines Global Inventory currently contains 167 sets of principles and guidelines. While responsible AI principles often vary in style and scope across different organizations, consensus has emerged around eight core principles. accountability, privacy and security, reliability and safety, transparency and explainability, fairness and non-discrimination, professional responsibility, human control, and the promotion of human values like civil and human rights. Let's explore some ways these eight core responsible AI principles are being implemented. The principle of accountability may be implemented through requirements that a developer or user of an AI-enabled tool monitor the proper functioning of the tool throughout its lifecycle. To support privacy and security, developers may implement privacy-by-design features such as data minimization, collecting and processing only personal data that is necessary to achieve the AI system's objective. As more data is used to train AI systems, ensuring the security of the data becomes increasingly important. To reduce potential vulnerabilities that data will be leaked or disclosed, a developer may decide to run an AI model locally on an end-user's device so that no personal data is transferred. To ensure reliability and safety, a developer may implement continuous monitoring to assess whether an AI system is performing reliably. In other words, it is repeatedly performing as intended. Performance reliability is closely related to safety. If an AI system is being implemented in a high-risk area, such as self-driving cars, developers should implement an AI risk or impact assessment to identify potential safety risks and necessary mitigations. We'll explore AI risk and impact assessments in more detail later in this chapter. Supporting transparency and explainability means that a developer communicates how and why they are using AI, using its features and limitations. Transparency and explainability are key to building greater trust among stakeholders who are interacting with or affected by the implementation of the AI system. Because AI systems are built on data that can reflect societal biases, one strategy for supporting fairness and non-discrimination is to implement robust tests to identify sources of bias in the data or model and methods to correct those biases before deployment. However, it is not always easy to operationalize concepts of bias into the development of AI models or implement tests and corrective actions. For example, the National Institute of Standards and Technology, NIST, issued a report in 2022 that outlined dozens of different types of bias that appear in AI models and several statistical and mathematical strategies to achieve fairness. As a result, human judgment is still needed. For example, a developer of a machine learning tool may decide it's important to mitigate gender bias. As such, the developer may put in place appropriate technological safeguards, such as repeated tests to assess whether the model is exhibiting gender-based discrimination. However, by focusing solely on gender-based discrimination, the model may inadvertently overlook racial bias, or bias that emerges at the intersection of race and gender. While the model has been optimized to mitigate gender-based discrimination, it may be committing race-based discrimination. Depending on the algorithmic fairness concept implemented, the machine learning tool can yield different predictions. Therefore, developers often need to make a value judgment in deciding how to achieve fairness. As the benefits and risks of AI systems are becoming evident, calls for professional responsibility have grown among those who design, develop, or deploy AI systems. Strategies to strengthen professional responsibility include working with a range of stakeholders who can critically question and evaluate the AI system, actively considering the societal and long-term effects of an AI system, and responsibly acting on those considerations, even if it means deciding not to implement an AI system due to its potential risks. Implementation of human control in AI systems means that any decision or action taken by an AI-enabled system must go through human review. For example, in high-stakes decision-making in healthcare or human resources, incorporating human control may mean that a radiologist reviews an ultrasound before additional tests are ordered, or a hiring manager reviews all resumes selected or not selected by an AI system before deciding whom to interview. Above all, AI systems should support the promotion of human values, in other words, civil and human rights. To do so, AI developers can conduct human rights impacts assessments, often referred to as HRIAs, of their AI system to identify and mitigate potential human rights harms. Responsible AI principles are key to helping organizations develop trustworthy artificial intelligence. As we transition to the next video, I encourage you to think about ways your organization may implement the responsible AI principles.
How were they developed?
- [Instructor] So how exactly are these generative models developed and how are they different to traditional machine learning models? It all comes down to the types of tools that were being used. So over time, Python libraries, like PyTorch and TensorFlow, had more and more investment and they made it easy and easier and more accessible to be able to train neural networks like this, especially neural networks that work together. So it's not a standard A to B type of flow. As you'll see in the architecture, it's kind of A to B, A to C, B to C connection between the different training and data sources. A lot of helper libraries, like Torchvision and Keras, also came about, which gave heaps of support for data loaders, for simple code management, and other tools to make it really easy to start training models. Additional cloud resources as well, like Azure and Amazon Web Services, they provide a number of code bases and tools to make training a lot faster and a lot more accessible. The datasets also increased over time. So research open data sets like MNIST handwritten digits, which we'll speak about in this course, and also CIFAR-10, which again are both in the original research paper, we used to be able to extend other research projects and get a deeper understanding of how to train these models. More and more niche research datasets became available for researchers as well, but they usually require you to sign up with your university or college email address. The web also grew far bigger, making it possible, really, download and sample images from a number of different data sources. So, in essence, many different tools were developed to create GANs and they really improved as the technology improved. Some libraries did need to be created, but those libraries helped support the overall training functions. Open image datasets, as well, accelerated the process of research.
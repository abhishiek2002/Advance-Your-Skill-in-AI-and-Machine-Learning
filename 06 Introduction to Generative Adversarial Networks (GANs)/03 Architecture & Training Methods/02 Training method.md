Training method
- [Instructor] When it comes to generation, as we spoke about before, the training is not that different from traditional machine learning but the system and the architecture is quite different. So what does this actually mean, again, in this context? So machine learning training with a general definition for neural networks is about changing the weights of the model with backpropagation in order to reduce the overall loss. So what this actually means is for each neural network in both the generator and the discriminator models and neural networks, what we're doing is we're measuring a loss or the ability to differentiate between a synthesized sample and a real sample and using that to update the weights in both of the models with backpropagation. When we go through the actual code sample, we can show how that happens in real time. So everything is starting to come together. The end-to-end training pipeline will have both the generator and the discriminator taking data from each other, working out the loss, and then updating each of their weights. So what we're going to do is we'll start with a set of weights for each model. And actually, when it starts out, they'll be quite random but you can instantiate the models with preexisting weights. But for our example, we'll just use random weights. What we do is we train once or I should say we push all the data through the system once and then we record the overall loss. Then we take that loss and we update the weights of both of the models. Ideally, we make small adjustments so they both improve. Let's have a look at how this actually will work.
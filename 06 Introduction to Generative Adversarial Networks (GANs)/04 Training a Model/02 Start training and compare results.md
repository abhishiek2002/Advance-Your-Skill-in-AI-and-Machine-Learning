Start training and compare results
- [Instructor] In the previous video, we gave some examples about how to set up the generative adversarial network training. We created the generator and the discriminator, data loader, loss function, and the optimizers. Now we'll go through the real engine and talk about how the training works. So, the training is essentially one large loop. It starts off with taking an epoch from a number so you can run the data through how many times you like. In this case, I've just put it as one, but a good idea would be to have it as 10 or a hundred or however patient you are. But just for an example, we'll just pass it through once. So that's how many times the entire dataset gets processed through the network. Then there's a smaller loop that actually samples each batch from the data loader and then pushes that through the network and then updates the weights. So let's go step-by-step. First, with the discriminator object, which is called netD, we set gradients to zero, meaning that it's forgotten any other changes that it's seen before. We'll take one sample from the real data and pass it through the discriminator. And the way that we'll pass it through is just running the object around the data. So this we'll call the forward function in the code. That will generate the output. With that output and the real labels, we can generate the error by using the criterion function. Then we can generate the gradients by running the backwards function on that error. Then we can get an average across the output. Now we're going to look at passing through fake data. So we'll generate some fake data using the generator, by passing random noise through the generator. Then we'll run that through the discriminator and get output. We'll pass that through the criterion function to be able to work out what the error is. Generate the gradients using the backwards function, get the average of the output, add those two error values together so the error of the real and the error of the fake. And then run the optimizer, which will update the weights of the discriminator model. So optimizerD will be for the discriminator. Now we're going to do something similar for the generator. So set the gradients to zero, pass the fake data, the fake sample that we had before, through the discriminator. Get the error, base of the generator, update the gradients of the generator this time, get the average of the output, and then update the whites of the generator using the optimizerG object. Then, for each step, we'll print out some metrics and each hundred steps we'll be able to get a safe example of a sample from the real dataset and then a sample from the fake dataset. So I can show you exactly what that would look like. So a sample from the real dataset would look something like this. So these are real, handwritten digits. A sample after a large amount of training of the fake dataset will look like this. So they kind of look like digits, but this is the current state of the generator model. This is the quality level that it can work at.
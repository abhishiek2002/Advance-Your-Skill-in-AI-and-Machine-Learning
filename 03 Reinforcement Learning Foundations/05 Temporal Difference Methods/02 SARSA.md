SARSA
- [Instructor] SARSA is the first form of temporary difference methods and is the acronym for, State Action Reward next State and next Action. Which is the process taken by the agent to update the queue table and also mix up a complete reinforcement learning cycle. Let's go back to the room with the orange fruits and see how the reward is updated in the queue table. This time around, you're the agents trying to reach your goal of getting the orange. With every step you take, you get a reward and because you wouldn't wait until the end of an episode before getting your reward, as this is temporary difference learning, your reward has to be updated immediately. Note that initially, you take random policies to update the queue table, which was originally empty at the very start. So for your new step or change in states, he gets a reward. This reward say negative one because you haven't reached your goal yet, together with the state action value of the new states you just stepped into, will contribute to updating the state action value of the state you just left. So let's say the state action value of the new state you just stepped into is positive six. Adding negative one to positive six would give us positive five. Assuming the state action value of the previous state you left is positive four, to update it, you move it closer to positive five. Because positive five is the expected reward of that previous state. So the new action value of the previous state is say, 4.2. That seems like a lot. You should go over it again a multiple times and make sure you understand the update process. I'll explain SARSA again, using the Bellman Equation. First, a quick recap of the Bellman Equation, where V pi S is the state value of a state following a policy pi, E implies expectation. So the state value is an expected result, not the actual. R represents the reward of moving to another state where t plus one implies that to be the next state. Gamma is the discount factor that helps us favor essential rewards as opposed to all the rewards. V pi S t plus one is the state value of the next state while St is equal to S, implies that the whole equation is applicable to the current state S. For the Temporal Difference method, the only difference or change, is in how frequently the reward is updated. Gt here is the total reward gotten at the end of an episode. While a different reward is used for temporal difference method, which is the sum of the reward of the next state and the action value of the next state. Don't get confused with these equations. They are similar to the general one you are already familiar with. The state value function V pi S, is the expected value of the sum of reward and state value of the next state. For the Monte Carlo method, Q St At is the action value function, similar to the state value function, except that this is recorded after an action has already been taken. Alpha regulates the rates at which the agent lands. Gt is the cumulative reward gotten at the end of an episode from which the current state's value is subtracted. Generally, we want to update the current state value by adding to the current value the difference between the total expected reward and the current states value. For the Temporary Difference method, the difference comes in the rewards section and how it is gotten. Instead of the cumulative reward Gt as in the Monte Carlo method the reward used is the current reward of next state and the current state value of the next state. Every other part of this equation is similar to that of the Monte Carlo method.
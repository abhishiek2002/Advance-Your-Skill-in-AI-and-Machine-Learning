SARSAMAX (Q-learning)
- [Instructor] SARSAMAX is another form of temporal difference method, also popularly known as Q-Learning. It is just another slight change in the Bellman Equation or how the Q table is updated. Quick recap, for SARSA, we use the same policy to pick a state, select an action for the next state get the reward of selecting that action, landing in the next state and then choosing an action. After this cycle, it then updates the action value of the four state thereby updating the policy. This update cycle is different for SARSAMAX. The similarity between SARSA and SARSAMAX is that the same policy is only used to the point where the agent selects the second state. After that point, the policy in SARSAMAX is then updated by updating the action value of the first state before choosing the next action. This next action is selected using the Greedy Policy as opposed to the Epsilon-Greedy Policy used in Monte Carlo methods. Remember that greedy policies are policies that select actions that maximize the cumulative reward. While Epsilon-Greedy Policies give the chance to select other actions even if they do not maximize the cumulative reward. For the purpose of exploration, and to discover new and better actions. Back to SARSAMAX, the action chosen from the state is the one that maximizes the actual value for that state. Looking at both the SARSA and the SARSAMAX algorithm represented in the Bellman Equation we can see the difference. We can see clearly how the agent uses a Greedy Policy to select the next action instead of an Epsilon-Greedy Policy.
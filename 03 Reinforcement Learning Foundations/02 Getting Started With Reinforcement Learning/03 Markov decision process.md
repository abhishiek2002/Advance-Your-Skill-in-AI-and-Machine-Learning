Markov decision process
- [Instructor] One very important topic left to discuss when describing a reinforcement learning problem, is the Markov decision process. You might have been wondering how everything discussed in the previous lesson is even possible mathematically, or even in code. Markov decision process, MDP in short, is how reinforcement learning problems are represented mathematically. Its variables include states, actions, rewards, one step dynamics of the environment, which is the states transition probability, and the discount factor. I know, I didn't mention discount factor before, so I'll do justice to that. Let's go back to the race example, but this time around we are running forever, definitely not possible. We are assuming this is a continuing task, as opposed to the hundred meter sprint, which is an episodic task. Now, because we're running forever, we are not very sure what the future holds, or whether our future rewards will be any better than the present. Due to this, we favor and cherish our present rewards, as opposed to the ones we might or might not get. The discount factor helps us make this possible mathematically. So now that we understand the discount factor, which is also called the reward function, let's consider the Markov decision process, MDP as a whole. Like I mentioned earlier, it has a set of states, a set of actions, a discount factor, gamma, which is set to a value less than one. The closer it is to one, the more priority is given to the present reward. While the closer it is to zero, the more the agent considers future rewards, which is not very good in the long run. So the discount factor is normally set to 0.9. Then there is a set of rewards, which the agent doesn't know initially. And finally, the state transition probability, which is the probability that the agent moves from one state to another, also referred to as the one step dynamics of the environment. Now, how exactly does this work? You might ask. I'll use another simple example. Assume you're the agent, and you're in a dark room. This room is empty except it contains parts of something you currently want. Well, for me, maybe a fruit salad. So there is an apple, a pawpaw, some call it papaya, which could be different in different parts of the world, a water melon and a banana, scattered around the room. And I need to pick all of them up to make my fruit salad. We could assume again that the floors are tiled with square tiles, to be able to keep track of our state. My current state is the first tile at the entrance of the room. If I decide to move forward to the next tile in front of me, I have taken an action, forward, in this case, into another state. There is a probability with which I have decided to move forward. That probability will be different, if I have decided to move backwards, left, or right. Moving from the first tile to the second, I would either get a reward immediately, or my reward could be after a set of actions, as mentioned earlier when explaining the episode. But now, I'm going with the former. So I get the reward after moving from tile one to tile two in front of me. If one of the fruits is at tile two, good, I get a positive reward for reaching one of my goals. That reward is let's say, positive 10. However, if in stage two, there is no fruit, I get a negative reward, say negative three. I will keep iterating this whole process, until I get all the fruits. Notice I have used different RL cases in my examples. So if you're trying to solve any problem, you have to consider if it can be broken down, and have all these different paths, the states, actions, possible rewards, and missions.
A basic RL solution
- [Instructor] An equation called bellman equation is used to solve the markov decision process. Before we proceed you need to understand some new terms, the state value function and the actual value function. They both represent the same thing which is the bellman equation. The state value function is the expected value of the reward in a particular state. Did you get that? It is the total reward gained by the agent from its current state to the goal state. Let's go back to the room containing fruits for my fruit salad. I decided to move forward, left, forward and forward again and that policy takes me to my first fruit the banana that's one policy I can take. My cumulative reward from the first state to the fourth state is the state value of the four states because that's where I started. If I want to get the state value of the second state I would sum up my rewards from the second state to the fourth state and so on. Note that different policies or paths would result in different state values even if they have the same stats and end state. The actual value function is used to get more optimal policies as it takes both action and the states of the agents into account when calculating rewards as opposed to the state value function which considers only the state. Note in both cases, we want to find the best path that will give us the optimal rewards. This path is known as the optimal policy. Now let's look at various forms of the bellman equation. Are there different ways to solve a reinforcement learning problem?
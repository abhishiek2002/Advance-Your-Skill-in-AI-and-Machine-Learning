A basic RL problem
- [Presenter] Now, how do you recognize a reinforcement learning problem? It has to have some or all of the properties stated in the previous lesson. By some, I mean in cases where the model of the environment is unknown. In such cases, the agent is set to be performing a model free prediction. This means it's trying to predict its next action in a state without knowing what the environment looks like. The second way of learning by the agent is the model-based prediction method. Opposite of model free, where the agent learns with full knowledge of its environment. Let's assume you're the agent and you move to a new place could be for vacation or you're relocating. You don't know the location of any place or anything. You visit a few new places a day and learn more about your environment and learn new ways to navigate it. This way, if you take a new action you're learning in the model free environment. On the other hand, you just woke up from bed in a house that you've been living in for over two years, you know its size and where everything is. In this case, if you take any new action, you're learning in a model-based environment. In both cases, your every action leads to a reward which could be positive or negative. This reward helps you decide on your next action and that's exactly how a reinforcement learning agent learns. Some agents have goals or missions to accomplish and once they're done accomplishing those missions they stop learning. These kinds of tasks are known as episodic task as they happen in an episode and have a final state. Some other tasks by the agents do not have an end and are known as continuing tasks. Most episodic tasks are solved via model-based methods because they are usually short which means a simpler environment to understand. While most continuing tasks are model-free because they have a larger environment space which cannot be totally understood by the agent. To not get confused later on, it's important that you know that the term episode could be used in a different context in reinforcement learning. Let's flash back to our understanding of rewards. The agent gets a reward after performing an action which could be right or wrong. In some types of reinforcement learning methods that we'll consider later and in the course, the reward doesn't come immediately after the action. But after a set of actions, this set of different actions in different states before a reward is known as an episode. So many episodes can occur before an agent reaches its goal and before getting its final reward which is the sum of all the rewards gotten at the end of the episodes. I'll use an example. Assume you are an agent in a race competition where you have to run 100 meters and you have a mission to spend the least time on the truck and reach your goal early. You don't just get a reward for the race by reaching the end, instead you get it at intervals. We can say after every 10 meters where every 10 meters is an episode in the race. What this will do is help you track what actions lead to you having the best final results. Assume at the seventh interval, you spend just six seconds instead of 10 by leaning your head forward while running. You'd want to try that as other intervals too for an overall shorter time therefore a final best reward. Please note that getting a reward after episodes instead of after each episodes does not happen in all reinforcement learning methods. The reason the total reward is not acquired as one result at the end instead of at intervals is to be able to track what actions lead to which reward so as to do more of them.
Monte Carlo prediction
- [Instructor] Monte-Carlo predictions can be understood using the Q-table, which is used to start the state values of all actions performed by the agent. It answers the question, "Given a policy, how will the agents estimate "the value function or the expected cumulative reward?" The Monte-Carlo prediction is just the Monte Carlo method as explained in earlier lessons. It uses the Bellman equation to estimate the state and action value functions, otherwise known as the expected cumulative reward for following the policy. The Bellman equation for the Monte Carlo method is represented by this equation. In simple terms, the state value of a current state is the expected value of the reward of the next state plus the state value of the same next state. Where V pi S is the state value of a state following a policy pi, E here implies expectation. So the state value is an expected result not the actual. R represents the reward of moving to another state, while T plus one in place to be the next state. Gamma is the discount factor that we discussed earlier which helps us favor recent rewards, as opposed to older rewards. V pi S, T plus one, is the state value of the next state while ST is equal to S, implies that the whole equation is applicable to a current state S. This equation is used to solve a reinforcement learning problem using the mark of decision process and thus interpreted directly in code as you'll see later in this course.
Additional modifications
- [Instructor] I'm going to put some official terms to some of the things I mentioned in previous lessons, and also introduce you to some new ones. One is the greedy policy. A policy is greedy when it only selects the best action, for a given state, all the time. Like we saw earlier, when an agent selects the best action from all the episodes to improve its policy. An improvement over the greedy policy, is the epsilon-greedy policy. When a policy is epsilon greedy, the agent sometimes gives the opportunity to explore other actions that are not the best as explained, when I talked about exploration, and exploitation. Another new term is the incremental mean. This provides a bit of improvement over the regular Monte Carlo method, tending towards the temporal difference method. Instead of updating the policy after a set of episodes, which normally helps decide the best action, the incremental mean helps us update the policy after every episode. Initially first state, we record the reward from multiple episodes, and then deduce the action value, by selecting the largest reward. On the other hand, using incremental mean, we improve the process by getting the action value after every episode, instead of after multiple episodes. The result of the action value is the average of all previous rewards. So for example, if in episode one, the agent gets a reward of two, for episode two, the agent gets a reward of four, for episode three, a reward of six. The corresponding action values for the episode, will be, for the first episode, two, because that's the only reward gotten so far. Three for the second episode, gotten from the average of two and four, which is the average reward of the first and second episode. And six for the third episode, which is the average of two, four and six, the current reward, and those of the previous episodes. The last term I want to talk about for this lesson is the constant alpha. This helps to regulate the rate at which the agent learns. Constant alpha is also called the learning rate, and is generally sets of value between zero and one. Setting it to zero means the Q-table is never updated, meaning it's not learning, while setting alpha to a high-value such as 0.9, means that learning can occur quickly.
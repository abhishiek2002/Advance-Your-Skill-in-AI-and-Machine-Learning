Monte Carlo method
- [Instructor] The Monte Carlo method is used mostly in episodic tasks, tasks that have a definite end. This method is one way an agents can get the best policy, path or trajectory, so as to get the best cumulative reward. Recall the room full of fruits, but now we'll simplify to have just one fruit, an orange. My goal is to get that orange and I am done, meaning that's the end of that episode. I perform many episodes that I believe might have better policies and lead me to better cumulative rewards and then compare them so as to get the actions that work best in all those episodes. So here's how it works for an episode. Once I step into the room, I'm on the first tile which is my first state. At this point I have four possible actions, I could decide to move left, right, forward or backward. None of these actions have a preference because this is the first time I am stepping into the room and I don't know which states will lead miss the best reward. There is an equal chance of selecting any of the states, and my actions are also chosen at random. This means I am initially going to follow an equiprobable random policy. This is a policy that has an equal chance of being selected at random. After taking many of this episodes using the equiprobable random policy, the agents will learn a better policy from studying the rewards of the previous policies. At my first state, if I decided to take action right and got a cumulative reward of positive 10 after reaching my goal, which is where the orange is. And in another state, I chose action left and got a cumulative reward of positive seven, action forward and got a cumulative reward of positive eight, action backwards and got a cumulative reward of negative one. All in different episodes I would deduce that selecting action right in the first state would be my go-to action for that state, in order to get the maximum reward. I will then compare the results for the second state also and see which of the actions from the states led to a maximum cumulative reward, and make that my go-to action for that state. Then I do that for the third, fourth and all of the future states until I get to the final state where the orange is. All these results of the community reward for different states needs to be stored somewhere for easy comparison, and that's what the Q-table is used for. It's just updated with every episode for each state and action taken in that state with the cumulative reward of starting from that state. This table will be discussed in more detail in a bit when we deep dive into Monte-Carlo methods.
Temporal difference methods
- [Instructor] Temporal difference methods are the second major category of ways in which reinforcement learning agents learn. Unlike the Monte Carlo method, temporary difference methods update the Q-table at every time step, instead of the end of every episode. There are more realistic ways of learning and solving tasks because real world tasks are mostly continuous. We need to know if we're taking the right action or wrong one at every time step instead of waiting over a long period of time when damages might have been caused. Note that this also works for episodic tasks. Considering a real world application of artificial intelligence systems like self-driving cars, the agent drive in the car needs to know if it's a slow down at the junction or maintain its speed. If it has to wait after an episode before knowing the right action to take, it might have crashed into another vehicle and caused an accident. This is the major difference between the Monte Carlo method and the temporal difference methods.
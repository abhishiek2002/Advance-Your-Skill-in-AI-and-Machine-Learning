Benchmarking models
- [Instructor] A key activity in the machine learning life cycle is benchmarking of models. With respect to machine learning, benchmarking is the activity of comparing models and their versions against baselines and other competing models to understand how they perform against each other with respect to the stated project requirements and environments. Benchmarking happens after a model is found to pass its fitness test and is ready for integration. Benchmarking tests the model in environments that are closer to production, and test for both performance and operational metrics. What setup is required for a benchmarking environment? First, it needs hardware compute power, like CPU, memory and discs. It also needs an isolated software and network setup where the benchmarking process won't be impacted by other activities. Next, it needs a test hardness. Both the baseline and the new model need to be subjected to the same test hardness. The test hardness contains a test data set that will be fed to the model and the expected results from the model. There will also be test executors that can execute the test cases with required pacing and capacity. Then comes the benchmarking tracking platform. The platform should provide for logging of prediction requests, the actual predictions, observed latency, and errors. Resource utilization during execution should also be monitored. Then various performance and operational metrics need to be computed. The tracking platform can use a combination of standard DevOps, observability platforms and ML experiment tracking platforms. Finally, the mode of testing also needs to be determined. It could be that both models run simultaneously at the same time, or it's a single model that is executed and then compared against the baseline. This is use case specific. Benchmarking activities should result in collection of various metrics for analytics. First, there are the model performance or effectiveness metrics that need to be computed, including accuracy, F1-scores, et cetera. Then operational metrics also need to be computed, like latency, scaling and resource utilization. A given model may perform better than its baseline when executed from a notebook. But when executed in a benchmarking setup, it may require more resources and may create additional latency. The benchmarking process provides validation that the model is fit for production for both performance and operational requirements. Let's review some of the best practices for benchmarking. Benchmarking should enable apples-to-apples comparison and care should be taken to make sure that it is indeed so. It's recommended to log all actions, results and resource utilization at the most granular level as possible. This is required for troubleshooting, if needed. The benchmarking task should be automated and integrated into the training and deployment pipelines. As the model gets promoted, benchmarking should run automatically and compared to its baselines. Experiment tracking tools can be used to track benchmarking results for long-term performance analytics.
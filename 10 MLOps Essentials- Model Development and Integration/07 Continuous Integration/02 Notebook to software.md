Notebook to software
- [Instructor] The notebook to software step deals with producing a deployable and executable version of the model. Code in ML notebooks need to be converted to executable software before it can be integrated and deployed. This conversion should ensure that processing and model aspects from the notebook are faithfully reproduced into the code, and has the same performance as the notebook. There are multiple ways in which a model can be made into an executable form. This depends upon the type of use case and deployment. To begin with, the model can be provided as embedded software, where it gets embedded into non-ML executables. Alternatively, it can be its own running service that receives requests from clients and returns responses. The use cases could be batch or historical predictions and streaming or online predictions. If it's an embedded software, it does not make a difference between the use cases. The model could be a function in a program. It could be made available as an SDK. It could be a Python package. It can be a pipeline Lambda function. It can be a form of bundled solution that is sold. If it runs as a service, the deployment forms will vary. For batch use cases, the model itself can be a batch ML job that reads data from a database and writes predictions back to the database. It can be an API, like a rest API, that receives requests from clients and returns predictions as responses. For streaming, APIs can do the work also. Predictions can be a stream processing job that reads requests from a queue and writes the predictions to another queue. Choose a form that is optimal for the use case being designed. Whichever deployment form is chosen, the process of conversion is similar. What needs to be converted and added to the executable? First, required steps for pre-processing of input data needs to be added. These may be steps like tokenization, encoding, input scaling, et cetera. Next, the model filed from the registry needs to be brought in. The executable can either package the model file as a part of the deployment package or read it in real time from an external repository. Next, the inference step needs to be coded in. After that, any kind of post-processing, like decoding, reverse scaling, conversion to business outcomes, et cetera, should also be added in. The code should provide interfaces to receive inputs and return outputs. These may be function calls, rest APIs or database queries. Finally, the build, test and deployment infrastructure for this executable also needs to be built, like any other software module. Once the notebook is converted to software, it is critical to do regression testing of the model to make sure that it has not lost any of the steps or performance that it had in the notebook. Here, it's recommended to use the benchmark results for the model as the baseline. Then, use the same input data used for benchmarking as input for testing. Regulation should ensure that the model delivers the same performance as the original notebook. If not, it needs to be fixed and retested. Special attention should be given to the pre-processing and post-processing steps. And their regression should also be tested. There are some tools available that can take a notebook and automatically convert that into a service. Explore those if they would meet your requirements before getting into manual conversion.
Training with generative AI
- [Instructor] Generative AI is the newest class of AI that is revolutionizing how machine learning could be used to help automate enterprise use cases. While most of the ML ops principles discussed in this course apply to generative AI also, there are some special considerations. In the case of model training, there are some unique processes. Let's briefly discuss them in this video. How are training models different in GenAI, compared to traditional machine learning? In GenAI, there are two key tasks namely, pre-training and fine-tuning in the training phase. In pre-training, the goal is to create a GenAI model from scratch. These models are also called foundation models. These models are huge in size with billions of model parameters. The goal of pre-training is to create a general purpose model using general purpose data sets. Typically, data sets like the entire Wikipedia are used build foundation models. The foundation model learns general patterns and structures in data. Typically, the model learns linguistics as well as general knowledge from the dataset. Building these models takes significant effort and time. The costs are really high, hence only few such models are built and shared in the GenAI world. The other task in training GenAI models is fine-tuning. In fine-tuning, a foundation model is taken and then fine-tuned to a specific domain or task. This is done when the foundation model does not sufficiently handle the specific domain or task. For fine-tuning domain or task-specific data sets are used. These data sets are typically enterprise-specific and collected from internal sources. This helps the model learn task-specific patterns and structures. This learning can supplement or override existing knowledge captured within the foundation model. This fine tuning process costs significantly lower than pre-training. It also takes less time when the data sets are readily available. How are enterprises training GenAI? In most of the cases, enterprises use fine-tuning to customize a foundation model to the use case using use case-specific data sets. Pre-training is left to the creators of general purpose foundation models.
Managed training pipelines
- [Instructor] Having discussed the data engineering side of MLOps. Let's get into model training in this chapter and how MLOps helps in making it efficient. We'll start off with managed training pipelines. Similar to managed data pipelines, training pipelines play a vital role in the ML workflow. A robust managed training pipeline helps create repeatable ML training and testing workflows while reducing human costs. Being the core activity, as well as the most unpredictable activity in building an ML application, ML training can benefit a lot from an organized MLOps setup for the project. What are the key training pipeline functions? It starts with the feature store. Training inputs are fetched from the feature store for model training. The hyper parameters are also set up for training. An experiment is then planned and executed. Executing the experiment results in the ML model. The model is then validated during training to ensure that desire levels of performance are achieved. Then an independent test data set is used from the feature store and the model is tested with this data set to analyze out of sample errors. The results of this testing are reviewed to see if desired performance goals are met, And if there are more opportunities for improvement, new experiments are planned. This results in updating the model parameters and retraining the model with possibly an updated training data set. This process keeps repeating until a model with the desired performance is achieved. What are the best practices of managing a training data pipeline? To begin with, there has to be a life cycle like agile that needs to be followed by the data science team. There has been some progress in customized processes for machine learning that are optimized for continuous experiments. Also, modeling involves writing code mostly in notebook. There should be separate development and test environments for training the models. It's recommended that the final run of the model takes place in a controlled environment. Next comes traceability of the experiments. Experiment tracking is a key MLOps activity that we will cover later in this chapter. Also, version control should be used for code, data, and models, and the corresponding versions need to be tied together for experiment tracking. Reproducibility of training is a key MLOps goal. It should be able to reproduce the building of the model from scratch from its training data and input parameters. Model as code approach is recommended where all the notebooks themselves are baseline and versioned. Version data should be used. Rerunning the code should result in the same model being produced again. Finally, automation plays a key role in scaling the training process efficiently. Automation can be used for parameter selection and tuning of models. Techniques like grid search can be executed with automation each time a new experiment is done. Similarly, when new training data is available, the data engineering and model training pipelines should be automatically executed to build a new model and compare it to the baseline.
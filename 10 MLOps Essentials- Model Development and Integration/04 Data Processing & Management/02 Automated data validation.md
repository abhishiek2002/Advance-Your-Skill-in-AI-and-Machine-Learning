Automated data validation
- [Instructor] Automated data validation should be a key feature of any data pipeline. Typically, the data-processing logic is decided based on the initial set of data used by data scientists. Those assumptions would carry over to the first model that is built by data scientists and deployed in production. After the model is deployed, new data is continuously acquired and processed by automated data pipelines. In some cases, AutoML is also used to create new models. It then becomes imperative to perform extensive validation on new data to ensure that they do not deviate from the initial training-set patterns. What should be validated in a data pipeline? To begin, basic feature validation should be done, including missing data or erroneous data. Data formats and ranges should also be checked. Even with data produced by machines like telemetry, errors can happen if there are issues with the source. Next comes data distribution validation. Here, metrics like mean, standard deviation, and quartiles should be computed and compared with baseline values. Similarly, distribution of classes in categorical variables should also be done. This is to ensure that the new set of data belongs to the same distribution population as the original training dataset. Then comes out-of-distribution validation. This is to check if outliers occur in data. This includes values that are beyond quartiles for continuous data and also new class values for categorical data. Finally, it is also recommended to check the correlation between the feature and target variables and if they are in line with the correlation seen in the baseline training data. Checking for correlation between features is also recommended.
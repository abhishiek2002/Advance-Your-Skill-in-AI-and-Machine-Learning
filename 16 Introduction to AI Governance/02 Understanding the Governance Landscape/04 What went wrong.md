What went wrong?
- [Instructor] Let us dive into the past and learn from the instances when AI went wrong. When I started my journey into building trustworthy and responsible AI solutions several years ago, I, too, like most of us, were struggling with the abstract nature of ethical guidelines. I understood everything theoretically, but was still not able to relate how to practically apply them in real life. Hence, I referred these historical examples that make a compilation of case studies for us to understand the context, how the issue escalated, what was done to solve them eventually, and most importantly, what could have been done to prevent them from happening in the first place? There is a whole spectrum of the risk and their implications. Take, for example, a not so good movie recommender, which can make us put extra effort into looking for movies of our interest. A product search at an e-commerce website might require us to navigate to the next page that otherwise should have been listed higher in the search results. No doubt, these search results were not at par, could negatively impact the business through bad customer experience, and make them churn candidates. But as bad as these might seem, there are far worse downsides. When algorithms discriminate against marginalized communities, denying them credit loans and employment, and even leading to events that can have lasting impacts on their lives, such as grant bail and access to healthcare. Yes, you read that right. It has seeped into our lives in ubiquitous ways, making decisions for us. Right or wrong, the data decides, along with several other factors impacting the model lifecycle. If you want to know more, I've shared a handout listing such rogue incidents from different sectors at the end of this video. Unfortunately we don't need to go far into history to understand the gravity of this situation. Recently, an AI algorithm adjusted students' scores based on the status of the schools. It downgraded the scores of the students from less advantaged schools and raised for those from affluent ones. Where do you think the algorithm learned it from? Well, this algorithmic scoring simply amplified the societal bias that judges on economic status. The fiasco later transformed into public outrage, eventually forcing authorities to recall the algorithmic scores and accept teachers' estimated scores. Now that we know the damage caused by lack of governance, let us look at how it could have been averted. The first thought that is immediately quoted to rescue such behavior is that, mm-hm, well, the algorithms are complex and black-box in nature. They can perform in unintended ways and are probabilistic, so we cannot guarantee 100% accuracy. While that is true, the fault extends beyond algorithms. It lies in the systems and processes. Wondering how? Let us start with fairness and think how often it is considered as one of the vital model evaluation criteria. For an application as critical as access to education, a group of diverse decision makers, including teachers, students, experts, auditors, and policy makers should have come together to highlight and address its shortcomings. As much as identifying shortcomings is essential, so is its timing. Timely audits and rigorous testing of such systems with clear disclosure of its decision-making steps should have been shared with all associated stakeholders, including schools, parents, and students, before even leveraging such algorithms for full scale-decisions, that is official scoring. Having discussed the impact of AI on access to education in the pre-generative AI era, let us also see the repercussions that have surfaced with the advent of gen AI. An algorithm recently raised a false alert, flagging the use of AI to complete an assignment, leading to poor grades for the student. I strongly believe that the purpose of technology is to make our lives easier, and in that process, innocent lives must not be impacted negatively, which is clearly not the case here. Even if the authorities trusted the outcome of the AI system, there must be a protocol to assess the student's plea and overrule the original decision if the student is right. Measures like assessing knowledge through verbal grading would have been a reasonable temporary effects till we evolve our current ways of working in today's world. I would like to hear your views on how you would solve or approach a problem. Think from both sides, as an end user and the creator of such advanced system. As you wear your problem-solving hats on, let us next understand the evolving landscape of AI governance.